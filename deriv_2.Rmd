---
title: "Estimación <br> <br>"
output: 
  html_document:
    code_folding: hide
    css: style.css
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

## Distribución normal

<br>

$$
\begin{align}
\mathbf{x} &= x_i, \dots, x_n \\\\
P(\mathbf{x}; \mu, \sigma) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
L(\mu, \sigma; \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &= \sum_{i=1}^n -\frac{(x_i - \mu)^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi}) \\\\
\text{log}(L) &= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi)
\end{align}
$$

<br>

*Location-scale property*

<br>

$$
\begin{align}
Z &\sim N(\mu, \sigma^2) \\\\
a + bZ &\sim N(a + \mu, b^2\sigma^2)
\end{align}
$$

<br>

*Stability property*

<br>

Si $Z_{IID} \sim N(\mu, \sigma^2)$, entonces

$$
\sum_{i=1}^n Z_i \sim N\left(n \mu, n \sigma^2\right)
$$

<br>

### Estimación de $\mu$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu}&= \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) \\\\
&= \frac{n}{\sigma^2} \, (\bar{x} - \mu)
\end{align}
$$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu} &= 0 \\\\
\widehat\mu &= \bar{x} \\\\
\end{align}
$$

<br>

### Valor esperado de $\bar{x}$

<br>

$$
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n x_i \right] = \mu
$$

<br>

### Varianza de $\widehat\mu$

<br>

$$
\begin{align}
\frac{\partial^2 \text{log}(L)}{\partial \mu^2} &= - \frac{n}{\sigma^2} \\\\
\text{VAR}\left[\widehat\mu\right] &= \mathbb{E}\left[(\widehat\mu - \mu)^2\right] \\\\
&= \frac{\sigma^2}{n}
\end{align}
$$

<br>

### Estimación de $\sigma^2$

<br>

$$
\begin{align}
\text{log}(L) &= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi) \\\\
\end{align}
$$

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= 0 \\\\
\sigma^2 &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{n}
\end{align}
$$

<br>

### Valor esperado de $\frac{\sum_{i=1}^n (x_i - \widehat\mu)^2}{n}$

<br>

$$
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &= \frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n ((x_i - \mu) - (\widehat\mu - \mu))^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2 + (\widehat\mu - \mu)^2 - 2(x_i - \mu)(\widehat\mu - \mu))\right] \\\\
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &= n\sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (\widehat\mu - \mu)^2\right] &= \sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)(\widehat\mu - \mu) \right] &= \mathbb{E}\left[n(\mu^2 + \bar{x}\widehat\mu - \bar{x}\mu - \widehat\mu\mu)\right] \\\\
&= \mathbb{E}\left[n(\mu^2 + \widehat\mu^2 - 2\widehat\mu\mu)\right] \\\\
&= n \, \mathbb{E}\left[(\widehat\mu - \mu)^2 \right] \\\\
&= \sigma^2
\end{align}
$$

<br>

$$
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &= \sigma^2 + \frac{\sigma^2}{n} - \frac{2\sigma^2}{n} \\\\
&= \sigma^2 \left(\frac{n - 1}{n}\right) \\\\
\end{align}
$$
$$
\begin{align}
\sigma^2 &= \frac{\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right]}{n - 1}
\end{align}
$$

<br>

$$
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n - 1}
$$

<br>

### Varianza de $\sigma^2$ y $\widehat\sigma^2$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial^2 \text{log}(L)}{\partial \sigma^4} &= - \frac{\sum_{i=1}^n (x_i - \mu)^2}{\sigma^6} + \frac{n}{2\sigma^4} \\\\
&= \frac{- 2\sum_{i=1}^n (x_i - \mu)^2 + n\sigma^2}{2\sigma^6} \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &= n\sigma^2 \\\\
\mathbb{E}\left[\frac{\partial^2 \text{log}(L)}{\partial \sigma^4}\right] &= \frac{- 2n\sigma^2 + n\sigma^2}{2\sigma^6} \\\\
&= - \frac{n}{2\sigma^4} \\\\
\text{VAR}\left[\sigma^2\right] &= \frac{2\sigma^4}{n}
\end{align}
$$

<br>

$$
\begin{align}
\frac{\epsilon_i}{\sigma} &\sim N(0, 1) \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 &\sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 = \frac{(n-1)\widehat\sigma^2}{\sigma^2} &\sim \chi^2_{n-1} \\\\
\text{VAR}\left[\chi^2_{n-1}\right] &= 2(n-1) \\\\
\text{VAR}\left[\frac{(n-1)\widehat\sigma^2}{\sigma^2}\right] &= 2(n-1) \\\\
\text{VAR}\left[(n-1)\widehat\sigma^2\right] &= 2\sigma^4(n-1) \\\\
\text{VAR}\left[\widehat\sigma^2\right] &= \frac{2\sigma^4}{n-1} \\\\
\end{align}
$$

<br>

## Función afín

<br>

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_i + \epsilon_i \\\\
y_i &= \widehat{\beta}_0 + \widehat{\beta}_1x_i + e_i
\end{align}
$$

<br>

$$
\begin{align}
P(\mathbf{y}; \beta_0, \beta_1, \sigma, \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
L(\beta_0, \beta_1, \sigma; \mathbf{y}, \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &= \sum_{i=1}^n -\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi})
\end{align}
$$

<br>

### Residuos

<br>

$$
\begin{align}
\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})e_i &= 0, \quad \text{COV}[X, e] = 0 \\\\
\frac{1}{n} \sum_{i=1}^n e_i &= 0
\end{align}
$$

<br>

### Estimación de $\beta_1$

<br>

$$
\begin{align}
\frac{\partial L}{\partial \beta_1} &= - \frac{\sum_{i=1}^n \beta_0x_i + \beta_1x_i^2 - y_ix_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_1} &= 0 \\\\
&= \overline{yx} - \beta_0\bar{x} - \beta_1\overline{x^2} \\\\
\end{align}
$$
$$
\begin{align}
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - \beta_0\mathbb{E}[X] &= 0 \\\\
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - (\mathbb{E}[Y] - \beta_1\mathbb{E}[X]) \, \mathbb{E}[X] &= 0 \\\\
\text{COV}[YX] - \beta_1\mathbb{E}[X^2] - \beta_1\mathbb{E}[X]^2 &= 0 \\\\
\text{COV}[YX] - \beta_1\text{VAR}[X] &= 0 \\\\
\end{align}
$$
$$
\begin{align}
\beta_1 &= \frac{\text{COV}[YX]}{\text{VAR}[X]} \\\\
\end{align}
$$

<br>

$$
\begin{align}
\frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} &= \frac{\sum_{i=1}^n x_i  y_i - \bar{x} \bar{y}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{\sum_{i=1}^n x_i (\beta_0 + \beta_1x_i + \epsilon_i) - \bar{x} (\beta_0 + \beta_1\bar{x} + \bar{\epsilon})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{n\beta_1(\overline{x^2} - \bar{x}^2)}{\sum_{i=1}^n (x_i - \bar{x})^2} + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_1 + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
\end{align}
$$

<br>

Según la ley de la esperanza total,

$$
\begin{align}
\mathbb{E}\left[\widehat{\beta}_1\right] &= \mathbb{E}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_i, \dots, x_n\right]\right] \\\\
&= \mathbb{E}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \mathbb{E}[\epsilon_i]}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \, , \quad \mathbb{E}[\epsilon \, | \, x_i] = 0 \\\\
&= \beta_1 \\\\
\widehat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{align}
$$

<br>

### Estimación de $\beta_0$

<br>

$$
\begin{align}
\frac{\partial L}{\partial \beta_0} &=- \frac{\sum_{i=1}^n \beta_0 + \beta_1x_i - y_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_0} &= 0 \\\\
\widehat{\beta}_0 &= \bar{y} - \widehat{\beta}_1\bar{x} \\\\
\mathbb{E}\left[\widehat{\beta}_0\right] &= \beta_0 + \beta_1\mathbb{E}[X] - \beta_1\mathbb{E}[X] \\\\
&= \beta_0
\end{align}
$$

<br>

### Varianza de $\widehat{\beta}_1$

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right] &= \text{VAR}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \\\\
&= \sum_{i=1}^n \frac{(x_i - \bar{x})^2 \, \text{VAR}\left[\epsilon_i\right]}{n^2 s_x^4} \\\\
&= \frac{\sigma^2}{n s_x^2}
\end{align}
$$

Aplicando la ley de la varianza total,

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_1\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2}{n s_x^2} \\\\
\widehat{\text{VAR}}\left[\widehat{\beta}_1\right] &= \frac{\widehat\sigma^2}{n s_x^2}
\end{align}
$$

Dado que $\epsilon_i$ sigue una distribución normal, la propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{\beta}_1 &\sim N\left(\beta_1, \frac{\sigma^2}{ns_x^2} \right) \\\\\
\frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} &\sim N\left(0, 1 \right) \\\\\
\end{align}
$$

Si conociéramos $\sigma^2$, entonces

$$
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
$$

Pero si estimamos $\sigma^2$ a través de $\widehat\sigma^2$, entonces

$$
\begin{align}
\frac{\widehat{\beta}_1 - \beta_1}{\widehat\sigma / s_x\sqrt{n}} &= \frac{\frac{\widehat{\beta}_1 - \beta_1}{\sigma}}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&\sim \frac{N(0, 1/ns_x^2)}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&\sim \frac{N(0, 1)}{\frac{\widehat\sigma}{\sigma}} \\\\
&\sim \frac{N(0, 1)}{\sqrt{\frac{\sum_{i=1}^n e_i^2}{\sigma^2 (n-2)}}} \\\\
&\sim \frac{N(0, 1)}{\sqrt{\frac{\chi^2_{n-2}}{n-2}}} \\\\
&\sim t_{n-2}
\end{align}
$$

<br>

### Varianza de $\widehat{\beta}_0$

<br>

$$
\begin{align}
\text{VAR}\left[ \widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &= \text{VAR}\left[\bar{y} - \widehat{\beta}_1\bar{x}\right] \\\\
&= \text{VAR}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon} - \widehat{\beta}_1\bar{x}\right] \\\\
&= \text{VAR}\left[\bar{\epsilon}\right] + \bar{x}^2 \, \text{VAR}\left[\widehat{\beta}_1\right] - 2\bar{x}^2\text{COV}\left[\bar{\epsilon}, \widehat{\beta}_1\right]
\end{align}
$$

<br>

$$
\text{COV}\left[\bar{\epsilon}, \widehat{\beta}_1\right] = 0
$$

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &= \frac{\sigma^2}{n} + \frac{\sigma^2 \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2} - \sigma^2 s_x^2}{n s_x^2} \\\\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2}}{n s_x^2} - \frac{\sigma^2}{n} \\\\
&= \frac{\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
$$

<br>

Según la ley de la esperanza total,

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_0\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2 \overline{x^2}}{n s_x^2} \\\\
\widehat{\text{VAR}}\left[\widehat{\beta}_0\right] &= \frac{\widehat\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
$$

Dado que $\epsilon_i$ sigue una distribución normal, la propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{\beta}_0 &\sim N\left(\beta_0, \frac{\sigma^2 \overline{x^2}}{n s_x^2} \right) \\\\\
\frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} &\sim N\left(0, 1 \right) \\\\\
\end{align}
$$

Si conociéramos $\sigma^2$, entonces

$$
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
$$

Pero si estimamos $\sigma^2$ a través de $\widehat\sigma^2$, entonces

$$
\frac{\widehat{\beta}_0 - \beta_0}{\widehat\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \sim t_{n-2}
$$

<br>

### Predicciones

<br>

$$
\begin{align}
\widehat{m}(x^*) &= \widehat{\beta}_0 + \widehat{\beta}_1x^* \\\\
&= \bar{y} - \widehat{\beta}_1\bar{x} + \widehat{\beta}_1x^* \\\\
&= \bar{y} + (x^* - \bar{x})\widehat{\beta}_1 \\\\
&= \beta_0 + \beta_1\bar{x} + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x^* - \bar{x})\left(\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\\\
&= \beta_0 + \beta_1\bar{x} + (x^* - \bar{x})\beta_1 + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x^* - \bar{x}) \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_0 + \beta_1x^* + \frac{1}{n} \sum_{i=1}^n \epsilon_i \left(1 + (x^* - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right)
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[\widehat{m}(x^*)\right] &= \beta_0 + \beta_1x^* + \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\epsilon_i] \left(1 + (x^* - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right) \\\\
&= \beta_0 + \beta_1x^*
\end{align}
$$

<br>

### Varianza del promedio de las predicciones

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{m}(x^*) \, | \, x_i, \dots, x_n \right] &= \frac{1}{n^2} \sum_{i=1}^n \text{VAR}[\epsilon_i] \left(1 + (x^* - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right)^2 \\\\
&= \frac{\sigma^2}{n^2} \sum_{i=1}^n \left(1 + (x^* - \bar{x})^2 \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&= \frac{\sigma^2}{n^2} \left(n + (x^* - \bar{x})^2 \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&= \frac{\sigma^2}{n^2} \left(n + (x^* - \bar{x})^2 \frac{n}{s_x^2}\right) \\\\
&= \frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)
\end{align}
$$

Según la ley de la esperanza total,

$$
\begin{align}
\text{VAR}\left[\widehat{m}(x^*)\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{m}(x^*) \, | \, x_i, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{m}(x^*) \, | \, x_i, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)
\end{align}
$$

<br>

La propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{m}(x^*) &\sim N\left(\beta_0 + \beta_1x^*, \frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)\right) \\\\
\frac{\widehat{m}(x^*) - m(x^*)}{\sqrt{\frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)}} &\sim N(0, 1) \\\\
\frac{\widehat{m}(x^*) - m(x^*)}{\sqrt{\frac{\widehat\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)}} &\sim t_{n-2} \\\\
\end{align}
$$

<br>

### Varianza del error de predicción

<br>

$$
\begin{align}
y^* &= \beta_0 + \beta_1x^* + \epsilon^* \\\\
\widehat{m}(x^*) &= \widehat\beta_0 + \widehat\beta_1x^* \\\\
y^* - \widehat{m}(x^*) &= \beta_0 + \beta_1x^* - \widehat\beta_0 - \widehat\beta_1x^* + \epsilon^* \\\\
\text{COV}[\widehat{m}(x^*), \epsilon^*] &= 0 \\\\
\text{VAR}[y^* - \widehat{m}(x^*)] &= \frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right) + \sigma^2 \\\\
\text{VAR}[y^* - \widehat{m}(x^*)] &= \sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{n s_x^2}\right)
\end{align}
$$

<br>

La propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{m}(x^*) &\sim N\left(y^*, \sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{n s_x^2}\right)\right) \\\\
\frac{y^* - \widehat{m}(x^*)}{\sqrt{\sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{n s_x^2}\right)}} &\sim N(0, 1) \\\\
\frac{y^* - \widehat{m}(x^*)}{\sqrt{\widehat\sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{n s_x^2}\right)}} &\sim t_{n-2}
\end{align}
$$

<br>

### Varianza de los valores ajustados

<br>

$$
\begin{align}
\widehat{\beta}_0 &= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
$$

<br>

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_i + \epsilon_i \\\\
\text{VAR}\left[y_i\right] &= \sigma^2 \\\\
\widehat{m}_i &= \widehat\beta_0 + \widehat\beta_1x_i \\\\
&= \bar{y} - \widehat\beta_1\bar{x} + \widehat\beta_1x_i \\\\
&= \bar{y} + (x_i - \bar{x})\widehat\beta_1 \\\\
&= \bar{y} + (x_i - \bar{x})\left(\beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
&= \beta_0 + \beta_1x_i + (x_i - \bar{x})\left(\frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
\text{VAR}\left[\widehat{m}_i\right] &=  \frac{\sigma^2}{n} + (x_i - \bar{x})^2 \frac{\sigma^2}{ns_x^2} \\\\
&= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
\end{align}
$$

<br>

### Varianza del error de estimación (Varianza de un residuo)

<br>

$$
\mathbb{E}\left[(y_i - \widehat{m}_i)^2\right] = \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\,\text{COV}\left[y_i, \widehat{m}_i\right]
$$

<br>

$$
\begin{align}
\text{COV}\left[y_i, \widehat{m}_i\right] &= \text{COV}\left[y_i + \widehat{m}_i - \widehat{m}_i, \widehat{m}_i \right] \\\\
&= \text{COV}\left[\widehat{m}_i, \widehat{m}_i \right] + \text{COV}\left[y_i - \widehat{m}_i, \widehat{m}_i \right] \\\\
&= \text{COV}\left[\widehat{m}_i, \widehat{m}_i \right] + \text{COV}\left[e_i, \widehat{m}_i \right] \\\\
&= \text{VAR}\left[\widehat{m}_i\right] + 0 \\\\
&= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[(y_i - \widehat{m}_i)^2\right] &= \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&= \sigma^2 - \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&= \sigma^2 \left(1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
$$

<br>

### Varianza de la estimación del error cuadrático medio, $\widehat\sigma^2$

<br>

$$
\mathbb{E}\left[(Y - (\beta_0 + \beta_1X))^2\right] = \sigma^2
$$

<br>

$$
\begin{align}
\text{VAR}\left[\bar{y} \, | \, x_i, \dots, x_n\right] &= \text{VAR}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon}\right] \\\\
&= \frac{1}{n^2} \sum_{i=1}^n \text{VAR}\left[\epsilon_i\right] \\\\
&= \frac{\sigma^2}{n}
\end{align}
$$

<br>

$$
\begin{align}
\widehat{\beta}_0 &= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
$$

<br>

$$
\begin{align}
 \mathbb{E}\left[\sum_{i=1}^n e_i^2 \, | \, x_i, \dots, x_n\right] &= \mathbb{E}\left[\sum_{i=1}^n (y_i - \widehat{m}_i)^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n \left(\beta_0 + \beta_1x_i + \epsilon_i - \widehat{\beta}_0 - \widehat{\beta}_1x_i\right)^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n \left((\beta_0 - \widehat{\beta}_0)^2 + (\beta_1 - \widehat{\beta}_1)^2x_i^2 + \epsilon_i^2\right)\right] + \\\\
& 2 \, \mathbb{E}\left[\sum_{i=1}^n (\beta_0 - \widehat{\beta}_0)(\beta_1 - \widehat{\beta}_1)x_i + (\beta_0 - \widehat{\beta}_0)\epsilon_i + (\beta_1 - \widehat{\beta}_1)x_i\epsilon_i \right] \\\\
&= n \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2}}{n s_x^2}\right] + 1 \right) - 2\sigma^2 \, \mathbb{E}\left[\frac{\bar{x}^2}{s_x^2} \right] - 2 \sigma^2 - 2 \sigma^2 \\\\
&= \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2} - \bar{x}^2}{s_x^2}\right] + n - 4 \right) \\\\
&= \sigma^2 \left(n - 2 \right) \\\\
\end{align}
$$

<br>

$$
\widehat\sigma^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2 \\\\
$$

$\widehat{\sigma}^2$ es estadísticamente independiente de $\widehat\beta_1$ y $\widehat\beta_0$ a pesar de que todos los estimadores son una función de $\epsilon$.

<br>

Sabiendo que si $Z \sim N(0, 1)$, entonces $Z^2 \sim \chi^2$ y que $\frac{\epsilon_i}{\sigma} \sim N(0, 1)$, entonces

<br>

$$
\begin{align}
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 &= \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 \sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 &\sim \chi^2_{n-2} \\\\
P\left(\chi^2_{n-2} \; \leq \; \chi^2_{n-2, \, 1-\alpha}\right) &= 1 - \alpha \\\\
P\left(\sigma^2 \; \leq \; \frac{(n-2) \widehat\sigma^2}{\chi^2_{n-2, \, 1-\alpha}}\right) &= 1 - \alpha \\\\
\text{VAR}\left[\chi^2_{n-2}\right] &= 2(n-2) \\\\
\text{VAR}\left[\frac{(n-2)\widehat\sigma^2}{\sigma^2}\right] &= 2(n-2) \\\\
\text{VAR}\left[(n-2)\widehat\sigma^2\right] &= 2\sigma^4(n-2) \\\\
\text{VAR}\left[\widehat\sigma^2\right] &= \frac{2\sigma^4}{n-2} \\\\
\end{align}
$$

<br>

### Modelo sin predictores

<br>

$$
\begin{align}
Y &= \beta_0 + \epsilon \\\\
\widehat\beta_0 &= \bar{y} \sim N\left(\beta_0, \frac{\sigma^2}{n}\right) \\\\
\widehat\sigma^2 &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \\\\
&= s_y^2 \\\\
\frac{(n-1) \widehat\sigma^2}{\sigma^2} &\sim \chi^2_{n-1}
\end{align}
$$

<br>

## El test F

<br>

La distribución $\chi^2$ procede de la suma del cuadrado de variables con distribución normal y la distribución $F$ procede de la razón entre variables aleatorias con distribución $\chi^2$.

$$
\frac{\chi^2_a}{\chi^2_b} \frac{b}{a} \sim F_{a, b}
$$

Según el modelo sin predictores, 

$$
\frac{(n-1) \widehat\sigma^2_{null}}{\sigma^2} \sim \chi^2_{n-1}
$$

mientras que según el modelo con un predictor,

$$
\frac{(n-2) \widehat\sigma_2^2}{\sigma^2} \sim \chi^2_{n-2}
$$

Por tanto,

$$
\begin{align}
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{\sigma^2} &\sim \chi^2_1 \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{(n-2) \widehat\sigma_2^2} &\sim \frac{\chi^2_1}{\chi^2_{n-2}} \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{(n-2) \widehat\sigma_2^2} \, \frac{n-2}{1} &\sim F_{1, n-2} \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{\widehat\sigma_2^2} &\sim F_{1, n-2}
\end{align}
$$

El test $F$ asume que los errores ($\epsilon$) siguen una distribución normal, son homocedásticos e independientes de la variable $X$ y entre sí. Se trata de un caso particular de la razón de verosimilitudes.

<br>

En el caso general, si estuviéramos contrastando la significación de $p-q$ estimaciones,

$$
\begin{align}
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{\sigma^2} &\sim \chi^2_{p-q} \\\\
\frac{(n-p) \widehat\sigma^2_{p}}{\sigma^2} &\sim \chi^2_{n-p} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(n-p) \widehat\sigma^2_{p}} &\sim \frac{\chi^2_{p-q}}{\chi^2_{n-p}} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(n-p) \widehat\sigma^2_{p}} \, \frac{n-p}{p-q} &\sim F_{p-q, n-p} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(p-q) \widehat\sigma^2_{p}} &\sim F_{p-q, n-p}
\end{align}
$$

<br>

## El test de razón de verosimilitudes

<br>

El espacio paramétrico del modelo nulo de $q$ coeficientes es un subconjunto del espacio paramétrico del modelo general de $p$ coeficientes.

El test de razón de verosimilitudes asume que las estimaciones de los parámetros siguen una distribución normal y esto es solo aproximadamente cierto para $\widehat\sigma^2$. Por tanto, solo es exactamente correcto cuando $n \rightarrow \infty$.

$$
\begin{align}
\Lambda &= L(\widehat\Theta) - L(\widehat\theta) \\\\
\lim_{n \rightarrow \infty} \quad 2\Lambda &\sim \chi^2_{p-q}
\end{align}
$$

<br>

$$
\begin{align}
L(\widehat\theta) &= - \frac{n}{2} \ \log2\pi - \frac{n}{2} \ \log (n-q)\widehat\sigma^2_q - \frac{1}{2(n-q)\widehat\sigma^2_q} \sum_{i=1}^n (y_i - \widehat\beta_0)^2 \\\\
&= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log (n-q)\widehat\sigma^2_q} \\\\
L(\widehat\Theta) &= - \frac{n}{2} \ \log2\pi - \frac{n}{2} \ \log (n-p)\widehat\sigma^2_p - \frac{1}{2(n-p)\widehat\sigma^2_p} \sum_{i=1}^n (y_i - \widehat\beta_0 - \widehat\beta_1x_i)^2 \\\\
&= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\ \log (n-p)\widehat\sigma^2_p}
\end{align}
$$

<br>

$$
\begin{align}
L(\widehat\Theta) - L(\widehat\theta)
&= \frac{n}{2} \log\frac{(n-q)\widehat\sigma^2_q}{(n-p)\widehat\sigma^2_p} \\\\
\lim_{n \rightarrow \infty} \quad n \log\frac{(n-q)\widehat\sigma^2_q}{(n-p)\widehat\sigma^2_p} &\sim \chi^2_{p-q}
\end{align}
$$

Incluso cuando ambos modelos son falsos, el test de razón de verosimilitudes nos puede indicar cuál de ellos se parece más al verdadero.

<br>

## $R^2$

<br>

$$
\begin{align}
R^2 &= \frac{\text{COV}\left[y, \widehat{m}\right]}{s_y^2} \\\\
\text{COV}\left[y, \widehat{m}\right] &= \text{COV}\left[\widehat{m} + e, \widehat{m}\right] \\\\
&= s_\widehat{m}^2 + \text{COV}\left[e, \widehat{m}\right] \\\\
&= s_\widehat{m}^2 \\\\
R^2 &= \frac{s_\widehat{m}^2}{s_y^2} \\\\
s_\widehat{m}^2 &= s_{\widehat{\beta}_0 + \widehat{\beta}_1X}^2 \\\\
&= \widehat{\beta}_1^2 s_x^2 \\\\
R^2 &= \widehat{\beta}_1^2 \frac{s_x^2}{s_y^2} \\\\
&= \left(\frac{\text{COV}\left[x, y\right]}{s_x s_y}\right)^2 \\\\
&= \frac{s_y^2 - \overline{e^2}}{s_y^2} \\\\
R_{\text{ajustado}}^2 &= \frac{\widehat{s}_y^2 - \widehat\sigma^2}{\widehat{s}_y^2}
\end{align}
$$

<br>

$$
\begin{align}
R^2 &= \frac{\text{VAR}\left[m(X)\right]}{\text{VAR}\left[Y\right]} \\\\
&= \frac{\text{VAR}\left[\beta_0 + \beta_1X\right]}{\text{VAR}\left[\beta_0 + \beta_1X + \epsilon\right]} \\\\
R^2 &= \frac{\beta_1^2 \text{VAR}\left[X\right]}{\beta_1^2 \text{VAR}\left[X\right] + \sigma}
\end{align}
$$

<br>

## $\rho_{xy}$

<br>

$$
\begin{align}
\rho_{XY} &= \frac{\text{COV}\left[X, Y\right]}{\sqrt{\text{VAR}\left[X\right] \text{VAR}\left[Y\right]}} \\\\
&= \beta_1 \frac{\text{VAR}\left[X\right]}{\sqrt{\text{VAR}\left[Y\right]}}
\end{align}
$$

<br>

## Notación matricial

> \newcommand{\m}[1]{\mathbf{#1}}
> \newcommand{\b}[1]{\boldsymbol{#1}}

<br>

$$
\begin{align}
\mathbf{y} &= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
\text{MSE} &= \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \boldsymbol{\beta}^\prime \mathbf{X}^\prime) (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime \mathbf{y} - \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
\mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} &= \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} \quad \text{porque} \; \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} \; \text{es una matriz} \; 1 \times 1 \\\\
\text{MSE} &= \frac{1}{n} (\mathbf{y}^\prime \mathbf{y} - 2 \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

### Estimación de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\nabla \text{MSE} &= \frac{1}{n} (\nabla \mathbf{y}^\prime \mathbf{y} - 2 \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (0 - 2 \mathbf{X}^\prime \mathbf{y} + 2 \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{2}{n} (- \mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

$$
\begin{align}
\mathbf{X}^\prime \mathbf{X} \boldsymbol{\widehat\beta} - \mathbf{X}^\prime \mathbf{y} &= 0 \\\\
\boldsymbol{\widehat\beta} &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y}
\end{align}
$$

<br>

### Varianza de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\mathbf{\widehat\beta} &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y} \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}) \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} + (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \boldsymbol{\epsilon} \\\\
&= \boldsymbol{\beta} + (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \boldsymbol{\epsilon} \\\\
\text{VAR}\left[\boldsymbol{\widehat\beta}\right] &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \,  \text{VAR}\left[\boldsymbol{\epsilon}\right] \left((\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime\right)^\prime \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \sigma^2 \mathbf{I} \, \mathbf{X}(\mathbf{X}^\prime \mathbf{X})^{-1} \\\\
&= \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \mathbf{X}(\mathbf{X}^\prime \mathbf{X})^{-1} \\\\
&= \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}
\end{align}
$$

<br>

### Distribución de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\widehat{\boldsymbol{\beta}} &\sim MVN(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}) \\\\
\widehat{\beta}_i &\sim N(\beta_i, \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}) \\\\
\frac{\widehat{\beta}_i - \beta_i}{\sigma \sqrt{(\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}}} &\sim N(0, 1) \\\\
\frac{\widehat{\beta}_i - \beta_i}{\widehat\sigma \sqrt{(\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}}} &\sim t_{n-p-1}
\end{align}
$$

<br>

### Valores ajustados

<br>

*Hat* o *influence matrix*:

$$
\begin{align}
\mathbf{H} &= \mathbf{X} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \\\\
\mathbf{\widehat{m}}(\mathbf{X}) &= \mathbf{X} \boldsymbol{\widehat\beta} \\\\
\mathbf{\widehat{m}}(\mathbf{X}) &= \mathbf{H} \mathbf{y}
\end{align}
$$

Propiedades de $\mathbf{H}$:

$$
\begin{align}
\frac{\partial \widehat{m}_i}{\partial y_j} &= H_{ij} \\\\
\mathbf{H} &= \mathbf{H}^\prime \\\\
\mathbf{H}^2 &= \mathbf{H} \\\\
\end{align}
$$

<br>

### Varianza de los valores ajustados

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{Hy}\right] &= \text{VAR}\left[\mathbf{H (\mathbf{X} \boldsymbol{\beta + \boldsymbol{\epsilon}})}\right] \\\\
&= \text{VAR}\left[\mathbf{H} \boldsymbol{\epsilon}\right] \\\\
&= \mathbf{H} \, \text{VAR}\left[\boldsymbol{\epsilon}\right] \, \mathbf{H}^\prime \\\\
&= \sigma^2 \mathbf{H}
\end{align}
$$

<br>

### Predicciones

<br>

$$
\begin{align}
\widehat{\mathbf{m}}(\mathbf{X^*}) &= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y} \\\\
&= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\widehat\beta} + \mathbf{e}) \\\\
&= \mathbf{X^*} \boldsymbol{\widehat\beta}
\end{align}
$$

<br>

### Varianza de las predicciones

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{\mathbf{m}}(\mathbf{X^*})\right] &= \text{VAR}\left[\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y}\right] \\\\
&= \text{VAR}\left[\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
&= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \text{VAR}\left[\boldsymbol{\epsilon}\right] \, (\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime)^\prime \\\\
&= \sigma^2 \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \mathbf{X} (\mathbf{X}^\prime \mathbf{X})^{-1} \, (\mathbf{X^*})^\prime \\\\
&= \sigma^2 \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, (\mathbf{X}^*)^\prime
\end{align}
$$

<br>

### Varianza del error de predicción

<br>

$$
\begin{align}
\text{COV}\left[\mathbf{y^*}, \widehat{\mathbf{m}}(\mathbf{X^*})\right] &= 0 \\\\
\text{VAR}\left[\mathbf{y^*} - \widehat{\mathbf{m}}(\mathbf{X^*})\right] &= \text{VAR}\left[\mathbf{y^*} \right] + \text{VAR}\left[\widehat{\mathbf{m}}(\mathbf{X^*}) \right] \\\\
&= \sigma^2 + \sigma^2 \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, (\mathbf{X}^*)^\prime
\end{align}
$$

<br>

### Residuos

<br>

$$
\begin{align}
\mathbf{e} &= \mathbf{y} - \mathbf{X}\boldsymbol{\widehat\beta} \\\\
&= \mathbf{y} - \mathbf{Hy} \\\\
&= (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
$$

Propiedades de $(\mathbf{I} - \mathbf{H}) \mathbf{y}$:

$$
\begin{align}
\frac{\partial e_i}{\partial y_j} &= (\mathbf{I} - \mathbf{H})_{ij} \\\\
(\mathbf{I} - \mathbf{H}) &= (\mathbf{I} - \mathbf{H})^\prime \\\\
(\mathbf{I} - \mathbf{H})^2 &= (\mathbf{I} - \mathbf{H}) \\\\
\end{align}
$$

<br>

$$
\begin{align}
\text{MSE} &= \frac{1}{n} \mathbf{e}^\prime \mathbf{e} \\\\
&= \frac{1}{n} \mathbf{y}^\prime (\mathbf{I} - \mathbf{H})^\prime \, (\mathbf{I} - \mathbf{H}) \mathbf{y} \\\\
&= \frac{1}{n} \mathbf{y}^\prime (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
$$

<br>

### Varianza de los residuos

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{e}\right] &= \text{VAR}\left[(\mathbf{I} - \mathbf{H}) (\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
\mathbf{HX} \boldsymbol{\beta} &= \mathbf{X} \boldsymbol{\beta} \\\\
\text{VAR}\left[\mathbf{e}\right] &= \text{VAR}\left[(\mathbf{I} - \mathbf{H}) \boldsymbol{\epsilon}\right] \\\\
&= (\mathbf{I} - \mathbf{H}) \text{VAR}\left[\boldsymbol{\epsilon}\right] (\mathbf{I} - \mathbf{H})^\prime \\\\
&= \sigma^2 (\mathbf{I} - \mathbf{H})
\end{align}
$$

<br>

### Varianza de la estimación del error cuadrático, $\widehat\sigma^2$

<br>

$$
\begin{align}
\frac{1}{n} \mathbb{E}\left[\mathbf{e}'\mathbf{e}\right] &= \frac{1}{n} \mathbb{E}\left[((\mathbf{I} - \mathbf{H})\mathbf{e})^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \mathbb{E}\left[\mathbf{e}^\prime(\mathbf{I} - \mathbf{H})^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \mathbb{E}\left[\mathbf{e}^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \text{VAR}[\mathbf{e}]\right]\\\\
&= \frac{1}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \sigma^2 (\mathbf{I} - \mathbf{H}) \right] \\\\
&= \frac{\sigma^2}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \right] \\\\
\text{tr}[\mathbf{I}] &= n \\\\
\text{tr}[\mathbf{H}] &= p + 1 \\\\
\frac{1}{n} \mathbb{E}\left[\mathbf{e}'\mathbf{e}\right] &= \frac{\sigma^2}{n} (n - p - 1) \\\\
\sigma^2 &= \frac{\mathbb{E}\left[\mathbf{e}'\mathbf{e}\right]}{n - p - 1} \\\\
\widehat\sigma^2 &= \frac{\mathbf{e}'\mathbf{e}}{n - p - 1}
\end{align}
$$

$$
\frac{(n-p-1) \widehat\sigma^2}{\sigma^2} \sim \chi^2_{n-p-1}
$$

<br>

## Derivación alternativa

<br>

$$
\begin{align}
\mathbf{y} &= \mathbf{1}\beta_0 + \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
\text{MSE} &= \frac{1}{n} (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \mathbf{1}^\prime\beta_0 - \boldsymbol{\beta} \mathbf{X}^\prime) (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \mathbf{1}^\prime\beta_0 - \boldsymbol{\beta}^\prime \mathbf{X}^\prime) (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

$$
\begin{align}
\frac{\partial \text{MSE}}{\partial \beta_0} &= \frac{1}{n}\left(- \nabla \mathbf{1}^\prime \beta_0 \mathbf{y} + \nabla \mathbf{1}^\prime \beta_0 \beta_0 \mathbf{1} + \\
\nabla \mathbf{1}^\prime \beta_0 \mathbf{X} \boldsymbol{\beta} - \nabla \mathbf{y}^\prime \beta_0 \mathbf{1} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \beta_0 \mathbf{1} \right) \\\\
&= - \frac{2}{n} \mathbf{1}^\prime \left(\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}\right) \\\\
0 &= - \frac{1}{n} \mathbf{1}^\prime \mathbf{y} + \frac{1}{n} \mathbf{1}^\prime \beta_0 \mathbf{1} + \frac{1}{n} \mathbf{1}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\widehat\beta_0 &= \frac{1}{n} \mathbf{1}^\prime \mathbf{y} - \frac{1}{n} \mathbf{1}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&= \bar{y} - \mathbf{\bar{x}}^\prime \boldsymbol{\beta}
\end{align}
$$

<br>

$$
\begin{align}
\nabla_{\boldsymbol{\beta}} \, \text{MSE} &= \frac{1}{n} \left(- \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \beta_0 \mathbf{1} + \\
\nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} - \nabla \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} + \nabla \mathbf{1}^\prime \beta_0 \mathbf{X} \boldsymbol{\beta} \right) \\\\
&= \frac{2}{n} \left(\beta_0 \mathbf{X}^\prime \mathbf{1} - \mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}\right) \\\\
0 &= \beta_0 \frac{1}{n} \mathbf{X}^\prime \mathbf{1} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&= \beta_0 \mathbf{\bar{x}} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&= \mathbf{\bar{x}} (\bar{y} - \mathbf{\bar{x}}^\prime \boldsymbol{\beta}) - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&= \mathbf{\bar{x}} \bar{y} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \boldsymbol{\beta} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \boldsymbol{\beta} &= \frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}} \bar{y} \\\\
\left(\frac{1}{n} \mathbf{X}^\prime \mathbf{X} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \right) \boldsymbol{\beta} &= \frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}} \bar{y} \\\\
\boldsymbol{\widehat\beta} &= \frac{\frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}}^\prime \bar{y}}{\frac{1}{n} \mathbf{X}^\prime \mathbf{X} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime}
\end{align}
$$

<br>

## Sesgo de estimación por omisión de variables relacionadas

<br>

$$
\begin{align}
\widehat\beta_1 &= \frac{\text{COV}\left[X_1, Y\right]}{\text{VAR}\left[X_1\right]} \\\\
&= \frac{\text{COV}\left[X_1, \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p\right]}{\text{VAR}\left[X_1\right]} \\\\
&= \frac{\beta_1 \text{VAR}\left[X_1\right] + \sum_{i=2}^p \beta_p \text{COV}\left[X_1, X_p\right]}{\text{VAR}\left[X_1\right]} \\\\
&= \beta_1 + \sum_{i=2}^p \beta_p \frac{\text{COV}\left[X_1, X_p\right]}{\text{VAR}\left[X_1\right]} \\\\
\end{align}
$$

<br>

## Colinealidad

<br>

Existen constantes $a_0, a_1, \ldots, a_p$ de manera que, para dos filas $i$,

$$
a_0 + \sum_{j=1}^p a_j x_{ij} = 0
$$

$\mathbf{X}^\prime \mathbf{X}$ no es invertible cuando $n < p + 1$, uno de los predictores es constante o alguno de los predictores es proporcional o está linealmente relacionado con otro.

Dos vectores son linealmente independientes si ninguna combinación lineal de ellos equivale a 0.

La multicolinealidad ocurre cuando

$$
\begin{align}
\sum_{i=1}^p a_i \mathbf{x}_i &= a_0 \\\\
\mathbf{a}^\prime \mathbf{X} &= a_0 \\\\
\text{VAR}\left[\mathbf{a}^\prime \mathbf{X} \right] &= \mathbf{a}^\prime \, \text{VAR}\left[\mathbf{X}\right] \mathbf{a} \\\\
&= 0
\end{align}
$$

<br>

## Factor de inflación de la varianza

<br>

$$
\begin{align}
\text{VIF}_i &= \frac{\widehat\sigma^2 / n s_{x_{i}}^2}{\widehat\sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}_{i+1, i+1}} \\\\
&= n s_{x_{i}}^2  \ (\mathbf{X}^\prime \mathbf{X})^{-1}_{i+1, i+1}
\end{align}
$$

El factor de inflación de la varianza equivale a regresar $\mathbf{x}_i$ en el resto de predictores y calcular $\frac{1}{1 - R^2}$.

<br>

## Autovalores y autovectores

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{X}\right] \mathbf{v}_i &= \lambda_i \mathbf{v}_i \\\\
\mathbf{V}^\prime \mathbf{V} &= \mathbf{I} \\\\
\mathbf{V}^{-1} &= \mathbf{V}^\prime \\\\
\text{VAR}\left[\mathbf{X}\right] &= \mathbf{V} \, \mathbf{U} \, \mathbf{V}^\prime
\end{align}
$$

Los autovectores de $\text{VAR}\left[\mathbf{X}\right]$ se conocen como los componentes principales de los predictores.

Cualquier vector $\mathbf{a}$ puede ser reescrito como una suma de autovectores:

$$
a = \sum_{i=1}^p (\mathbf{a}^\prime \mathbf{v}_i) \mathbf{v}_i
$$

<br>

## Derivadas respecto a vectores

<br>

Si $\mathbf{a}$ y $\mathbf{x}$ son vectores $p \times 1$, entonces

$$
\begin{align}
\nabla_{\mathbf{x}} (\mathbf{x}^\prime \mathbf{a}) &= \mathbf{a} \\\\
\nabla_{\mathbf{x}} (\mathbf{b} \mathbf{x}) &= \mathbf{b}^\prime \\\\
\end{align}
$$

Si $\mathbf{c}$ es una matriz de dimensiones $p \times p$, entonces

$$
\nabla_{\mathbf{x}} (\mathbf{x}^\prime \mathbf{c} \mathbf{x}) = (\mathbf{c} + \mathbf{c}^\prime) \mathbf{x}
$$

Si, además, $\mathbf{c} = \mathbf{c}^\prime$, entonces

$$
\nabla{\mathbf{x}} (\mathbf{x}^\prime \mathbf{c} \mathbf{x}) = 2 \mathbf{c} \mathbf{x}
$$

<br>

## Esperanza y varianza de vectores y matrices

<br>

Si $\mathbf{x}$ es un vector aleatorio $n \times 1$, entonces

$$
\begin{align}
\text{VAR}\left[\mathbf{x}\right] &= \mathbb{E}\left[\mathbf{x} \mathbf{x}^\prime\right] - \mathbb{E}\left[\mathbf{x}\right] \mathbb{E}\left[\mathbf{x}\right]^\prime \\\\
\text{VAR}\left[b\mathbf{x}\right] &= b^2 \mathbf{x} \\\\
\text{VAR}\left[\mathbf{cx}\right] &= \mathbf{c} \text{VAR}\left[\mathbf{x} \right] \mathbf{c}^\prime \\\\
\mathbb{E}\left[\mathbf{x}^\prime \mathbf{cx} \right] &= \mathbb{E}\left[\mathbf{x}\right]^\prime \mathbf{c}\mathbb{E}\left[\mathbf{x}\right] + \text{tr} \, \mathbf{c} \text{VAR}\left[\mathbf{x}\right] \\\\
\mathbf{x}^\prime \mathbf{cx} &= \text{tr} \, \mathbf{x}^\prime \mathbf{cx} \\\\
&= \text{tr} \, \mathbf{cxx}^\prime
\end{align}
$$

<br>

## El método Delta

<br>

$\theta$ es un parámetro que pretendemos estimar a través de un estimador insesgado $\hat\theta$. Supongamos que $\theta$ es una función de un vector de parámetros $\boldsymbol{\psi}$,

<br>

$$
\theta = f\left(\psi_1, \dots, \psi_n \right)
$$

Entonces,

$$
\widehat\theta = f\left(\widehat\psi_1, \dots, \widehat\psi_p \right)
$$

<br>

Usando la expansión de Taylor,

<br>

$$
\begin{align}
\theta &\approx \widehat\theta + \sum_{i=1}^p \left(\psi_i - \widehat\psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\hat\theta &\approx \theta + \sum_{i=1}^p \left(\widehat\psi_i - \psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\text{VAR}\left[\hat\theta\right] &\approx \sum_{i=1}^{p} {\left. \frac{\partial^2 f}{\partial \psi_i^2} \right|}_{\psi=\widehat\psi} \, \text{VAR}\left[{\widehat{\psi_i}}\right] + \\
& 2 \sum_{i=1}^{p-1} \sum_{j=i+1}^{p} {\left. \frac{\partial f}{\partial \psi_i}\right|}_{\psi=\widehat\psi} \, {\left. \frac{\partial f}{\partial \psi_j}\right|}_{\psi=\widehat\psi} \, \text{COV}\left[{\widehat{\psi_i},\widehat{\psi_j}}\right]
\end{align}
$$

<br>

## Regresión de componentes principales

<br>

Los componentes principales son

$$
\mathbf{W} = \mathbf{X} \mathbf{V}
$$

Cada componente se define como la proyección de $\mathbf{X}$ en los autovectores.

El modelo de regresión con $k$ componentes es

$$
Y = \gamma_0 + \gamma_1 W_1 + \ldots + \gamma_k W_k + \eta
$$

$\eta = \epsilon$ si $k = p$.

<br>

## Regresión *Ridge*

<br>

Se trata de penalizar la longitud del vector de coeficientes $(\| \boldsymbol{\beta} \|)$. En *OLS* optimizamos la ecuación

$$
\frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
$$

Sin embargo, ahora optimizamos

$$
\frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \| \boldsymbol{\beta} \|^2 = \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \boldsymbol{\beta}^\prime \boldsymbol{\beta}
$$

donde $\lambda > 0$ regula el *trade-off* entre el MSE y $\| \boldsymbol{\beta} \|$.

Esta regresión se emplea tras centrar predictores y variable dependiente, para que $\beta_0 = 0$ y así no se penalice el tamaño de la intersección.

Si los predictores tienen diferente escala, entonces conviene estandarizarlos para que la penalización por $\| \boldsymbol{\beta} \|^2$ tenga sentido.

$$
\begin{align}
\nabla_\boldsymbol{\beta} \, \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \boldsymbol{\beta}^\prime \boldsymbol{\beta} &= \frac{2}{n} \left(-\mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} + \lambda \boldsymbol{\beta} \right) \\\\
\frac{2}{n} \left(-\mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\widehat\beta} + \lambda \boldsymbol{\widehat\beta} \right) &= 0 \\\\
\mathbf{X}^\prime \mathbf{y} &= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right) \boldsymbol{\widehat\beta} \\\\
\boldsymbol{\widehat\beta} &= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{y}
\end{align}
$$

La diferencia entre el estimador *OLS* y el anterior es que se le añade $\lambda$ a la diagonal de $\mathbf{X}^\prime \mathbf{X}$ (este es el *ridge*). Como consecuencia, se minimizan posibles problemas por colinealidad y la varianza de los coeficientes en $\boldsymbol{\widehat\beta}$ es menor.

Esta menor varianza implica sesgo (*bias-variance trade-off*)

$$
\begin{align}
\mathbb{E}\left[ \boldsymbol{\widehat\beta} \right] &= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbb{E}\left[ \mathbf{y} \right] \\\\
&= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\text{VAR}\left[ \boldsymbol{\widehat\beta} \right] &= \text{VAR}\left[\left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{y} \right] \\\\
&= \text{VAR}\left[\left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{\epsilon} \right] \\\\
&= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \, \sigma^2 \mathbf{I} \, \mathbf{X} \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \\\\
&= \sigma^2 \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{X} \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1}
\end{align}
$$

Si $\lambda$ no es fijado sino estimado, entonces tendríamos que preocuparnos de su distribución.

Una de las principales ventajas de esta regresión es que permite comprobar con gran precisión la contribución de los predictores sin necesidad de preocuparse de qué variables prescindir.

<br>

## Regresión *Lasso*

<br>

Esta regresión sustituye la penalización de $\| \boldsymbol{\beta} \|^2$ por otra medida de la longitud del vector:

$$
\| \boldsymbol{\beta} \|_q = \left( \sum_{i=1}^p b_i^q \right)^{1/q}
$$

Si $q = 2$, entonces la penalización es la misma que en la regresión *ridge*. La regresión *lasso* (*least angle selection and shrinkage operator*) utiliza la penalización $q = 1$.

<br>

## Corrección de Bonferroni

<br>

Supongamos que estamos interesados en que el error tipo I de $k$ coeficientes sea, conjuntamente, $1 - \alpha$. Una forma común de lograrlo es establecer el error tipo I de cada coeficiente en $\alpha / k$ (o, equivalentemente, multiplicar el valor p por $k$). Sin embargo, esto solo es exacto si los coeficientes son independientes.

En un contraste estándar, cada coeficiente $\beta_i$ posee un conjunto de confianza $C_i(\alpha)$ de manera que

<br>

$$
\begin{align}
P(\beta_i \in C_i(\alpha)) &= 1 - \alpha \\\\
P(\beta_i \notin C_i(\alpha)) &= \alpha \\\\
P\left( \bigcup_i^k \beta_i \notin C_i(\alpha) \right) &= \sum_{i}^k P(\beta_i \notin C_i(\alpha)) - P(\text{Conjunto de intersecciones en } \beta) \\\\
&= k \alpha - P(\text{Conjunto de intersecciones en } \beta) \\\\
&\leq k \alpha \\\\
\end{align}
$$

$$
\begin{align}
1 - P\left( \bigcup_i^k \beta_i \notin C_i(\alpha) \right) &\leq 1 - k \alpha \\\\
1 - P\left( \bigcup_i^k \beta_i \notin C_i(\alpha / k) \right) &\leq 1 - \alpha
\end{align}
$$

<br>

Por tanto, tras dividir por el número de contrastes $k$, la probabilidad de que, al menos, un solo parámetro no se encuentre en su intervalo de confianza es igual o inferior a $\alpha$. Por esta razón, la correción de Bonferroni es conservadora.

Esta corrección tan solo exige que

$$
C(\alpha) = \prod_{i}^k C(\alpha_i / k)
$$

o equivalentemente,

$$
\alpha = \sum_{i}^k \alpha_i/k
$$

Por tanto, es legítimo asumir distintos niveles de error tipo I $(\alpha/k)$ en cada contraste siempre que la suma de ellos equivalga a $\alpha$.

<br>

## Elipsoides de confianza

<br>

Si nuestros coeficientes siguen una diitribución normal y son independientes unos de otros,

$$
\begin{align}
\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]} \sim N(0, 1) \\\\
\sum_{i=1}^k \left(\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]}\right)^2 \sim \chi^2_k \\\\
\end{align}
$$

El intervalo $1 - \alpha$ de confianza vendría dado por

<br>

$$
P\left(\sum_{i=1}^k \left(\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]}\right)^2 \leq \;\;  \chi^2_{k, 1-\alpha} \right) = 1 - \alpha
$$

Esta región de confianza es un elipsoide.

Sin embargo, si los coeficientes covarían y la matriz de varianzas-covarianzas de los coeficientes es $\boldsymbol{\Sigma}$, entonces

<br>

$$
\begin{align}
\boldsymbol{\Sigma} &= \mathbf{VUV}^\prime \\\\
\boldsymbol{\Sigma}^{1/2} &= \mathbf{VU}^{1/2} \mathbf{V}^\prime \\\\
\text{VAR}\left[ \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \right] &= 
\boldsymbol{\Sigma}^{-1/2} \, \text{VAR}\left[ \boldsymbol{\widehat\beta - \boldsymbol{\beta}} \right] \, \left(\boldsymbol{\Sigma}^{-1/2}\right)^\prime \\\\
&= \left(\mathbf{VU}^{1/2} \mathbf{V}^\prime\right)^{-1} \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\left(\mathbf{VU}^{1/2} \mathbf{V}^\prime\right)^{-1}\right)^\prime \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\mathbf{VU}^{-1/2} \mathbf{V}^\prime\right)^\prime \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\mathbf{VU}^{-1/2} \mathbf{V}^\prime\right)^\prime \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \mathbf{VU}^{-1/2} \mathbf{V}^\prime \\\\
&= \mathbf{VU}^{-1/2} \mathbf{UU}^{-1/2} \mathbf{V}^\prime \\\\
&= \mathbf{VV}^\prime \\\\
&= \mathbf{I}
\end{align}
$$

$$
\begin{align}
\boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim MVN(\mathbf{0}, \mathbf{I}) \\\\
\left(\boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})\right)^\prime \; \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k \\\\
(\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \; \left(\boldsymbol{\Sigma}^{-1/2} \right)^\prime \; \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k \\\\
(\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k
\end{align}
$$

<br>

$$
P\left( (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \, \leq \,  \chi^2_{k, 1-\alpha} \right) = 1 - \alpha
$$

Si se utiliza $\widehat\sigma^2$ para estimar el error típico, entonces debemos emplear la distribución $F$:

$$
P\left( (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \ \boldsymbol{\widehat\Sigma}^{-1} \ (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \, \leq \,  F_{q, \, n-p-1, (1-\alpha)} \right) = 1 - \alpha
$$

<br>

## Interacciones

<br>

Si la verdadera función de regresión es $\mathbb{E}\left[Y \, | \, X_1, X_2\right] = \mu(X_1, X_2)$, que incluye una interacción de forma desconocida entre las variables $X_1$ y $X_2$, ¿por qué representar la interacción entre $X_1$ y $X_2$ como $X_1X_2$?

<br>

$$
Y = \beta_0 + X_1 \beta_1 + X_2 \beta_2 + X_1X_2\beta_3 + \epsilon
$$

<br>

Si hiciéramos una expansión de Taylor,

<br>

$$
      \begin{align}
\mu(x_1, x_2) \approx \mu(x_1^*, x_2^*) &+ \sum_{n=1}^\infty \frac{1}{n!} (x_1 - x_1^*)^n \, \left. \frac{\partial^n \mu}{\partial x_1^n} \right|_{x_1 = x_1^*} + \\
&+ \sum_{n=1}^\infty \frac{1}{n!} (x_2 - x_2^*)^n \, \left. \frac{\partial^n \mu}{\partial x_{2}^n} \right|_{x_2 = x_2^*} + \\
&+ (x_1 - x_1^*) (x_2 - x_2^*) \left. \frac{\partial^2 \mu}{\partial x_1 x_2} \right|_{x_1x_2 = x_1^*x_2^*} + \\
&+ \frac{1}{2} (x_1 - x_1^*)^2 (x_2 - x_2^*) \left. \frac{\partial^2 \mu}{\partial x_1^2 x_2} \right|_{x_1x_2 = x_1^*x_2^*} + \\
&+ \frac{1}{2} (x_1 - x_1^*) (x_2 - x_2^*)^2 \left. \frac{\partial^2 \mu}{\partial x_1 x_2^2} \right|_{x_1x_2 = x_1^*x_2^*}
\end{align}
$$

<br>

entonces podemos comprobar que una buena aproximación incluye el producto entre las variables $X_1$ y $X_2$ así como sus términos cuadráticos.

<br>

## *Leverage*

<br>

$$
\widehat{m}(x^*) = \bar{y} + \frac{1}{n} \sum_{i=1}^n \frac{(x_i - \bar{x}) (y_i - \bar{y})}{s_x^2} (x^* - \bar{x})
$$

<br>

Cuanto más se distancia $x_i$ respecto a $\bar{x}$, mayor es su contribución a $\widehat{\beta}_1$ y, por tanto, a $\widehat{m}(x^*)$. Esto quiere decir que el modelo tiende a conceder más peso a las realizaciones $y$ cuyo predictor $x$ se aleja más de su media. Si $x_i = \bar{x}$, entonces $y_i$ solo contribuye a $\bar{y}$ (la intersección) y no a la pendiente. 
Visto de otra manera,

<br>

$$
\sum_{i=1}^n e_i(x_i - \bar{x}) = 0
$$

<br>

Cuando $x_i - \bar{x}$ es grande, el residuo y su minimización adquieren más importancia.

La tasa de cambio del valor ajustado $\widehat{m}_i$ respecto a $y_i$ viene dado por

<br>

$$
\frac{\partial \widehat{m}_i}{\partial y_i} = \mathbf{H}_{ii}
$$

<br>

Es decir, $\mathbf{H}_{ii}$ es la influencia de $y_i$ en su propio valor ajustado y nos dice cúanto de $\widehat{m}_i$ es verdaderamente $y_i$ (las filas de $\mathbf{H}$ suman 1). Por tanto, a $\mathbf{H}_{ii}$ se le conoce como el **leverage** de $x_i$.

<br>

$$
\mathbf{H} \mathbf{1} = \mathbf{1}
$$

<br>

El leverage promedio es $\frac{p + 1}{n}$ ya que $\text{tr}\left(\mathbf{H}\right) = p + 1$ y $\mathbf{H}$ es una matrix $n \times n$.

La función de regresión es propensa a ajustar los puntos de mayor *leverage*. Cuanto más cercano sea al valor 1, más se ajusta $\widehat{m}_i$ a $y_i$.

Adicionalmente, $x_i - \bar{x}$ tendrá un mayor *leverage* si la varianza de dicho predictor es menor como se puede ver en

<br>

$$
\widehat{m}(\mathbf{x_i}) = \bar{y} + \frac{1}{n} (\mathbf{x_i} - \mathbf{\bar{x}}) \text{VAR}\left[\mathbf{X_c}\right]^{-1} \mathbf{X_c}^\prime \mathbf{y_c}
$$

<br>

donde $\mathbf{x}_i$ es el vector de predictores en la fila $i$, $\mathbf{X_c}$ representa la matriz de diseño centrada sin la columna para la intersección y $\mathbf{y_c}$ es el vector $\mathbf{y}$ centrado.

Nota: los valores ajustados del modelo pueden interpretarse como una distancia de Mahalanobis,

<br>

$$
\widehat{m}\left(\mathbf{X}\right) = \bar{y} + \mathbf{X_c} \text{VAR}\left[\mathbf{X_c}\right]^{-1} \mathbf{X_c}^\prime \mathbf{y_c}
$$

<br>

## Residuos estandarizados

<br>

El problema de que la función de regresión tienda a ajustar mejor aquellas realizaciones cuyos predictores tengan un considerable *leverage* es que la varianza de los residuos sea menor,

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{e}\right] &= \sigma^2 \left(\mathbf{I} - \mathbf{H} \right) \\\\
\text{VAR}\left[e_i\right] &= \sigma^2 \left(1 - \mathbf{H}_{ii} \right)
\end{align}
$$

<br>

Por esta razón, se suelen considerar los residuos estandarizados

<br>

$$
r_i = \frac{e_i}{\widehat\sigma \sqrt{1 - \mathbf{H}_{ii}}}
$$

<br>

Aunque $r_i$ resulta de la división entre una variable normalmente distribuida y otra variable con distribución $\sqrt{\chi^2}$, $e_i$ y $\widehat\sigma$ no son estadísticamente independientes: la segunda se estima empleando la primera. En realidad,

<br>

$$
\frac{r_i^2}{n-p-1} \sim \beta\left(\frac{1}{2}, \frac{n-p-2}{2} \right)
$$

<br>

## Residuos de validación cruzada

<br>

Si eliminamos del modelo de regresión la realización $y_i$, entonces el hipotético valor ajustado sería

<br>

$$
\widehat{m}^{-i} = \frac{(\mathbf{Hy})_i - \mathbf{H}_{ii} y_i}{1 - \mathbf{H}_{ii}}
$$

<br>

El residuo de validación cruzada es

<br>

$$
e_i^{(-i)} = y_i - \widehat{m}^{(-i)}
$$

<br>

Y su estadístico es

<br>

$$
\begin{align}
t_i &= \frac{e_i}{\widehat\sigma_{(-i)} \sqrt{1 - \mathbf{H}_{ii}}} \\\\
&= r_i \sqrt{\frac{n - p -2}{n - p -1 - r_i^2}} \\\\
&\sim t_{n-p-2}
\end{align}
$$

<br>

## Distancia de Cook

<br>

$$
\begin{align}
D_i &= \frac{\left\| \widehat{\mathbf{m}} - \widehat{\mathbf{m}}^{(-i)} \right\|^2}{\widehat\sigma^2(p+1)} \\\\
&= \frac{e_i^2 \ \mathbf{H}_{ii}}{\widehat\sigma^2 (p+1) \ (1 - \mathbf{H}_{ii})^2} \\\\
&\sim F_{p+1, \ n-p-1}
\end{align}
$$

<br>

Es decir, la influencia total de un predictor en los valores ajustados depende de su *leverage* y correspondiente residuo.

También podemos obtener una expresión de la estimación de $\beta$ ante la ausencia de la realización $i$:

<br>

$$
\widehat{\beta}^{(-i)} = \widehat{\beta} - \frac{(\mathbf{X}^\prime \mathbf{X})^{-1} \ \mathbf{x}_i^\prime \ e_i}{1 - \mathbf{H}_{ii}} \\\\
$$

<br>

Lo que nos permite expresar $D_i$ como

<br>

$$
D_i = \left(\widehat{\beta}^{(-i)} - \widehat{\beta}\right)^\prime \ \frac{\mathbf{X}^\prime \mathbf{X}}{(p + 1) \widehat{\sigma}^2}  \left(\widehat{\beta}^{(-i)} - \widehat{\beta}\right)
$$

<br>

## Regresión lineal robusta

<br>

$$
\widehat{\beta} = \underset{\mathbf{b}}{\text{argmin}} \ \frac{1}{n} \sum_{i=1}^n p(y_i - \mathbf{x}_i \mathbf{b})
$$

<br>

$p$ es una *loss function*. Si $p(u) = |u|$, entonces incurrimos en la estimación por desviación absoluta y si $p(u) = u^2$, entonces la estimación es por mínimos cuadrados ordinarios.

El *loss function* de Huber viene definido por

$$
p(u) = \begin{cases} 
      u^2 & |u| \leq c \\
      2c|u| - c^2 & |u| > c
\end{cases}
$$

<br>

## Selección de modelos

<br>

Si repetimos el experimento empleando los mismos valores de los predictores ($\mathbf{X}$), Las nuevas realizaciones de la variable $Y$ son:

$$
Y^\prime = \mathbf{X} \boldsymbol{\beta} + \epsilon^\prime
$$

<br>

La esperanza del error cuadrático medio fuera de la muestra sería

<br>

$$
\mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \left(Y_i^\prime - \widehat{m}_i\right)^2 \right]
$$

<br>

$$
\begin{align}
\mathbb{E}\left[ \left(Y_i - \widehat{m}_i \right)^2 \right] &= \text{VAR}\left[ Y_i - \widehat{m}_i \right] + \left( \mathbb{E} \left[Y_i - \widehat{m}_i \right] \right)^2 \\\\
&= \text{VAR}\left[ Y_i \right] + \text{VAR}\left[ \widehat{m}_i \right] - 2 \ \text{COV}\left[ Y_i, \widehat{m}_i \right] + \left( \mathbb{E} \left[Y_i - \widehat{m}_i \right] \right)^2 \\\\
\mathbb{E}\left[ \left(Y_i^\prime - \widehat{m}_i \right)^2 \right] &= \text{VAR}\left[ Y_i^\prime - \widehat{m}_i \right] + \left( \mathbb{E} \left[Y_i^\prime - \widehat{m}_i \right] \right)^2 \\\\
&= \text{VAR}\left[ Y_i^\prime \right] + \text{VAR}\left[ \widehat{m}_i \right] + \left( \mathbb{E} \left[Y_i^\prime - \widehat{m}_i \right] \right)^2
\end{align}
$$

<br>

$\text{COV}\left[ Y_i^\prime, \widehat{m}_i \right]$ es necesariamente $0$ porque $\widehat{m}_i$ es independiente de $Y^\prime$.

<br>

$$
\begin{align}
\mathbb{E}\left[ Y_i \right] &= \mathbb{E}\left[ Y_i^\prime \right] \\\\
\text{VAR}\left[ Y_i \right] &= \text{VAR}\left[ Y_i^\prime \right] \\\\
\mathbb{E}\left[ \left(Y_i^\prime - \widehat{m}_i \right)^2 \right] &= \mathbb{E}\left[ \left(Y_i - \widehat{m}_i \right)^2 \right] + 2 \ \text{COV}\left[ Y_i, \widehat{m}_i \right]
\end{align}
$$

<br>

Ahora bien,

<br>

$$
\begin{align}
\text{COV}\left[ Y_i, \widehat{m}_i \right] &= \sigma^2 \mathbf{H}_{ii} \\\\
\mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \left(Y_i^\prime - \widehat{m}_i \right)^2 \right] &= \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \left(Y_i - \widehat{m}_i \right)^2 \right] + \frac{2}{n} \sigma^2 (p+1)
\end{align}
$$

<br>

$\frac{2}{n} \sigma^2 (p+1)$ se conoce como el **optimismo** del modelo porque es la cantidad en la que el error cuadrático medio de la muestra infraestima el error cuadrático medio de futuras realizaciones de $Y^\prime$.

Si tuviéramos una expresión adecuada del optimismo, tendríamos un estimador insesgado del error cuadrático medio de futuras realizaciones.

<br>

### Estadístico $C_p$ de Mallow

<br>

$$
C_p = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \widehat{m}_i \right)^2 + \frac{2}{n} \widehat\sigma^2 (p+1)
$$

<br>

$\widehat\sigma^2$ se corresponde con la estimación del error cuadrático medio del modelo más general entre los que se comparan. $\widehat\sigma^2$ es un estimador insesgado de $\sigma^2$ si dicho modelo subsume el verdadero. La idea es elegir el modelo con menor $C_p$.

<br>

### $R^2$

<br>

Maximizar $R^2$ es equivalente a minimizar el error cuadrático medio ya que 

<br>

$$
R^2 = 1 - \frac{\bar{e^2}}{s_y^2}
$$

<br>

Sin embargo,

<br>

$$
\begin{align}
R^2_{\text{adj}} &= 1 - \frac{\widehat\sigma^2}{\hat{s}_y^2} \\\\
&= 1 - \frac{\bar{e^2}}{\hat{s}_y^2} \ \frac{n}{n-p-1}
\end{align}
$$

<br>

En este caso, nuestra intención sería minimizar

<br>

$$
\bar{e^2} \ \frac{n}{n-p-1} = \bar{e^2} \ \frac{1}{1 - (p+1)/n}
$$

<br>

Empleando el teorema binomial para conseguir una aproximación de primer orden,

<br>

$$
\begin{align}
\bar{e^2} \ \frac{1}{1 - (p+1)/n} &\approx \bar{e^2} \ \left( 1 + \frac{p+1}{n} \right) \\\\
&\approx \bar{e^2} + \bar{e^2} \left(\frac{p+1}{n} \right)
\end{align}
$$

<br>

Es decir, $R^2_{\text{adj}}$ utiliza la misma penalización que $C_p$ pero $2$ veces menor, como máximo.

<br>

### AIC

<br>

Digamos que $\ell(\widehat\theta)$ es la esperanza del logaritmo de la verosimilitud de una nueva realización bajo el vector estimado $\widehat\theta$. Si $\theta^*$ es el verdadero vector de parámetros, una expansión de Taylor indica que

$$
\ell(\widehat\theta) \approx \ell(\theta^*) + (\widehat\theta - \theta^*)^\prime \nabla \ell(\theta^*) + \frac{1}{2} (\widehat\theta - \theta^*)^\prime \ \nabla \nabla \ell(\theta^*) \ (\widehat\theta - \theta^*)
$$

<br>

La esperanza del logaritmo de la verosimilitud es maximizada por el vector de parámetros verdaderos, por lo que

<br>

$$
\begin{align}
\nabla \ell(\theta^*) &= 0 \\\\
\ell(\widehat\theta) &\approx \ell(\theta^*) + \frac{1}{2} (\widehat\theta - \theta^*)^\prime \ \nabla \nabla \ell(\theta^*) \ (\widehat\theta - \theta^*)
\end{align}
$$

<br>

Si definimos la verdadera matriz hessiana como $\mathbf{h}$, entonces

$$
\begin{align}
\nabla \nabla \ell(\theta^*) &= \mathbf{h} \\\\
\ell(\widehat\theta) &\approx \ell(\theta^*) + \frac{1}{2} (\widehat\theta - \theta^*)^\prime \ \mathbf{h} \ (\widehat\theta - \theta^*)
\end{align}
$$

<br>

Ahora pensemos en la expansión de Taylor de la derivada del logaritmo de la verosimilitud evaluada en el vector estimado de parámetros, que por definición equivale a 0.

<br>

$$
\begin{align}
\nabla L(\widehat\theta) \approx \nabla L(\theta^*) + (\widehat\theta - \theta^*)^\prime \ \nabla \nabla L(\theta^*) \\\\
\mathbf{0} \approx \nabla L(\theta^*) + (\widehat\theta - \theta^*)^\prime \ \nabla \nabla L(\theta^*)
\end{align}
$$

<br>

$\nabla \nabla L(\theta^*)$ es la matriz hessiana evaluada en el vector de parámetros, así que, si la definimos con $\mathbf{H}$,

<br>

$$
\begin{align}
\mathbf{0} &\approx \nabla L(\theta^*) + (\widehat\theta - \theta^*)^\prime \ \mathbf{H} \\\\
\widehat\theta &\approx \theta^* - \nabla L(\theta^*) \ \mathbf{H}^{-1}
\end{align}
$$

<br>

Si sustituimos esta última expresión de $\widehat\theta$ por $\widehat\theta$ en

<br>

$$
\ell(\widehat\theta) \approx \ell(\theta^*) + \frac{1}{2} (\widehat\theta - \theta^*)^\prime \ \mathbf{h} \ (\widehat\theta - \theta^*)
$$

<br>

entonces,

<br>

$$
\begin{align}
\ell(\widehat\theta) &\approx \ell(\theta^*) + \frac{1}{2} \left( \theta^* - \nabla L(\theta^*) \ \mathbf{H}^{-1} - \theta^* \right)^\prime \ \mathbf{h} \ \left( \theta^* - \nabla L(\theta^*) \ \mathbf{H}^{-1} - \theta^* \right) \\\\
&\approx \ell(\theta^*) + \frac{1}{2} \left( \nabla L(\theta^*) \ \mathbf{H}^{-1} \right)^\prime \ \mathbf{h} \ \left( \nabla L(\theta^*) \ \mathbf{H}^{-1} \right) \\\\
&\approx \ell(\theta^*) + \frac{1}{2} \nabla L(\theta^*) \ \mathbf{H}^{-1} \ \mathbf{h} \ \mathbf{H}^{-1} \ \nabla L(\theta^*)
\end{align}
$$

<br>

Si a continuación tomamos esperanzas, sabiendo que $\text{VAR} \left[ \nabla L(\theta^*) \right] = - \mathbf{h}$ según la **identidad de Fisher**, entonces

<br>

$$
\begin{align}
\mathbb{E} \left[ \ell(\widehat\theta) \right] &\approx \ell(\theta^*) - \frac{1}{2} \ \mathbf{h}^{-1} \ \mathbf{h} \\\\
&\approx \ell(\theta^*) - \frac{1}{2} \ \text{tr} \ \mathbf{I}
\end{align}
$$

<br>

Volvamos a hacer una expansión de Taylor, esta vez del logaritmo de la verosimilitud del verdadero vector de parámetros.

<br>

$$
\begin{align}
L(\theta^*) &\approx L(\widehat\theta) + (\theta^* - \widehat\theta)^\prime \ \nabla L(\widehat\theta) + \frac{1}{2} (\theta^* - \widehat\theta)^\prime \ \nabla \nabla L(\widehat\theta) \ (\theta^* - \widehat\theta) \\\\
&\approx L(\widehat\theta) + \frac{1}{2} (\theta^* - \widehat\theta)^\prime \ \nabla \nabla L(\widehat\theta) \ (\theta^* - \widehat\theta) \\\\
&\approx L(\widehat\theta) + \frac{1}{2} \left( \theta^* + \nabla L(\theta^*) \ \mathbf{H}^{-1} - \theta^* \right)^\prime \ \nabla \nabla L(\widehat\theta) \ \left( \theta^* + \nabla L(\theta^*) \ \mathbf{H}^{-1} - \theta^* \right) \\\\
&\approx L(\widehat\theta) + \frac{1}{2} \left( \nabla L(\theta^*) \ \mathbf{H}^{-1} \right)^\prime \ \nabla \nabla L(\widehat\theta) \ \left( \nabla L(\theta^*) \ \mathbf{H}^{-1} \right) \\\\
&\approx L(\widehat\theta) + \frac{1}{2} \nabla L(\theta^*) \ \mathbf{H}^{-1} \ \nabla \nabla L(\widehat\theta) \ \mathbf{H}^{-1} \ \nabla L(\theta^*)
\end{align}
$$

<br>

Según la identidad de Fisher, $\text{VAR} \left[ \nabla L(\theta^*) \right] = - \mathbf{h}$, y volviendo a tomar esperanzas,

<br>

$$
\begin{align}
\mathbb{E} \left[ \nabla \nabla L(\widehat\theta) \right] &= \mathbb{E} \left[\nabla \nabla L(\theta^*) \right] = \nabla \nabla \ell(\theta^*) = \mathbf{h} \\\\
\ell(\theta^*) &\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \frac{1}{2} \ \mathbf{h}^{-1} \ \mathbf{h} \\\\
&\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \frac{1}{2} \ \text{tr} \ \mathbf{I}
\end{align}
$$

<br>

Por último, combinando las expresiones

<br>

$$
\begin{align}
\ell(\theta^*) &\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \frac{1}{2} \ \text{tr} \ \mathbf{I} \\\\
\mathbb{E} \left[ \ell(\widehat\theta) \right] &\approx \ell(\theta^*) - \frac{1}{2} \ \text{tr} \ \mathbf{I}
\end{align}
$$

<br>

obtenemos

<br>

$$\begin{align}
\mathbb{E} \left[ \ell(\widehat\theta) \right] &\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \frac{1}{2} \ \text{tr} \ \mathbf{I} - \frac{1}{2} \ \text{tr} \ \mathbf{I} \\\\
\mathbb{E} \left[ \ell(\widehat\theta) \right] &\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \text{tr} \ \mathbf{I}
\end{align}
$$

<br>

Por tanto, si nuestro modelo es correcto, una estimación insesgada de $\ell(\widehat\theta)$ es 

<br>

$$
\text{AIC} = L(\widehat\theta) - \text{número de parámetros estimados}
$$

<br>

En este caso, $AIC/n$ se corresponde con la esperanza del logaritmo de la verosimilitud de una nueva realización bajo el modelo estimado si dicho modelo es correcto.

Sin embargo, si no hemos estimado el modelo correcto, entonces $\text{VAR}\left[ \nabla L(\theta^*) \right] \neq - \nabla\nabla \ \ell(\theta^*) \neq - \mathbf{h}$.

<br>

### Equivalencias

<br>

Supongamos que $L(\widehat\Theta)$ es el logaritmo de la verosimilitud del modelo más general y el verdadero mientras que $L(\widehat\theta)$ es el de un modelo más simple e incorrecto. Bajo el modelo lineal normal,

<br>

$$
\begin{align}
L(\widehat\Theta) &= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log \text{MSE}_{\ \widehat\Theta}} \\\\
L(\widehat\theta) &= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log \text{MSE}_{\ \widehat\theta}} \\\\
\Delta \ \text{AIC} &= L(\widehat\Theta) - L(\widehat\theta) - (p_{\Theta} - p_{\theta}) \\\\
&= \frac{n}{2}{\log \text{MSE}_{\ \widehat\theta}} - \frac{n}{2}{\log \text{MSE}_{\ \widehat\Theta}} - (p_{\Theta} - p_{\theta}) \\\\
&= \frac{n}{2} \log \left( \frac{\text{MSE}_{\ \widehat\theta}}{\text{MSE}_{\ \widehat\Theta}} \right) - (p_{\Theta} - p_{\theta}) \\\\
&= \frac{n}{2} \log \left( 1 + \frac{\text{MSE}_{\ \widehat\theta} - \text{MSE}_{\ \widehat\Theta}}{\text{MSE}_{\ \widehat\Theta}} \right) - (p_{\Theta} - p_{\theta}) \\\\
&\approx \frac{n}{2} \ \frac{\text{MSE}_{\ \widehat\theta} - \text{MSE}_{\ \widehat\Theta}}{\text{MSE}_{\ \widehat\Theta}} - (p_{\Theta} - p_{\theta})
\end{align}
$$

<br>

Dado que $\Theta$ es el modelo verdadero,

<br>

$$
\begin{align}
\Delta \ \text{AIC} &\approx \frac{n}{2} \ \frac{\text{MSE}_{\ \widehat\theta} - \text{MSE}_{\ \widehat\Theta}}{\widehat\sigma^2} - (p_{\Theta} - p_{\theta}) \\\\
\frac{2 \widehat\sigma^2}{n} \Delta \ \text{AIC} &\approx \text{MSE}_{\ \widehat\theta} - \text{MSE}_{\ \widehat\Theta} + \frac{2 \widehat\sigma^2}{n} (p_{\Theta} - p_{\theta}) \\\\
&\approx \Delta \ C_p
\end{align}
$$

<br>

Por tanto, si uno de los modelos es el verdadero, entonces escoger el modelo que maximiza el $AIC$ o el que minimiza el $C_p$ convergerá, aproximadamente, en el mismo resultado.

La diferencia es que $AIC$ puede aplicarse en cualquier modelo mientras que el $C_p$ solo tiene sentido cuando se emplea el error cuadrático medio.

<br>

### Validación cruzada *Leave-one-out* (LOOCV)

<br>

$$
\begin{align}
\text{LOOCV} &= \frac{1}{n} \sum_{i=1}^n \left( Y_i - \widehat{m}_i^{(-i)} \right)^2 \\\\
&= \frac{1}{n} \sum_{i=1}^n \left( \frac{Y_i - \widehat{m}_i}{1 - \mathbf{H}_{ii}} \right)^2 \\\\
&\approx \frac{1}{n} \sum_{i=1}^n \left( Y_i - \widehat{m}_i \right)^2 \ \left(1 + \mathbf{H}_{ii} \right)^2 \\\\
&\approx \frac{1}{n} \sum_{i=1}^n \left( Y_i - \widehat{m}_i \right)^2 \ \left(1 + 2 \ \mathbf{H}_{ii} \right) \\\\
&\approx \text{MSE} + 2 \ \text{MSE} \ \text{tr} \ \mathbf{H}
\end{align}
$$

<br>

Cuando $n \rightarrow \infty$, LOOCV, $C_p$ y AIC convergerán.

De la misma manera, a medida que $n \rightarrow \infty$ y si el modelo verdadero se encuentra entre los que se comparan, todos estos métodos tenderán a elegir un modelo con más parámetros que el verdadero.

LOOCV, $C_p$ y AIC son estimadores insesgados del error de generalización pero no penalizan la varianza de las estimaciones que incrementa con la adición de más parámetros al modelo.

<br>

### BIC

<br>

$$
\text{BIC} = L(\widehat\theta) - \frac{\log n}{2} \cdot \text{número de parámetros estimados}
$$

<br>

A diferencia de los otros métodos, a medida que $n \rightarrow \infty$ y asi el modelo verdadero se encuentra entre los comparados, BIC tenderá a seleccionar el modelo verdadero.

<br>

## Distribución de los valores p bajo la hipótesis nula

<br>

$$
\begin{align}
P &= F(T) \\\\
\text{Pr}(P < p) &= \text{Pr}\left(F^{-1}(P) < F^{-1}(p)\right) \\\\
&= \text{Pr}(T < t) \\\\
&= F\left(F^{-1}(p)\right) \\\\
&= p
\end{align}
$$

<br>

## Desigualdad de Chebyshev

<br>

$$
\text{P}(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}
$$

La probabilidad de que una variable aleatoria $X$ exceda su valor esperado $\mu$ por $k$ errores típicos es siempre inferior a $\frac{1}{k^2}$.

<br>




## *Capture percentage*

<br>

La esperanza de cobertura del intervalo de confianza para la distribución de $\widehat\beta_0$ o lo que es lo mismo, el promedio de veces que un intervalo de confianza contiene otras estimaciones de $\beta_0$, es:

<br>

$$
\int \phi\left(\widehat\beta_0; \, \beta_0, \text{se}\right) \left(\Phi\left(\widehat\beta_0 + 1.96 \cdot \text{se}; \, \beta_0, \text{se}\right) - \Phi\left(\widehat\beta_0 - 1.96 \cdot \text{se}; \, \beta_0, \text{se}\right)\right) d\widehat\beta_0
$$

<br>


















