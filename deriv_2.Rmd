---
title: "Estimación"
output: 
  html_document:
    code_folding: hide
    css: style.css
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

## Distribución normal

<br>

$$
\begin{align}
\mathbf{x} &= x_i, \dots, x_n \\\\
P(\mathbf{x}; \mu, \sigma) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
L(\mu, \sigma; \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &= \sum_{i=1}^n -\frac{(x_i - \mu)^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi}) \\\\
\text{log}(L) &= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi)
\end{align}
$$

<br>

*Location-scale property*

<br>

$$
\begin{align}
Z &\sim N(\mu, \sigma^2) \\\\
a + bZ &\sim N(a + \mu, b^2\sigma^2)
\end{align}
$$

<br>

*Stability property*

<br>

Si $Z_{IID} \sim N(\mu, \sigma^2)$, entonces

$$
\sum_{i=1}^n Z_i \sim N\left(\sum_{i}^n \mu, \sum_{i=1}^n \sigma^2\right)
$$

<br>

## Estimación de $\mu$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu}&= \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) \\\\
&= \frac{n}{\sigma^2} \, (\bar{x} - \mu)
\end{align}
$$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu} &= 0 \\\\
\widehat\mu &= \bar{x} \\\\
\end{align}
$$

<br>

## Valor esperado de $\bar{x}$

<br>

$$
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n x_i \right] = \mu
$$

<br>

## Varianza de $\widehat\mu$

<br>

$$
\begin{align}
\frac{\partial^2 \text{log}(L)}{\partial \mu^2} &= - \frac{n}{\sigma^2} \\\\
\text{VAR}\left[\widehat\mu\right] &= \mathbb{E}\left[(\widehat\mu - \mu)^2\right] \\\\
&= \frac{\sigma^2}{n}
\end{align}
$$

<br>

## Estimación de $\sigma^2$

<br>

$$
\begin{align}
\text{log}(L) &= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi) \\\\
\end{align}
$$

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= 0 \\\\
\sigma^2 &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{n}
\end{align}
$$

<br>

## Valor esperado de $\frac{\sum_{i=1}^n (x_i - \widehat\mu)^2}{n}$

<br>

$$
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &= \frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n ((x_i - \mu) - (\widehat\mu - \mu))^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2 + (\widehat\mu - \mu)^2 - 2(x_i - \mu)(\widehat\mu - \mu))\right] \\\\
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &= n\sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (\widehat\mu - \mu)^2\right] &= \sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)(\widehat\mu - \mu) \right] &= \mathbb{E}\left[n(\mu^2 + \bar{x}\widehat\mu - \bar{x}\mu - \widehat\mu\mu)\right] \\\\
&= \mathbb{E}\left[n(\mu^2 + \widehat\mu^2 - 2\widehat\mu\mu)\right] \\\\
&= n \, \mathbb{E}\left[(\widehat\mu - \mu)^2 \right] \\\\
&= \sigma^2
\end{align}
$$

<br>

$$
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &= \sigma^2 + \frac{\sigma^2}{n} - \frac{2\sigma^2}{n} \\\\
&= \sigma^2 \left(\frac{n - 1}{n}\right) \\\\
\end{align}
$$
$$
\begin{align}
\sigma^2 &= \frac{\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right]}{n - 1}
\end{align}
$$

<br>

$$
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n - 1}
$$

<br>

## Varianza de $\sigma^2$ y $\widehat\sigma^2$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial^2 \text{log}(L)}{\partial \sigma^4} &= - \frac{\sum_{i=1}^n (x_i - \mu)^2}{\sigma^6} + \frac{n}{2\sigma^4} \\\\
&= \frac{- 2\sum_{i=1}^n (x_i - \mu)^2 + n\sigma^2}{2\sigma^6} \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &= n\sigma^2 \\\\
\mathbb{E}\left[\frac{\partial^2 \text{log}(L)}{\partial \sigma^4}\right] &= \frac{- 2n\sigma^2 + n\sigma^2}{2\sigma^6} \\\\
&= - \frac{n}{2\sigma^4} \\\\
\text{VAR}\left[\sigma^2\right] &= \frac{2\sigma^4}{n}
\end{align}
$$

<br>

$$
\begin{align}
\frac{\epsilon_i}{\sigma} &\sim N(0, 1) \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 &\sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 = \frac{(n-1)\widehat\sigma^2}{\sigma^2} &\sim \chi^2_{n-1} \\\\
\text{VAR}\left[\chi^2_{n-1}\right] &= 2(n-1) \\\\
\text{VAR}\left[\frac{(n-1)\widehat\sigma^2}{\sigma^2}\right] &= 2(n-1) \\\\
\text{VAR}\left[(n-1)\widehat\sigma^2\right] &= 2\sigma^4(n-1) \\\\
\text{VAR}\left[\widehat\sigma^2\right] &= \frac{2\sigma^4}{n-1} \\\\
\end{align}
$$

<br>

## Función afín

<br>

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_i + \epsilon_i \\\\
y_i &= \widehat{\beta}_0 + \widehat{\beta}_1x_i + e_i
\end{align}
$$

<br>

$$
\begin{align}
P(\mathbf{y}; \beta_0, \beta_1, \sigma, \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
L(\beta_0, \beta_1, \sigma; \mathbf{y}, \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &= \sum_{i=1}^n -\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi})
\end{align}
$$

<br>

## Residuos

<br>

$$
\begin{align}
\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})e_i &= 0, \quad \text{COV}[X, e] = 0 \\\\
\frac{1}{n} \sum_{i=1}^n e_i &= 0
\end{align}
$$

<br>

## Estimación de $\beta_1$

<br>

$$
\begin{align}
\frac{\partial L}{\partial \beta_1} &= - \frac{\sum_{i=1}^n \beta_0x_i + \beta_1x_i^2 - y_ix_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_1} &= 0 \\\\
&= \overline{yx} - \beta_0\bar{x} - \beta_1\overline{x^2} \\\\
\end{align}
$$
$$
\begin{align}
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - \beta_0\mathbb{E}[X] &= 0 \\\\
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - (\mathbb{E}[Y] - \beta_1\mathbb{E}[X]) \, \mathbb{E}[X] &= 0 \\\\
\text{COV}[YX] - \beta_1\mathbb{E}[X^2] - \beta_1\mathbb{E}[X]^2 &= 0 \\\\
\text{COV}[YX] - \beta_1\text{VAR}[X] &= 0 \\\\
\end{align}
$$
$$
\begin{align}
\beta_1 &= \frac{\text{COV}[YX]}{\text{VAR}[X]} \\\\
\end{align}
$$

<br>

$$
\begin{align}
\frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} &= \frac{\sum_{i=1}^n x_i  y_i - \bar{x} \bar{y}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{\sum_{i=1}^n x_i (\beta_0 + \beta_1x_i + \epsilon_i) - \bar{x} (\beta_0 + \beta_1\bar{x} + \bar{\epsilon})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{n\beta_1(\overline{x^2} - \bar{x}^2)}{\sum_{i=1}^n (x_i - \bar{x})^2} + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_1 + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
\end{align}
$$

<br>

Según la ley de la esperanza total,

$$
\begin{align}
\mathbb{E}\left[\widehat{\beta}_1\right] &= \mathbb{E}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_i, \dots, x_n\right]\right] \\\\
&= \mathbb{E}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \mathbb{E}[\epsilon_i]}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \, , \quad \mathbb{E}[\epsilon \, | \, x_i] = 0 \\\\
&= \beta_1 \\\\
\widehat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{align}
$$

<br>

## Estimación de $\beta_0$

<br>

$$
\begin{align}
\frac{\partial L}{\partial \beta_0} &=- \frac{\sum_{i=1}^n \beta_0 + \beta_1x_i - y_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_0} &= 0 \\\\
\widehat{\beta}_0 &= \bar{y} - \widehat{\beta}_1\bar{x} \\\\
\mathbb{E}\left[\widehat{\beta}_0\right] &= \beta_0 + \beta_1\mathbb{E}[X] - \beta_1\mathbb{E}[X] \\\\
&= \beta_0
\end{align}
$$

<br>

## Varianza de $\widehat{\beta}_1$

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right] &= \text{VAR}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \\\\
&= \sum_{i=1}^n \frac{(x_i - \bar{x})^2 \, \text{VAR}\left[\epsilon_i\right]}{n^2 s_x^4} \\\\
&= \frac{\sigma^2}{n s_x^2}
\end{align}
$$

Aplicando la ley de la varianza total,

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_1\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2}{n s_x^2} \\\\
\widehat{\text{VAR}}\left[\widehat{\beta}_1\right] &= \frac{\widehat\sigma^2}{n s_x^2}
\end{align}
$$

Dado que $\epsilon_i$ sigue una distribución normal, la propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{\beta}_1 &\sim N\left(\beta_1, \frac{\sigma^2}{ns_x^2} \right) \\\\\
\frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} &\sim N\left(0, 1 \right) \\\\\
\end{align}
$$

Si conociéramos $\sigma^2$, entonces

$$
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
$$

Pero si estimamos $\sigma^2$ a través de $\widehat\sigma^2$, entonces

$$
\begin{align}
\frac{\widehat{\beta}_1 - \beta_1}{\widehat\sigma / s_x\sqrt{n}} &= \frac{\frac{\widehat{\beta}_1 - \beta_1}{\sigma}}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&\sim \frac{N(0, 1/ns_x^2)}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&\sim \frac{N(0, 1)}{\frac{\widehat\sigma}{\sigma}} \\\\
&\sim \frac{N(0, 1)}{\sqrt{\frac{\sum_{i=1}^n e_i^2}{\sigma^2 (n-2)}}} \\\\
&\sim \frac{N(0, 1)}{\sqrt{\frac{\chi^2_{n-2}}{n-2}}} \\\\
&\sim t_{n-2}
\end{align}
$$

<br>

## Varianza de $\widehat{\beta}_0$

<br>

$$
\begin{align}
\text{VAR}\left[ \widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &= \text{VAR}\left[\bar{y} - \widehat{\beta}_1\bar{x}\right] \\\\
&= \text{VAR}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon} - \widehat{\beta}_1\bar{x}\right] \\\\
&= \text{VAR}\left[\bar{\epsilon}\right] + \bar{x}^2 \, \text{VAR}\left[\widehat{\beta}_1\right] - 2\bar{x}^2\text{COV}\left[\bar{\epsilon}, \widehat{\beta}_1\right]
\end{align}
$$

<br>

$$
\text{COV}\left[\bar{\epsilon}, \widehat{\beta}_1\right] = 0
$$

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &= \frac{\sigma^2}{n} + \frac{\sigma^2 \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2} - \sigma^2 s_x^2}{n s_x^2} \\\\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2}}{n s_x^2} - \frac{\sigma^2}{n} \\\\
&= \frac{\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
$$

<br>

Según la ley de la esperanza total,

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_0\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2 \overline{x^2}}{n s_x^2} \\\\
\widehat{\text{VAR}}\left[\widehat{\beta}_0\right] &= \frac{\widehat\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
$$

Dado que $\epsilon_i$ sigue una distribución normal, la propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{\beta}_0 &\sim N\left(\beta_0, \frac{\sigma^2 \overline{x^2}}{n s_x^2} \right) \\\\\
\frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} &\sim N\left(0, 1 \right) \\\\\
\end{align}
$$

Si conociéramos $\sigma^2$, entonces

$$
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
$$

Pero si estimamos $\sigma^2$ a través de $\widehat\sigma^2$, entonces

$$
\frac{\widehat{\beta}_0 - \beta_0}{\widehat\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \sim t_{n-2}
$$

<br>

## Predicciones

<br>

$$
\begin{align}
\widehat{m}(x_0) &= \widehat{\beta}_0 + \widehat{\beta}_1x_0 \\\\
&= \bar{y} - \widehat{\beta}_1\bar{x} + \widehat{\beta}_1x_0 \\\\
&= \bar{y} + (x_0 - \bar{x})\widehat{\beta}_1 \\\\
&= \beta_0 + \beta_1\bar{x} + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x_0 - \bar{x})\left(\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\\\
&= \beta_0 + \beta_1\bar{x} + (x_0 - \bar{x})\beta_1 + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x_0 - \bar{x}) \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_0 + \beta_1x_0 + \frac{1}{n} \sum_{i=1}^n \epsilon_i \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2}\right)
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[\widehat{m}(x_0)\right] &= \beta_0 + \beta_1x_0 + \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\epsilon_i] \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2}\right) \\\\
&= \beta_0 + \beta_1x_0
\end{align}
$$

<br>

## Varianza del promedio de las predicciones

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n \right] &= \frac{1}{n^2} \sum_{i=1}^n \text{VAR}[\epsilon_i] \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2}\right)^2 \\\\
&= \frac{\sigma^2}{n^2} \sum_{i=1}^n \left(1 + (x_0 - \bar{x})^2 \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&= \frac{\sigma^2}{n^2} \left(n + (x_0 - \bar{x})^2 \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&= \frac{\sigma^2}{n^2} \left(n + (x_0 - \bar{x})^2 \frac{n}{s_x^2}\right) \\\\
&= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)
\end{align}
$$

Según la ley de la esperanza total,

$$
\begin{align}
\text{VAR}\left[\widehat{m}(x_0)\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)
\end{align}
$$

<br>

La propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{m}(x_0) &\sim N\left(\beta_0 + \beta_1x_0, \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)\right) \\\\
\frac{\widehat{m}(x_0) - m(x_0)}{\sqrt{\frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)}} &\sim N(0, 1) \\\\
\frac{\widehat{m}(x_0) - m(x_0)}{\sqrt{\frac{\widehat\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)}} &\sim t_{n-2} \\\\
\end{align}
$$

<br>

## Varianza del error de predicción

<br>

$$
\begin{align}
y_0 &= \beta_0 + \beta_1x_0 + \epsilon_0 \\\\
\widehat{m}(x_0) &= \widehat\beta_0 + \widehat\beta_1x_0 \\\\
y_0 - \widehat{m}(x_0) &= \beta_0 + \beta_1x_0 - \widehat\beta_0 - \widehat\beta_1x_0 + \epsilon_0 \\\\
\text{COV}[\widehat{m}(x_0), \epsilon_0] &= 0 \\\\
\text{VAR}[y_0 - \widehat{m}(x_0)] &= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right) + \sigma^2 \\\\
\text{VAR}[y_0 - \widehat{m}(x_0)] &= \sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)
\end{align}
$$

<br>

La propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{m}(x_0) &\sim N\left(y_0, \sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)\right) \\\\
\frac{y_0 - \widehat{m}(x_0)}{\sqrt{\sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)}} &\sim N(0, 1) \\\\
\frac{y_0 - \widehat{m}(x_0)}{\sqrt{\widehat\sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)}} &\sim t_{n-2}
\end{align}
$$

<br>

## Varianza del error de estimación (Varianza de un residuo)

<br>

$$
\begin{align}
\widehat{\beta}_0 &= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
$$

<br>

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_i + \epsilon_i \\\\
\text{VAR}\left[y_i\right] &= \sigma^2 \\\\
\widehat{m}(x_i) &= \widehat\beta_0 + \widehat\beta_1x_i \\\\
&= \bar{y} - \widehat\beta_1\bar{x} + \widehat\beta_1x_i \\\\
&= \bar{y} + (x_i - \bar{x})\widehat\beta_1 \\\\
&= \bar{y} + (x_i - \bar{x})\left(\beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
&= \beta_0 + \beta_1x_i + (x_i - \bar{x})\left(\frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
\text{VAR}\left[\widehat{m}(x_i)\right] &=  \frac{\sigma^2}{n} + (x_i - \bar{x})^2 \frac{\sigma^2}{ns_x^2} \\\\
&= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
\mathbb{E}\left[(y_i - \widehat{m}(x_i))^2\right] &= \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\,\text{COV}\left[y_i, \widehat{m}(x_i)\right]
\end{align}
$$

<br>

$$
\begin{align}
\text{COV}\left[y_i, \widehat{m}(x_i)\right] &= \text{COV}\left[y_i + \widehat{m}(x_i) - \widehat{m}(x_i), \widehat{m}(x_i) \right] \\\\
&= \text{COV}\left[\widehat{m}(x_i), \widehat{m}(x_i) \right] + \text{COV}\left[y_i - \widehat{m}(x_i), \widehat{m}(x_i) \right] \\\\
&= \text{COV}\left[\widehat{m}(x_i), \widehat{m}(x_i) \right] + \text{COV}\left[e_i, \widehat{m}(x_i) \right] \\\\
&= \text{VAR}\left[\widehat{m}(x_i)\right] + 0 \\\\
&= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[(y_i - \widehat{m}(x_i))^2\right] &= \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&= \sigma^2 - \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&= \sigma^2 \left(1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
$$

<br>

## Varianza del error cuadrático, $\sigma^2$

<br>

$$
\mathbb{E}\left[(Y - (\beta_0 + \beta_1X))^2\right] = \sigma^2
$$

<br>

$$
\begin{align}
\text{VAR}\left[\bar{y} \, | \, x_i, \dots, x_n\right] &= \text{VAR}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon}\right] \\\\
&= \frac{1}{n^2} \sum_{i=1}^n \text{VAR}\left[\epsilon_i\right] \\\\
&= \frac{\sigma^2}{n}
\end{align}
$$

<br>

$$
\begin{align}
\widehat{\beta}_0 &= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
$$

<br>

$$
\begin{align}
 \mathbb{E}\left[\sum_{i=1}^n e_i^2 \, | \, x_i, \dots, x_n\right] &= \mathbb{E}\left[\sum_{i=1}^n (y_i - \widehat{m}(x_i))^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n \left(\beta_0 + \beta_1x_i + \epsilon_i - \widehat{\beta}_0 - \widehat{\beta}_1x_i\right)^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n \left((\beta_0 - \widehat{\beta}_0)^2 + (\beta_1 - \widehat{\beta}_1)^2x_i^2 + \epsilon_i^2\right)\right] + \\\\
& 2 \, \mathbb{E}\left[\sum_{i=1}^n (\beta_0 - \widehat{\beta}_0)(\beta_1 - \widehat{\beta}_1)x_i + (\beta_0 - \widehat{\beta}_0)\epsilon_i + (\beta_1 - \widehat{\beta}_1)x_i\epsilon_i \right] \\\\
&= n \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2}}{n s_x^2}\right] + 1 \right) - 2\sigma^2 \, \mathbb{E}\left[\frac{\bar{x}^2}{s_x^2} \right] - 2 \sigma^2 - 2 \sigma^2 \\\\
&= \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2} - \bar{x}^2}{s_x^2}\right] + n - 4 \right) \\\\
&= \sigma^2 \left(n - 2 \right) \\\\
\end{align}
$$

<br>

$$
\widehat\sigma^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2 \\\\
$$

$\widehat{\sigma}^2$ es estadísticamente independiente de $\widehat\beta_1$ y $\widehat\beta_0$ a pesar de que todos los estimadores son una función de $\epsilon$.

<br>

Sabiendo que si $Z \sim N(0, 1)$, entonces $Z^2 \sim \chi^2$ y que $\frac{\epsilon_i}{\sigma} \sim N(0, 1)$, entonces

$$
\begin{align}
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 &= \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 \sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 &\sim \chi^2_{n-2} \\\\
P\left(\sigma^2 \leq \frac{(n-2) \widehat\sigma^2}{\chi^2_{n-2, \alpha}}\right) &= 1 - \alpha \\\\
\text{VAR}\left[\chi^2_{n-2}\right] &= 2(n-2)X \\\\
\text{VAR}\left[\frac{(n-2)\widehat\sigma^2}{\sigma^2}\right] &= 2(n-2) \\\\
\text{VAR}\left[(n-2)\widehat\sigma^2\right] &= 2\sigma^4(n-2) \\\\
\text{VAR}\left[\widehat\sigma^2\right] &= \frac{2\sigma^4}{n-2} \\\\
\end{align}
$$

<br>

## Modelo sin predictores

<br>

$$
\begin{align}
Y &= \beta_0 + \epsilon \\\\
\widehat\beta_0 &= \bar{y} \sim N\left(\beta_0, \frac{\sigma^2}{n}\right) \\\\
\widehat\sigma_0^2 &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \\\\
&= s_y^2 \\\\
\frac{(n-1) \widehat\sigma^2_0}{\sigma^2} &\sim \chi^2_{n-1}
\end{align}
$$

<br>

## El test F

<br>

La distribución $\chi^2$ procede de la suma del cuadrado de variables con distribución normal y la distribución $F$ procede de la razón entre variables aleatorias con distribución $\chi^2$.

$$
\frac{\chi^2_a}{\chi^2_b} \frac{b}{a} \sim F_{a, b}
$$

Según el modelo sin predictores, 

$$
\frac{(n-1) \widehat\sigma^2_0}{\sigma^2} \sim \chi^2_{n-1}
$$

mientras que según el modelo con un predictor,

$$
\frac{(n-2) \widehat\sigma_1^2}{\sigma^2} \sim \chi^2_{n-2}
$$

Por tanto,

$$
\begin{align}
\frac{(n-1) \widehat\sigma^2_0 - (n-2) \widehat\sigma_1^2}{\sigma^2} &\sim \chi^2_1 \\\\
\frac{(n-1) \widehat\sigma^2_0 - (n-2) \widehat\sigma_1^2}{(n-2) \widehat\sigma_1^2} &\sim \frac{\chi^2_1}{\chi^2_{n-2}} \\\\
\frac{(n-1) \widehat\sigma^2_0 - (n-2) \widehat\sigma_1^2}{(n-2) \widehat\sigma_1^2} \, \frac{n-2}{1} &\sim F_{1, n-2} \\\\
\frac{(n-1) \widehat\sigma^2_0 - (n-2) \widehat\sigma_1^2}{\widehat\sigma_1^2} &\sim F_{1, n-2}
\end{align}
$$

El test $F$ asume que los errores ($\epsilon$) siguen una distribución normal, son homocedásticos e independientes de la variable $X$ y entre sí. Se trata de un caso particular de la razón de verosimilitudes.

<br>

En el caso general, si estuviéramos contrastando la significación de $p-q$ estimaciones,

$$
\begin{align}
\frac{(n-q) \widehat\sigma^2_0 - (n-p) \widehat\sigma_1^2}{\sigma^2} &\sim \chi^2_{p-q} \\\\
\frac{(n-p) \widehat\sigma_1^2}{\sigma^2} &\sim \chi^2_{n-p} \\\\
\frac{(n-q) \widehat\sigma^2_0 - (n-p) \widehat\sigma_1^2}{(n-p) \widehat\sigma_1^2} &\sim \frac{\chi^2_{p-q}}{\chi^2_{n-p}} \\\\
\frac{(n-q) \widehat\sigma^2_0 - (n-p) \widehat\sigma_1^2}{(n-p) \widehat\sigma_1^2} \, \frac{n-p}{p-q} &\sim F_{p-q, n-p} \\\\
\frac{(n-q) \widehat\sigma^2_0 - (n-p) \widehat\sigma_1^2}{(p-q) \widehat\sigma_1^2} &\sim F_{p-q, n-p}
\end{align}
$$

<br>

## El test de razón de verosimilitudes

<br>

El espacio paramétrico del modelo nulo es un subconjunto del espacio paramétrico del modelo general.

$$
\begin{align}
\Lambda &= L(\widehat\Theta) - L(\widehat\theta) \\\\
\lim_{n \rightarrow \infty} \quad 2\Lambda &\sim \chi^2_{p-q}
\end{align}
$$

<br>

$$
\begin{align}
L(\widehat\theta) &= - \frac{n}{2} \log2\pi - \frac{n}{2} \log2s_0^2 - \frac{1}{2s_0^2} \sum_{i=1}^n (y_i - \widehat\beta_0)^2 \\\\
&= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log s_0^2} \\\\
L(\widehat\Theta) &= - \frac{n}{2} \log2\pi - \frac{n}{2} \log2s_1^2 - \frac{1}{2s_1^2} \sum_{i=1}^n (y_i - \widehat\beta_0 - \widehat\beta_1x_i)^2 \\\\
&= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log s_1^2}
\end{align}
$$

<br>

$$
\begin{align}
L(\widehat\beta_0, \widehat\beta_1, s^2_1) - L(\widehat\beta_0, s^2_0)
&= \frac{n}{2} \log\frac{s_0^2}{s_1^2} \\\\
n \log\frac{s_0^2}{s_1^2} &\sim \chi^2_1
\end{align}
$$

<br>

## $R^2$

<br>

$$
\begin{align}
R^2 &= \frac{\text{COV}\left[y, \widehat{m}\right]}{s_y^2} \\\\
\text{COV}\left[y, \widehat{m}\right] &= \text{COV}\left[\widehat{m} + e, \widehat{m}\right] \\\\
&= s_\widehat{m}^2 + \text{COV}\left[e, \widehat{m}\right] \\\\
&= s_\widehat{m}^2 \\\\
R^2 &= \frac{s_\widehat{m}^2}{s_y^2} \\\\
s_\widehat{m}^2 &= s_{\widehat{\beta}_0 + \widehat{\beta}_1X}^2 \\\\
&= \widehat{\beta}_1^2 s_x^2 \\\\
R^2 &= \widehat{\beta}_1^2 \frac{s_x^2}{s_y^2} \\\\
&= \left(\frac{\text{COV}\left[x, y\right]}{s_x s_y}\right)^2 \\\\
&= \frac{s_y^2 - \overline{e^2}}{s_y^2} \\\\
R_{\text{ajustado}}^2 &= \frac{\widehat{s}_y^2 - \widehat\sigma^2}{\widehat{s}_y^2}
\end{align}
$$

<br>

$$
\begin{align}
R^2 &= \frac{\text{VAR}\left[m(X)\right]}{\text{VAR}\left[Y\right]} \\\\
&= \frac{\text{VAR}\left[\beta_0 + \beta_1X\right]}{\text{VAR}\left[\beta_0 + \beta_1X + \epsilon\right]} \\\\
R^2 &= \frac{\beta_1^2 \text{VAR}\left[X\right]}{\beta_1^2 \text{VAR}\left[X\right] + \sigma}
\end{align}
$$

<br>

## $\rho_{xy}$

<br>

$$
\begin{align}
\rho_{XY} &= \frac{\text{COV}\left[X, Y\right]}{\sqrt{\text{VAR}\left[X\right] \text{VAR}\left[Y\right]}} \\\\
&= \beta_1 \frac{\text{VAR}\left[X\right]}{\sqrt{\text{VAR}\left[Y\right]}}
\end{align}
$$

<br>

## Notación matricial

> \newcommand{\m}[1]{\mathbf{#1}}
> \newcommand{\b}[1]{\boldsymbol{#1}}

<br>

$$
\begin{align}
\mathbf{y} &= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
\mathbf{e} &= \mathbf{y} - \mathbf{X}\boldsymbol{\widehat\beta} \\\\
MSE &= \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \boldsymbol{\beta}^\prime \mathbf{X}^\prime) (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime \mathbf{y} - \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
\mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} &= \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} \quad \text{porque} \; \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} \; \text{es una matriz} \; 1 \times 1 \\\\
MSE &= \frac{1}{n} (\mathbf{y}^\prime \mathbf{y} - 2 \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

### Estimación de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\nabla MSE &= \frac{1}{n} (\nabla \mathbf{y}^\prime \mathbf{y} - 2 \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (0 - 2 \mathbf{X}^\prime \mathbf{y} + 2 \mathbf{X}^\prime \mathbf{X} \mathbf{\beta}) \\\\
&= \frac{2}{n} (- \mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \mathbf{\beta})
\end{align}
$$

<br>

$$
\begin{align}
\mathbf{X}^\prime \mathbf{X} \mathbf{\widehat\beta} - \mathbf{X}^\prime \mathbf{y} &= 0 \\\\
\mathbf{\widehat\beta} &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y}
\end{align}
$$

<br>

### Predicciones

<br>

*Hat* o *influence matrix*:

$$
\begin{align}
\mathbf{H} &= \mathbf{X} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \\\\
\mathbf{\widehat{m}} &= \mathbf{X} \boldsymbol{\widehat\beta} \\\\
\mathbf{\widehat{m}} &= \mathbf{H} \mathbf{y}
\end{align}
$$

Propiedades de $\mathbf{H}$:

$$
\begin{align}
\frac{\partial \widehat{m}_i}{\partial y_j} &= H_{ij} \\\\
\mathbf{H} &= \mathbf{H}^\prime \\\\
\mathbf{H}^2 &= \mathbf{H} \\\\
\end{align}
$$

<br>

### Varianza de las predicciones

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{Hy}\right] &= \text{VAR}\left[\mathbf{H (\mathbf{x} \boldsymbol{\beta + \boldsymbol{\epsilon}})}\right] \\\\
&= \text{VAR}\left[\mathbf{H} \boldsymbol{\epsilon}\right] \\\\
&= \mathbf{H} \, \text{VAR}\left[\boldsymbol{\epsilon}\right] \, \mathbf{H}^\prime \\\\
&= \sigma^2 \mathbf{H}
\end{align}
$$

<br>

### Residuos

<br>

$$
\begin{align}
\mathbf{e} &= \mathbf{y} - \mathbf{X}\boldsymbol{\widehat\beta} \\\\
&= \mathbf{y} - \mathbf{Hy} \\\\
&= (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
$$

Propiedades de $(\mathbf{I} - \mathbf{H}) \mathbf{y}$:

$$
\begin{align}
\frac{\partial e_i}{\partial y_j} &= (\mathbf{I} - H)_{ij} \\\\
(\mathbf{I} - H) &= (\mathbf{I} - H)^\prime \\\\
(\mathbf{I} - H)^2 &= (\mathbf{I} - H) \\\\
\end{align}
$$

<br>

$$
\begin{align}
MSE &= \frac{1}{n} \mathbf{e}^\prime \mathbf{e} \\\\
&= \frac{1}{n} \mathbf{y}^\prime (\mathbf{I} - \mathbf{H})^\prime \, (\mathbf{I} - \mathbf{H}) \mathbf{y} \\\\
&= \frac{1}{n} \mathbf{y}^\prime (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
$$

<br>

### Varianza de los residuos

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{e}\right] &= \text{VAR}\left[(\mathbf{I} - \mathbf{H}) (\mathbf{x}\boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
&= \text{VAR}\left[(\mathbf{I} - \mathbf{H}) \mathbf{\epsilon}\right] \\\\
&= (\mathbf{I} - \mathbf{H}) \text{VAR}\left[\mathbf{\epsilon}\right] (\mathbf{I} - \mathbf{H})^\prime \\\\
&= \sigma^2 (\mathbf{I} - \mathbf{H})
\end{align}
$$

<br>

### Varianza de los coeficientes

<br>

$$
\begin{align}
\mathbf{\widehat\beta} &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y} \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\beta} + \mathbf{\epsilon}) \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} + (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{\epsilon} \\\\
&= \boldsymbol{\beta} + (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{\epsilon} \\\\
\text{VAR}\left[\boldsymbol{\widehat\beta}\right] &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \,  \text{VAR}\left[\mathbf{\epsilon}\right] \left((\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime\right)^\prime \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \sigma^2 \mathbf{I} \, \mathbf{X}(\mathbf{X}^\prime \mathbf{X})^{-1} \\\\
&= \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \mathbf{X}(\mathbf{X}^\prime \mathbf{X})^{-1} \\\\
&= \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}
\end{align}
$$

<br>

## Derivación alternativa

<br>

$$
\begin{align}
\mathbf{y} &= \mathbf{1}\beta_0 + \mathbf{X}\boldsymbol{\beta_1} + \boldsymbol{\epsilon} \\\\
MSE &= \frac{1}{n} (\mathbf{y} - \mathbf{1}\beta_0 - \mathbf{X} \boldsymbol{\beta_1})^\prime (\mathbf{y} - \mathbf{1}\beta_0 - \mathbf{X} \boldsymbol{\beta_1}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \mathbf{1}^\prime\beta_0^\prime - \boldsymbol{\beta_1}^\prime \mathbf{X}^\prime) (\mathbf{y} - \boldsymbol{\beta_0}\mathbf{1} - \mathbf{X} \boldsymbol{\beta_1}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \mathbf{1}^\prime\beta_0^\prime - \boldsymbol{\beta_1}^\prime \mathbf{X}^\prime) (\mathbf{y} - \boldsymbol{\beta_0}\mathbf{1} - \mathbf{X} \boldsymbol{\beta_1}) \\\\
\frac{\partial MSE}{\partial \beta_0} &= - \frac{2}{n} \mathbf{1}^\prime \left(\mathbf{y} - \boldsymbol{\beta_0}\mathbf{1} - \mathbf{X} \boldsymbol{\beta_1}\right) \\\\
\nabla_{\boldsymbol{\beta_1}} MSE &= \frac{2}{n} \left(\beta_0 \mathbf{X}^\prime \mathbf{1} - \mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta_1}\right) \\\\
\widehat\beta_0 &= \frac{1}{n} \mathbf{1}^\prime \mathbf{y} - \frac{1}{n} \mathbf{1}^\prime \mathbf{x} \boldsymbol{\beta_1} \\\\
&= \bar{y} - \mathbf{\bar{x}} \boldsymbol{\beta_1} \\\\
\boldsymbol{\widehat\beta_1} &= \frac{\frac{1}{n} \mathbf{x}^\prime \mathbf{y} - \mathbf{\bar{x}}^\prime \bar{y}}{\frac{1}{n} \mathbf{x}^\prime \mathbf{x} - \mathbf{\bar{x}}^\prime \mathbf{\bar{x}}}
\end{align}
$$

<br>

## Derivadas respecto a vectores

<br>

Si $\mathbf{a}$ y $\mathbf{x}$ son vectores $p \times 1$, entonces

$$
\begin{align}
\nabla (\mathbf{x}^\prime \mathbf{a}) &= \mathbf{a} \\\\
\nabla (\mathbf{b} \mathbf{x}) &= \mathbf{b}^\prime \\\\
\end{align}
$$

Si $\mathbf{c}$ es una matriz de dimensiones $p \times p$, entonces

$$
\nabla (\mathbf{x}^\prime \mathbf{c} \mathbf{x}) = (\mathbf{c} + \mathbf{c}^\prime) \mathbf{x}
$$

Si, además, $\mathbf{c} = \mathbf{c}^\prime$, entonces

$$
\nabla (\mathbf{x}^\prime \mathbf{c} \mathbf{x}) = 2 \mathbf{c} \mathbf{x}
$$

<br>

## Esperanza y varianza de vectores y matrices

<br>

Si $\mathbf{x}$ es un vector aleatorio $n \times 1$, entonces

$$
\begin{align}
\text{VAR}\left[\mathbf{x}\right] &= \mathbb{E}\left[\mathbf{x} \mathbf{x}^\prime\right] - \mathbb{E}\left[\mathbf{x}\right] \mathbb{E}\left[\mathbf{x}\right]^\prime \\\\
\text{VAR}\left[b\mathbf{x}\right] &= b^2 \mathbf{x} \\\\
\text{VAR}\left[\mathbf{cx}\right] &= \mathbf{c} \text{VAR}\left[\mathbf{x} \right] \mathbf{c}^\prime \\\\
\mathbb{E}\left[\mathbf{x}^\prime \mathbf{cx} \right] &= \mathbb{E}\left[\mathbf{x}\right]^\prime \mathbf{c}\mathbb{E}\left[\mathbf{x}\right] + \text{tr} \, \mathbf{c} \text{VAR}\left[\mathbf{x}\right] \\\\
\mathbf{x}^\prime \mathbf{cx} &= \text{tr} \, \mathbf{x}^\prime \mathbf{cx} \\\\
&= \text{tr} \, \mathbf{cxx}^\prime
\end{align}
$$

<br>

## El método Delta

<br>

$\theta$ es un parámetro que pretendemos estimar a través de un estimador insesgado $\hat\theta$. Supongamos que $\theta$ es una función de un vector de parámetros $\boldsymbol{\psi}$,

<br>

$$
\theta = f\left(\psi_1, \dots, \psi_n \right)
$$

Entonces,

$$
\widehat\theta = f\left(\widehat\psi_1, \dots, \widehat\psi_p \right)
$$

<br>

Usando la expansión de Taylor,

<br>

$$
\begin{align}
\theta &\approx \widehat\theta + \sum_{i=1}^p \left(\psi_i - \widehat\psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\hat\theta &\approx \theta + \sum_{i=1}^p \left(\widehat\psi_i - \psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\text{VAR}\left[\hat\theta\right] &\approx \sum_{i=1}^{p} {\left. \frac{\partial^2 f}{\partial \psi_i^2} \right|}_{\psi=\widehat\psi} \, \text{VAR}\left[{\widehat{\psi_i}}\right] + 2 \sum_{i=1}^{p-1} \sum_{j=i+1}^{p} {\left. \frac{\partial f}{\partial \psi_i}\right|}_{\psi=\widehat\psi} \, {\left. \frac{\partial f}{\partial \psi_j}\right|}_{\psi=\widehat\psi} \, \text{COV}\left[{\widehat{\psi_i},\widehat{\psi_j}}\right]
\end{align}
$$

<br>

## Distribución de los valores p bajo la hipótesis nula

<br>

$$
\begin{align}
P &= F(T) \\\\
\text{Pr}(P < p) &= \text{Pr}\left(F^{-1}(P) < F^{-1}(p)\right) \\\\
&= \text{Pr}(T < t) \\\\
&= F\left(F^{-1}(p)\right) \\\\
&= p
\end{align}
$$

<br>

## Desigualdad de Chebyshev

<br>

$$
\text{P}(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}
$$

La probabilidad de que una variable aleatoria $X$ exceda su valor esperado $\mu$ por $k$ errores típicos es siempre inferior a $\frac{1}{k^2}$.

<br>



