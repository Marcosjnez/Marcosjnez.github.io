---
title: "Estimación"
output: 
  html_document:
    code_folding: hide
    css: style.css
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

## Distribución normal

<br>

$$
\begin{align}
\mathbf{x} &= x_i, \dots, x_n \\\\
P(\mathbf{x}; \mu, \sigma) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
L(\mu, \sigma; \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &= \sum_{i=1}^n -\frac{(x_i - \mu)^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi}) \\\\
\text{log}(L) &= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi)
\end{align}
$$

<br>

*Location-scale property*

<br>

$$
\begin{align}
Z &\sim N(\mu, \sigma^2) \\\\
a + bZ &\sim N(a + \mu, b^2\sigma^2)
\end{align}
$$

<br>

*Stability property*

<br>

Si $Z_{IID} \sim N(\mu, \sigma^2)$, entonces

$$
\sum_{i=1}^n Z_i \sim N\left(n \mu, n \sigma^2\right)
$$

<br>

## Estimación de $\mu$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu}&= \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) \\\\
&= \frac{n}{\sigma^2} \, (\bar{x} - \mu)
\end{align}
$$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu} &= 0 \\\\
\widehat\mu &= \bar{x} \\\\
\end{align}
$$

<br>

## Valor esperado de $\bar{x}$

<br>

$$
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n x_i \right] = \mu
$$

<br>

## Varianza de $\widehat\mu$

<br>

$$
\begin{align}
\frac{\partial^2 \text{log}(L)}{\partial \mu^2} &= - \frac{n}{\sigma^2} \\\\
\text{VAR}\left[\widehat\mu\right] &= \mathbb{E}\left[(\widehat\mu - \mu)^2\right] \\\\
&= \frac{\sigma^2}{n}
\end{align}
$$

<br>

## Estimación de $\sigma^2$

<br>

$$
\begin{align}
\text{log}(L) &= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi) \\\\
\end{align}
$$

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= 0 \\\\
\sigma^2 &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{n}
\end{align}
$$

<br>

## Valor esperado de $\frac{\sum_{i=1}^n (x_i - \widehat\mu)^2}{n}$

<br>

$$
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &= \frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n ((x_i - \mu) - (\widehat\mu - \mu))^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2 + (\widehat\mu - \mu)^2 - 2(x_i - \mu)(\widehat\mu - \mu))\right] \\\\
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &= n\sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (\widehat\mu - \mu)^2\right] &= \sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)(\widehat\mu - \mu) \right] &= \mathbb{E}\left[n(\mu^2 + \bar{x}\widehat\mu - \bar{x}\mu - \widehat\mu\mu)\right] \\\\
&= \mathbb{E}\left[n(\mu^2 + \widehat\mu^2 - 2\widehat\mu\mu)\right] \\\\
&= n \, \mathbb{E}\left[(\widehat\mu - \mu)^2 \right] \\\\
&= \sigma^2
\end{align}
$$

<br>

$$
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &= \sigma^2 + \frac{\sigma^2}{n} - \frac{2\sigma^2}{n} \\\\
&= \sigma^2 \left(\frac{n - 1}{n}\right) \\\\
\end{align}
$$
$$
\begin{align}
\sigma^2 &= \frac{\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right]}{n - 1}
\end{align}
$$

<br>

$$
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n - 1}
$$

<br>

## Varianza de $\sigma^2$ y $\widehat\sigma^2$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial^2 \text{log}(L)}{\partial \sigma^4} &= - \frac{\sum_{i=1}^n (x_i - \mu)^2}{\sigma^6} + \frac{n}{2\sigma^4} \\\\
&= \frac{- 2\sum_{i=1}^n (x_i - \mu)^2 + n\sigma^2}{2\sigma^6} \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &= n\sigma^2 \\\\
\mathbb{E}\left[\frac{\partial^2 \text{log}(L)}{\partial \sigma^4}\right] &= \frac{- 2n\sigma^2 + n\sigma^2}{2\sigma^6} \\\\
&= - \frac{n}{2\sigma^4} \\\\
\text{VAR}\left[\sigma^2\right] &= \frac{2\sigma^4}{n}
\end{align}
$$

<br>

$$
\begin{align}
\frac{\epsilon_i}{\sigma} &\sim N(0, 1) \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 &\sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 = \frac{(n-1)\widehat\sigma^2}{\sigma^2} &\sim \chi^2_{n-1} \\\\
\text{VAR}\left[\chi^2_{n-1}\right] &= 2(n-1) \\\\
\text{VAR}\left[\frac{(n-1)\widehat\sigma^2}{\sigma^2}\right] &= 2(n-1) \\\\
\text{VAR}\left[(n-1)\widehat\sigma^2\right] &= 2\sigma^4(n-1) \\\\
\text{VAR}\left[\widehat\sigma^2\right] &= \frac{2\sigma^4}{n-1} \\\\
\end{align}
$$

<br>

## Función afín

<br>

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_i + \epsilon_i \\\\
y_i &= \widehat{\beta}_0 + \widehat{\beta}_1x_i + e_i
\end{align}
$$

<br>

$$
\begin{align}
P(\mathbf{y}; \beta_0, \beta_1, \sigma, \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
L(\beta_0, \beta_1, \sigma; \mathbf{y}, \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &= \sum_{i=1}^n -\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi})
\end{align}
$$

<br>

## Residuos

<br>

$$
\begin{align}
\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})e_i &= 0, \quad \text{COV}[X, e] = 0 \\\\
\frac{1}{n} \sum_{i=1}^n e_i &= 0
\end{align}
$$

<br>

## Estimación de $\beta_1$

<br>

$$
\begin{align}
\frac{\partial L}{\partial \beta_1} &= - \frac{\sum_{i=1}^n \beta_0x_i + \beta_1x_i^2 - y_ix_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_1} &= 0 \\\\
&= \overline{yx} - \beta_0\bar{x} - \beta_1\overline{x^2} \\\\
\end{align}
$$
$$
\begin{align}
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - \beta_0\mathbb{E}[X] &= 0 \\\\
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - (\mathbb{E}[Y] - \beta_1\mathbb{E}[X]) \, \mathbb{E}[X] &= 0 \\\\
\text{COV}[YX] - \beta_1\mathbb{E}[X^2] - \beta_1\mathbb{E}[X]^2 &= 0 \\\\
\text{COV}[YX] - \beta_1\text{VAR}[X] &= 0 \\\\
\end{align}
$$
$$
\begin{align}
\beta_1 &= \frac{\text{COV}[YX]}{\text{VAR}[X]} \\\\
\end{align}
$$

<br>

$$
\begin{align}
\frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} &= \frac{\sum_{i=1}^n x_i  y_i - \bar{x} \bar{y}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{\sum_{i=1}^n x_i (\beta_0 + \beta_1x_i + \epsilon_i) - \bar{x} (\beta_0 + \beta_1\bar{x} + \bar{\epsilon})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{n\beta_1(\overline{x^2} - \bar{x}^2)}{\sum_{i=1}^n (x_i - \bar{x})^2} + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_1 + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
\end{align}
$$

<br>

Según la ley de la esperanza total,

$$
\begin{align}
\mathbb{E}\left[\widehat{\beta}_1\right] &= \mathbb{E}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_i, \dots, x_n\right]\right] \\\\
&= \mathbb{E}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \mathbb{E}[\epsilon_i]}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \, , \quad \mathbb{E}[\epsilon \, | \, x_i] = 0 \\\\
&= \beta_1 \\\\
\widehat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{align}
$$

<br>

## Estimación de $\beta_0$

<br>

$$
\begin{align}
\frac{\partial L}{\partial \beta_0} &=- \frac{\sum_{i=1}^n \beta_0 + \beta_1x_i - y_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_0} &= 0 \\\\
\widehat{\beta}_0 &= \bar{y} - \widehat{\beta}_1\bar{x} \\\\
\mathbb{E}\left[\widehat{\beta}_0\right] &= \beta_0 + \beta_1\mathbb{E}[X] - \beta_1\mathbb{E}[X] \\\\
&= \beta_0
\end{align}
$$

<br>

## Varianza de $\widehat{\beta}_1$

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right] &= \text{VAR}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \\\\
&= \sum_{i=1}^n \frac{(x_i - \bar{x})^2 \, \text{VAR}\left[\epsilon_i\right]}{n^2 s_x^4} \\\\
&= \frac{\sigma^2}{n s_x^2}
\end{align}
$$

Aplicando la ley de la varianza total,

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_1\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2}{n s_x^2} \\\\
\widehat{\text{VAR}}\left[\widehat{\beta}_1\right] &= \frac{\widehat\sigma^2}{n s_x^2}
\end{align}
$$

Dado que $\epsilon_i$ sigue una distribución normal, la propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{\beta}_1 &\sim N\left(\beta_1, \frac{\sigma^2}{ns_x^2} \right) \\\\\
\frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} &\sim N\left(0, 1 \right) \\\\\
\end{align}
$$

Si conociéramos $\sigma^2$, entonces

$$
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
$$

Pero si estimamos $\sigma^2$ a través de $\widehat\sigma^2$, entonces

$$
\begin{align}
\frac{\widehat{\beta}_1 - \beta_1}{\widehat\sigma / s_x\sqrt{n}} &= \frac{\frac{\widehat{\beta}_1 - \beta_1}{\sigma}}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&\sim \frac{N(0, 1/ns_x^2)}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&\sim \frac{N(0, 1)}{\frac{\widehat\sigma}{\sigma}} \\\\
&\sim \frac{N(0, 1)}{\sqrt{\frac{\sum_{i=1}^n e_i^2}{\sigma^2 (n-2)}}} \\\\
&\sim \frac{N(0, 1)}{\sqrt{\frac{\chi^2_{n-2}}{n-2}}} \\\\
&\sim t_{n-2}
\end{align}
$$

<br>

## Varianza de $\widehat{\beta}_0$

<br>

$$
\begin{align}
\text{VAR}\left[ \widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &= \text{VAR}\left[\bar{y} - \widehat{\beta}_1\bar{x}\right] \\\\
&= \text{VAR}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon} - \widehat{\beta}_1\bar{x}\right] \\\\
&= \text{VAR}\left[\bar{\epsilon}\right] + \bar{x}^2 \, \text{VAR}\left[\widehat{\beta}_1\right] - 2\bar{x}^2\text{COV}\left[\bar{\epsilon}, \widehat{\beta}_1\right]
\end{align}
$$

<br>

$$
\text{COV}\left[\bar{\epsilon}, \widehat{\beta}_1\right] = 0
$$

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &= \frac{\sigma^2}{n} + \frac{\sigma^2 \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2} - \sigma^2 s_x^2}{n s_x^2} \\\\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2}}{n s_x^2} - \frac{\sigma^2}{n} \\\\
&= \frac{\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
$$

<br>

Según la ley de la esperanza total,

$$
\begin{align}
\text{VAR}\left[\widehat{\beta}_0\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2 \overline{x^2}}{n s_x^2} \\\\
\widehat{\text{VAR}}\left[\widehat{\beta}_0\right] &= \frac{\widehat\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
$$

Dado que $\epsilon_i$ sigue una distribución normal, la propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{\beta}_0 &\sim N\left(\beta_0, \frac{\sigma^2 \overline{x^2}}{n s_x^2} \right) \\\\\
\frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} &\sim N\left(0, 1 \right) \\\\\
\end{align}
$$

Si conociéramos $\sigma^2$, entonces

$$
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
$$

Pero si estimamos $\sigma^2$ a través de $\widehat\sigma^2$, entonces

$$
\frac{\widehat{\beta}_0 - \beta_0}{\widehat\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \sim t_{n-2}
$$

<br>

## Predicciones

<br>

$$
\begin{align}
\widehat{m}(x_0) &= \widehat{\beta}_0 + \widehat{\beta}_1x_0 \\\\
&= \bar{y} - \widehat{\beta}_1\bar{x} + \widehat{\beta}_1x_0 \\\\
&= \bar{y} + (x_0 - \bar{x})\widehat{\beta}_1 \\\\
&= \beta_0 + \beta_1\bar{x} + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x_0 - \bar{x})\left(\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\\\
&= \beta_0 + \beta_1\bar{x} + (x_0 - \bar{x})\beta_1 + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x_0 - \bar{x}) \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_0 + \beta_1x_0 + \frac{1}{n} \sum_{i=1}^n \epsilon_i \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right)
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[\widehat{m}(x_0)\right] &= \beta_0 + \beta_1x_0 + \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\epsilon_i] \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right) \\\\
&= \beta_0 + \beta_1x_0
\end{align}
$$

<br>

## Varianza del promedio de las predicciones

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n \right] &= \frac{1}{n^2} \sum_{i=1}^n \text{VAR}[\epsilon_i] \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right)^2 \\\\
&= \frac{\sigma^2}{n^2} \sum_{i=1}^n \left(1 + (x_0 - \bar{x})^2 \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&= \frac{\sigma^2}{n^2} \left(n + (x_0 - \bar{x})^2 \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&= \frac{\sigma^2}{n^2} \left(n + (x_0 - \bar{x})^2 \frac{n}{s_x^2}\right) \\\\
&= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)
\end{align}
$$

Según la ley de la esperanza total,

$$
\begin{align}
\text{VAR}\left[\widehat{m}(x_0)\right] &= \text{VAR}\left[\mathbb{E}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)
\end{align}
$$

<br>

La propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{m}(x_0) &\sim N\left(\beta_0 + \beta_1x_0, \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)\right) \\\\
\frac{\widehat{m}(x_0) - m(x_0)}{\sqrt{\frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)}} &\sim N(0, 1) \\\\
\frac{\widehat{m}(x_0) - m(x_0)}{\sqrt{\frac{\widehat\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)}} &\sim t_{n-2} \\\\
\end{align}
$$

<br>

## Varianza del error de predicción

<br>

$$
\begin{align}
y_0 &= \beta_0 + \beta_1x_0 + \epsilon_0 \\\\
\widehat{m}(x_0) &= \widehat\beta_0 + \widehat\beta_1x_0 \\\\
y_0 - \widehat{m}(x_0) &= \beta_0 + \beta_1x_0 - \widehat\beta_0 - \widehat\beta_1x_0 + \epsilon_0 \\\\
\text{COV}[\widehat{m}(x_0), \epsilon_0] &= 0 \\\\
\text{VAR}[y_0 - \widehat{m}(x_0)] &= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right) + \sigma^2 \\\\
\text{VAR}[y_0 - \widehat{m}(x_0)] &= \sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)
\end{align}
$$

<br>

La propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{m}(x_0) &\sim N\left(y_0, \sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)\right) \\\\
\frac{y_0 - \widehat{m}(x_0)}{\sqrt{\sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)}} &\sim N(0, 1) \\\\
\frac{y_0 - \widehat{m}(x_0)}{\sqrt{\widehat\sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)}} &\sim t_{n-2}
\end{align}
$$

<br>

## Varianza del error de estimación (Varianza de un residuo)

<br>

$$
\begin{align}
\widehat{\beta}_0 &= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
$$

<br>

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_i + \epsilon_i \\\\
\text{VAR}\left[y_i\right] &= \sigma^2 \\\\
\widehat{m}(x_i) &= \widehat\beta_0 + \widehat\beta_1x_i \\\\
&= \bar{y} - \widehat\beta_1\bar{x} + \widehat\beta_1x_i \\\\
&= \bar{y} + (x_i - \bar{x})\widehat\beta_1 \\\\
&= \bar{y} + (x_i - \bar{x})\left(\beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
&= \beta_0 + \beta_1x_i + (x_i - \bar{x})\left(\frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
\text{VAR}\left[\widehat{m}(x_i)\right] &=  \frac{\sigma^2}{n} + (x_i - \bar{x})^2 \frac{\sigma^2}{ns_x^2} \\\\
&= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
\mathbb{E}\left[(y_i - \widehat{m}(x_i))^2\right] &= \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\,\text{COV}\left[y_i, \widehat{m}(x_i)\right]
\end{align}
$$

<br>

$$
\begin{align}
\text{COV}\left[y_i, \widehat{m}(x_i)\right] &= \text{COV}\left[y_i + \widehat{m}(x_i) - \widehat{m}(x_i), \widehat{m}(x_i) \right] \\\\
&= \text{COV}\left[\widehat{m}(x_i), \widehat{m}(x_i) \right] + \text{COV}\left[y_i - \widehat{m}(x_i), \widehat{m}(x_i) \right] \\\\
&= \text{COV}\left[\widehat{m}(x_i), \widehat{m}(x_i) \right] + \text{COV}\left[e_i, \widehat{m}(x_i) \right] \\\\
&= \text{VAR}\left[\widehat{m}(x_i)\right] + 0 \\\\
&= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[(y_i - \widehat{m}(x_i))^2\right] &= \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&= \sigma^2 - \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&= \sigma^2 \left(1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
$$

<br>

## Varianza de la estimación del error cuadrático, $\widehat\sigma^2$

<br>

$$
\mathbb{E}\left[(Y - (\beta_0 + \beta_1X))^2\right] = \sigma^2
$$

<br>

$$
\begin{align}
\text{VAR}\left[\bar{y} \, | \, x_i, \dots, x_n\right] &= \text{VAR}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon}\right] \\\\
&= \frac{1}{n^2} \sum_{i=1}^n \text{VAR}\left[\epsilon_i\right] \\\\
&= \frac{\sigma^2}{n}
\end{align}
$$

<br>

$$
\begin{align}
\widehat{\beta}_0 &= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
$$

<br>

$$
\begin{align}
 \mathbb{E}\left[\sum_{i=1}^n e_i^2 \, | \, x_i, \dots, x_n\right] &= \mathbb{E}\left[\sum_{i=1}^n (y_i - \widehat{m}(x_i))^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n \left(\beta_0 + \beta_1x_i + \epsilon_i - \widehat{\beta}_0 - \widehat{\beta}_1x_i\right)^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n \left((\beta_0 - \widehat{\beta}_0)^2 + (\beta_1 - \widehat{\beta}_1)^2x_i^2 + \epsilon_i^2\right)\right] + \\\\
& 2 \, \mathbb{E}\left[\sum_{i=1}^n (\beta_0 - \widehat{\beta}_0)(\beta_1 - \widehat{\beta}_1)x_i + (\beta_0 - \widehat{\beta}_0)\epsilon_i + (\beta_1 - \widehat{\beta}_1)x_i\epsilon_i \right] \\\\
&= n \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2}}{n s_x^2}\right] + 1 \right) - 2\sigma^2 \, \mathbb{E}\left[\frac{\bar{x}^2}{s_x^2} \right] - 2 \sigma^2 - 2 \sigma^2 \\\\
&= \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2} - \bar{x}^2}{s_x^2}\right] + n - 4 \right) \\\\
&= \sigma^2 \left(n - 2 \right) \\\\
\end{align}
$$

<br>

$$
\widehat\sigma^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2 \\\\
$$

$\widehat{\sigma}^2$ es estadísticamente independiente de $\widehat\beta_1$ y $\widehat\beta_0$ a pesar de que todos los estimadores son una función de $\epsilon$.

<br>

Sabiendo que si $Z \sim N(0, 1)$, entonces $Z^2 \sim \chi^2$ y que $\frac{\epsilon_i}{\sigma} \sim N(0, 1)$, entonces

$$
\begin{align}
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 &= \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 \sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 &\sim \chi^2_{n-2} \\\\
P\left(\chi^2_{n-2} \; \leq \; \chi^2_{n-2, \, 1-\alpha}\right) &= 1 - \alpha \\\\
P\left(\sigma^2 \; \leq \; \frac{(n-2) \widehat\sigma^2}{\chi^2_{n-2, \, 1-\alpha}}\right) &= 1 - \alpha \\\\
\text{VAR}\left[\chi^2_{n-2}\right] &= 2(n-2) \\\\
\text{VAR}\left[\frac{(n-2)\widehat\sigma^2}{\sigma^2}\right] &= 2(n-2) \\\\
\text{VAR}\left[(n-2)\widehat\sigma^2\right] &= 2\sigma^4(n-2) \\\\
\text{VAR}\left[\widehat\sigma^2\right] &= \frac{2\sigma^4}{n-2} \\\\
\end{align}
$$

<br>

## Modelo sin predictores

<br>

$$
\begin{align}
Y &= \beta_0 + \epsilon \\\\
\widehat\beta_0 &= \bar{y} \sim N\left(\beta_0, \frac{\sigma^2}{n}\right) \\\\
\widehat\sigma^2 &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \\\\
&= s_y^2 \\\\
\frac{(n-1) \widehat\sigma^2}{\sigma^2} &\sim \chi^2_{n-1}
\end{align}
$$

<br>

## El test F

<br>

La distribución $\chi^2$ procede de la suma del cuadrado de variables con distribución normal y la distribución $F$ procede de la razón entre variables aleatorias con distribución $\chi^2$.

$$
\frac{\chi^2_a}{\chi^2_b} \frac{b}{a} \sim F_{a, b}
$$

Según el modelo sin predictores, 

$$
\frac{(n-1) \widehat\sigma^2_{null}}{\sigma^2} \sim \chi^2_{n-1}
$$

mientras que según el modelo con un predictor,

$$
\frac{(n-2) \widehat\sigma_2^2}{\sigma^2} \sim \chi^2_{n-2}
$$

Por tanto,

$$
\begin{align}
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{\sigma^2} &\sim \chi^2_1 \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{(n-2) \widehat\sigma_2^2} &\sim \frac{\chi^2_1}{\chi^2_{n-2}} \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{(n-2) \widehat\sigma_2^2} \, \frac{n-2}{1} &\sim F_{1, n-2} \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{\widehat\sigma_2^2} &\sim F_{1, n-2}
\end{align}
$$

El test $F$ asume que los errores ($\epsilon$) siguen una distribución normal, son homocedásticos e independientes de la variable $X$ y entre sí. Se trata de un caso particular de la razón de verosimilitudes.

<br>

En el caso general, si estuviéramos contrastando la significación de $p-q$ estimaciones,

$$
\begin{align}
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{\sigma^2} &\sim \chi^2_{p-q} \\\\
\frac{(n-p) \widehat\sigma^2_{p}}{\sigma^2} &\sim \chi^2_{n-p} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(n-p) \widehat\sigma^2_{p}} &\sim \frac{\chi^2_{p-q}}{\chi^2_{n-p}} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(n-p) \widehat\sigma^2_{p}} \, \frac{n-p}{p-q} &\sim F_{p-q, n-p} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(p-q) \widehat\sigma^2_{p}} &\sim F_{p-q, n-p}
\end{align}
$$

<br>

## El test de razón de verosimilitudes

<br>

El espacio paramétrico del modelo nulo de $q$ coeficientes es un subconjunto del espacio paramétrico del modelo general de $p$ coeficientes.

El test de razón de verosimilitudes asume que las estimaciones de los parámetros siguen una distribución normal y esto es solo aproximadamente cierto para $\widehat\sigma^2$. Por tanto, solo es exactamente correcto cuando $n \rightarrow \infty$.

$$
\begin{align}
\Lambda &= L(\widehat\Theta) - L(\widehat\theta) \\\\
\lim_{n \rightarrow \infty} \quad 2\Lambda &\sim \chi^2_{p-q}
\end{align}
$$

<br>

$$
\begin{align}
L(\widehat\theta) &= - \frac{n}{2} \log2\pi - \frac{n}{2} \log2(n-q)\widehat\sigma^2_q - \frac{1}{2(n-q)\widehat\sigma^2_q} \sum_{i=1}^n (y_i - \widehat\beta_0)^2 \\\\
&= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log (n-q)\widehat\sigma^2_q} \\\\
L(\widehat\Theta) &= - \frac{n}{2} \log2\pi - \frac{n}{2} \log2(n-p)\widehat\sigma^2_p - \frac{1}{2(n-p)\widehat\sigma^2_p} \sum_{i=1}^n (y_i - \widehat\beta_0 - \widehat\beta_1x_i)^2 \\\\
&= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log (n-p)\widehat\sigma^2_p}
\end{align}
$$

<br>

$$
\begin{align}
L(\widehat\Theta) - L(\widehat\theta)
&= \frac{n}{2} \log\frac{(n-q)\widehat\sigma^2_q}{(n-p)\widehat\sigma^2_p} \\\\
\lim_{n \rightarrow \infty} \quad n \log\frac{(n-q)\widehat\sigma^2_q}{(n-p)\widehat\sigma^2_p} &\sim \chi^2_{p-q}
\end{align}
$$

Incluso cuando ambos modelos son falsos, el test de razón de verosimilitudes nos puede indicar cuál de ellos se parece más al verdadero.

<br>

## $R^2$

<br>

$$
\begin{align}
R^2 &= \frac{\text{COV}\left[y, \widehat{m}\right]}{s_y^2} \\\\
\text{COV}\left[y, \widehat{m}\right] &= \text{COV}\left[\widehat{m} + e, \widehat{m}\right] \\\\
&= s_\widehat{m}^2 + \text{COV}\left[e, \widehat{m}\right] \\\\
&= s_\widehat{m}^2 \\\\
R^2 &= \frac{s_\widehat{m}^2}{s_y^2} \\\\
s_\widehat{m}^2 &= s_{\widehat{\beta}_0 + \widehat{\beta}_1X}^2 \\\\
&= \widehat{\beta}_1^2 s_x^2 \\\\
R^2 &= \widehat{\beta}_1^2 \frac{s_x^2}{s_y^2} \\\\
&= \left(\frac{\text{COV}\left[x, y\right]}{s_x s_y}\right)^2 \\\\
&= \frac{s_y^2 - \overline{e^2}}{s_y^2} \\\\
R_{\text{ajustado}}^2 &= \frac{\widehat{s}_y^2 - \widehat\sigma^2}{\widehat{s}_y^2}
\end{align}
$$

<br>

$$
\begin{align}
R^2 &= \frac{\text{VAR}\left[m(X)\right]}{\text{VAR}\left[Y\right]} \\\\
&= \frac{\text{VAR}\left[\beta_0 + \beta_1X\right]}{\text{VAR}\left[\beta_0 + \beta_1X + \epsilon\right]} \\\\
R^2 &= \frac{\beta_1^2 \text{VAR}\left[X\right]}{\beta_1^2 \text{VAR}\left[X\right] + \sigma}
\end{align}
$$

<br>

## $\rho_{xy}$

<br>

$$
\begin{align}
\rho_{XY} &= \frac{\text{COV}\left[X, Y\right]}{\sqrt{\text{VAR}\left[X\right] \text{VAR}\left[Y\right]}} \\\\
&= \beta_1 \frac{\text{VAR}\left[X\right]}{\sqrt{\text{VAR}\left[Y\right]}}
\end{align}
$$

<br>

## Notación matricial

> \newcommand{\m}[1]{\mathbf{#1}}
> \newcommand{\b}[1]{\boldsymbol{#1}}

<br>

$$
\begin{align}
\mathbf{y} &= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
MSE &= \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \boldsymbol{\beta}^\prime \mathbf{X}^\prime) (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime \mathbf{y} - \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
\mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} &= \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} \quad \text{porque} \; \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} \; \text{es una matriz} \; 1 \times 1 \\\\
MSE &= \frac{1}{n} (\mathbf{y}^\prime \mathbf{y} - 2 \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

### Estimación de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\nabla MSE &= \frac{1}{n} (\nabla \mathbf{y}^\prime \mathbf{y} - 2 \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (0 - 2 \mathbf{X}^\prime \mathbf{y} + 2 \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{2}{n} (- \mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

$$
\begin{align}
\mathbf{X}^\prime \mathbf{X} \boldsymbol{\widehat\beta} - \mathbf{X}^\prime \mathbf{y} &= 0 \\\\
\boldsymbol{\widehat\beta} &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y}
\end{align}
$$

<br>

### Varianza de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\mathbf{\widehat\beta} &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y} \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}) \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} + (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \boldsymbol{\epsilon} \\\\
&= \boldsymbol{\beta} + (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \boldsymbol{\epsilon} \\\\
\text{VAR}\left[\boldsymbol{\widehat\beta}\right] &= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \,  \text{VAR}\left[\boldsymbol{\epsilon}\right] \left((\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime\right)^\prime \\\\
&= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \sigma^2 \mathbf{I} \, \mathbf{X}(\mathbf{X}^\prime \mathbf{X})^{-1} \\\\
&= \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \mathbf{X}(\mathbf{X}^\prime \mathbf{X})^{-1} \\\\
&= \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}
\end{align}
$$

<br>

### Distribución de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\widehat{\boldsymbol{\beta}} &\sim MVN(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}) \\\\
\widehat{\beta}_i &\sim N(\beta_i, \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}) \\\\
\frac{\widehat{\beta}_i - \beta_i}{\sigma \sqrt{(\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}}} &\sim N(0, 1) \\\\
\frac{\widehat{\beta}_i - \beta_i}{\widehat\sigma \sqrt{(\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}}} &\sim t_{n-p-1}
\end{align}
$$

<br>

### Valores ajustados

<br>

*Hat* o *influence matrix*:

$$
\begin{align}
\mathbf{H} &= \mathbf{X} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \\\\
\mathbf{\widehat{m}}(\mathbf{X}) &= \mathbf{X} \boldsymbol{\widehat\beta} \\\\
\mathbf{\widehat{m}}(\mathbf{X}) &= \mathbf{H} \mathbf{y}
\end{align}
$$

Propiedades de $\mathbf{H}$:

$$
\begin{align}
\frac{\partial \widehat{m}_i}{\partial y_j} &= H_{ij} \\\\
\mathbf{H} &= \mathbf{H}^\prime \\\\
\mathbf{H}^2 &= \mathbf{H} \\\\
\end{align}
$$

<br>

### Varianza de los valores ajustados

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{Hy}\right] &= \text{VAR}\left[\mathbf{H (\mathbf{X} \boldsymbol{\beta + \boldsymbol{\epsilon}})}\right] \\\\
&= \text{VAR}\left[\mathbf{H} \boldsymbol{\epsilon}\right] \\\\
&= \mathbf{H} \, \text{VAR}\left[\boldsymbol{\epsilon}\right] \, \mathbf{H}^\prime \\\\
&= \sigma^2 \mathbf{H}
\end{align}
$$

<br>

### Predicciones

<br>

$$
\begin{align}
\widehat{m}(X^*) &= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y} \\\\
&= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\widehat\beta} + \mathbf{e}) \\\\
&= \mathbf{X^*} \boldsymbol{\widehat\beta}
\end{align}
$$

<br>

### Varianza de las predicciones

<br>

$$
\begin{align}
\text{VAR}\left[\widehat{m}(X^*)\right] &= \text{VAR}\left[\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y}\right] \\\\
&= \text{VAR}\left[\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
&= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \text{VAR}\left[\boldsymbol{\epsilon}\right] \, (\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime)^\prime \\\\
&= \sigma^2 \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \mathbf{X} (\mathbf{X}^\prime \mathbf{X})^{-1} \, (\mathbf{X^*})^\prime \\\\
&= \sigma^2 \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, (\mathbf{X}^*)^\prime
\end{align}
$$

<br>

### Residuos

<br>

$$
\begin{align}
\mathbf{e} &= \mathbf{y} - \mathbf{X}\boldsymbol{\widehat\beta} \\\\
&= \mathbf{y} - \mathbf{Hy} \\\\
&= (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
$$

Propiedades de $(\mathbf{I} - \mathbf{H}) \mathbf{y}$:

$$
\begin{align}
\frac{\partial e_i}{\partial y_j} &= (\mathbf{I} - H)_{ij} \\\\
(\mathbf{I} - H) &= (\mathbf{I} - H)^\prime \\\\
(\mathbf{I} - H)^2 &= (\mathbf{I} - H) \\\\
\end{align}
$$

<br>

$$
\begin{align}
MSE &= \frac{1}{n} \mathbf{e}^\prime \mathbf{e} \\\\
&= \frac{1}{n} \mathbf{y}^\prime (\mathbf{I} - \mathbf{H})^\prime \, (\mathbf{I} - \mathbf{H}) \mathbf{y} \\\\
&= \frac{1}{n} \mathbf{y}^\prime (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
$$

<br>

### Varianza de los residuos

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{e}\right] &= \text{VAR}\left[(\mathbf{I} - \mathbf{H}) (\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
\mathbf{HX} \boldsymbol{\beta} &= \mathbf{X} \boldsymbol{\beta} \\\\
\text{VAR}\left[\mathbf{e}\right] &= \text{VAR}\left[(\mathbf{I} - \mathbf{H}) \boldsymbol{\epsilon}\right] \\\\
&= (\mathbf{I} - \mathbf{H}) \text{VAR}\left[\boldsymbol{\epsilon}\right] (\mathbf{I} - \mathbf{H})^\prime \\\\
&= \sigma^2 (\mathbf{I} - \mathbf{H})
\end{align}
$$

<br>

### Varianza de la estimación del error cuadrático, $\widehat\sigma^2$

<br>

$$
\begin{align}
\frac{1}{n} \mathbb{E}\left[\mathbf{e}'\mathbf{e}\right] &= \frac{1}{n} \mathbb{E}\left[((\mathbf{I} - \mathbf{H})\mathbf{e})^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \mathbb{E}\left[\mathbf{e}^\prime(\mathbf{I} - \mathbf{H})^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \mathbb{E}\left[\mathbf{e}^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \text{VAR}[\mathbf{e}]\right]\\\\
&= \frac{1}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \sigma^2 (\mathbf{I} - \mathbf{H}) \right] \\\\
&= \frac{\sigma^2}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \right] \\\\
\text{tr}[\mathbf{I}] &= n \\\\
\text{tr}[\mathbf{H}] &= p + 1 \\\\
\frac{1}{n} \mathbb{E}\left[\mathbf{e}'\mathbf{e}\right] &= \frac{\sigma^2}{n} (n - p - 1) \\\\
\sigma^2 &= \frac{\mathbb{E}\left[\mathbf{e}'\mathbf{e}\right]}{n - p - 1} \\\\
\widehat\sigma^2 &= \frac{\mathbf{e}'\mathbf{e}}{n - p - 1}
\end{align}
$$

$$
\frac{(n-p-1) \widehat\sigma^2}{\sigma^2} \sim \chi^2_{n-p-1}
$$

<br>

## Derivación alternativa

<br>

$$
\begin{align}
\mathbf{y} &= \mathbf{1}\beta_0 + \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
MSE &= \frac{1}{n} (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \mathbf{1}^\prime\beta_0 - \boldsymbol{\beta} \mathbf{X}^\prime) (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^\prime - \mathbf{1}^\prime\beta_0 - \boldsymbol{\beta}^\prime \mathbf{X}^\prime) (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

$$
\begin{align}
\frac{\partial MSE}{\partial \beta_0} &= \frac{1}{n}\left(- \nabla \mathbf{1}^\prime \beta_0 \mathbf{y} + \nabla \mathbf{1}^\prime \beta_0 \beta_0 \mathbf{1} + \nabla \mathbf{1}^\prime \beta_0 \mathbf{X} \boldsymbol{\beta} - \nabla \mathbf{y}^\prime \beta_0 \mathbf{1} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \beta_0 \mathbf{1} \right) \\\\
&= - \frac{2}{n} \mathbf{1}^\prime \left(\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}\right) \\\\
0 &= - \frac{1}{n} \mathbf{1}^\prime \mathbf{y} + \frac{1}{n} \mathbf{1}^\prime \beta_0 \mathbf{1} + \frac{1}{n} \mathbf{1}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\widehat\beta_0 &= \frac{1}{n} \mathbf{1}^\prime \mathbf{y} - \frac{1}{n} \mathbf{1}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&= \bar{y} - \mathbf{\bar{x}}^\prime \boldsymbol{\beta}
\end{align}
$$

<br>

$$
\begin{align}
\nabla_{\boldsymbol{\beta}} \, MSE &= \frac{1}{n} \left(- \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \beta_0 \mathbf{1} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} - \nabla \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} + \nabla \mathbf{1}^\prime \beta_0 \mathbf{X} \boldsymbol{\beta} \right) \\\\
&= \frac{2}{n} \left(\beta_0 \mathbf{X}^\prime \mathbf{1} - \mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}\right) \\\\
0 &= \beta_0 \frac{1}{n} \mathbf{X}^\prime \mathbf{1} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&= \beta_0 \mathbf{\bar{x}} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&= \mathbf{\bar{x}} (\bar{y} - \mathbf{\bar{x}}^\prime \boldsymbol{\beta}) - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&= \mathbf{\bar{x}} \bar{y} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \boldsymbol{\beta} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \boldsymbol{\beta} &= \frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}} \bar{y} \\\\
\left(\frac{1}{n} \mathbf{X}^\prime \mathbf{X} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \right) \boldsymbol{\beta} &= \frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}} \bar{y} \\\\
\boldsymbol{\widehat\beta} &= \frac{\frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}}^\prime \bar{y}}{\frac{1}{n} \mathbf{X}^\prime \mathbf{X} - \mathbf{\bar{x}}^\prime \mathbf{\bar{x}}}
\end{align}
$$

<br>

## Sesgo de estimación por omisión de variables relacionadas

<br>

$$
\begin{align}
\widehat\beta_1 &= \frac{\text{COV}\left[X_1, Y\right]}{\text{VAR}\left[X_1\right]} \\\\
&= \frac{\text{COV}\left[X_1, \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p\right]}{\text{VAR}\left[X_1\right]} \\\\
&= \frac{\beta_1 \text{VAR}\left[X_1\right] + \sum_{i=2}^p \beta_p \text{COV}\left[X_1, X_p\right]}{\text{VAR}\left[X_1\right]} \\\\
&= \beta_1 + \sum_{i=2}^p \beta_p \frac{\text{COV}\left[X_1, X_p\right]}{\text{VAR}\left[X_1\right]} \\\\
\end{align}
$$

<br>

## Colinealidad

<br>

Existen constantes $a_0, a_1, \ldots, a_p$ de manera que, para dos filas $i$,

$$
a_0 + \sum_{j=1}^p a_j x_{ij} = 0
$$

$\mathbf{X}^\prime \mathbf{X}$ no es invertible cuando $n < p + 1$, uno de los predictores es constante o alguno de los predictores es proporcional o está linealmente relacionado con otro.

Dos vectores son linealmente independientes si ninguna combinación lineal de ellos equivale a 0.

La multicolinealidad ocurre cuando

$$
\begin{align}
\sum_{i=1}^p a_i \mathbf{x}_i &= a_0 \\\\
\mathbf{a}^\prime \mathbf{X} &= a_0 \\\\
\text{VAR}\left[\mathbf{a}^\prime \mathbf{X} \right] &= \mathbf{a}^\prime \, \text{VAR}\left[\mathbf{X}\right] \mathbf{a} \\\\
&= 0
\end{align}
$$

<br>

## Factor de inflación de la varianza

<br>

$$
\text{VIF}_i = \frac{\frac{\widehat\sigma^2}{n s_{x_{i}}^2}}{\widehat\sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}_{i+1, i+1}}
$$

El factor de inflación de la varianza equivale a regresar $\mathbf{x}_i$ en el resto de predictores y calcular $\frac{1}{1 - R^2}$.

<br>

## Autovalores y autovectores

<br>

$$
\begin{align}
\text{VAR}\left[\mathbf{X}\right] \mathbf{v}_i &= \lambda_i \mathbf{v}_i \\\\
\mathbf{V}^\prime \mathbf{V} &= \mathbf{I} \\\\
\mathbf{V}^{-1} &= \mathbf{V}^\prime \\\\
\text{VAR}\left[\mathbf{X}\right] &= \mathbf{V} \, \mathbf{U} \, \mathbf{V}^\prime
\end{align}
$$

Los autovectores de $\text{VAR}\left[\mathbf{X}\right]$ se conocen como los componentes principales de los predictores.

Cualquier vector $\mathbf{a}$ puede ser reescrito como una suma de autovectores:

$$
a = \sum_{i=1}^p (\mathbf{a}^\prime \mathbf{v}_i) \mathbf{v}_i
$$

<br>

## Derivadas respecto a vectores

<br>

Si $\mathbf{a}$ y $\mathbf{x}$ son vectores $p \times 1$, entonces

$$
\begin{align}
\nabla_{\mathbf{x}} (\mathbf{x}^\prime \mathbf{a}) &= \mathbf{a} \\\\
\nabla_{\mathbf{x}} (\mathbf{b} \mathbf{x}) &= \mathbf{b}^\prime \\\\
\end{align}
$$

Si $\mathbf{c}$ es una matriz de dimensiones $p \times p$, entonces

$$
\nabla_{\mathbf{x}} (\mathbf{x}^\prime \mathbf{c} \mathbf{x}) = (\mathbf{c} + \mathbf{c}^\prime) \mathbf{x}
$$

Si, además, $\mathbf{c} = \mathbf{c}^\prime$, entonces

$$
\nabla{\mathbf{x}} (\mathbf{x}^\prime \mathbf{c} \mathbf{x}) = 2 \mathbf{c} \mathbf{x}
$$

<br>

## Esperanza y varianza de vectores y matrices

<br>

Si $\mathbf{x}$ es un vector aleatorio $n \times 1$, entonces

$$
\begin{align}
\text{VAR}\left[\mathbf{x}\right] &= \mathbb{E}\left[\mathbf{x} \mathbf{x}^\prime\right] - \mathbb{E}\left[\mathbf{x}\right] \mathbb{E}\left[\mathbf{x}\right]^\prime \\\\
\text{VAR}\left[b\mathbf{x}\right] &= b^2 \mathbf{x} \\\\
\text{VAR}\left[\mathbf{cx}\right] &= \mathbf{c} \text{VAR}\left[\mathbf{x} \right] \mathbf{c}^\prime \\\\
\mathbb{E}\left[\mathbf{x}^\prime \mathbf{cx} \right] &= \mathbb{E}\left[\mathbf{x}\right]^\prime \mathbf{c}\mathbb{E}\left[\mathbf{x}\right] + \text{tr} \, \mathbf{c} \text{VAR}\left[\mathbf{x}\right] \\\\
\mathbf{x}^\prime \mathbf{cx} &= \text{tr} \, \mathbf{x}^\prime \mathbf{cx} \\\\
&= \text{tr} \, \mathbf{cxx}^\prime
\end{align}
$$

<br>

## El método Delta

<br>

$\theta$ es un parámetro que pretendemos estimar a través de un estimador insesgado $\hat\theta$. Supongamos que $\theta$ es una función de un vector de parámetros $\boldsymbol{\psi}$,

<br>

$$
\theta = f\left(\psi_1, \dots, \psi_n \right)
$$

Entonces,

$$
\widehat\theta = f\left(\widehat\psi_1, \dots, \widehat\psi_p \right)
$$

<br>

Usando la expansión de Taylor,

<br>

$$
\begin{align}
\theta &\approx \widehat\theta + \sum_{i=1}^p \left(\psi_i - \widehat\psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\hat\theta &\approx \theta + \sum_{i=1}^p \left(\widehat\psi_i - \psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\text{VAR}\left[\hat\theta\right] &\approx \sum_{i=1}^{p} {\left. \frac{\partial^2 f}{\partial \psi_i^2} \right|}_{\psi=\widehat\psi} \, \text{VAR}\left[{\widehat{\psi_i}}\right] + 2 \sum_{i=1}^{p-1} \sum_{j=i+1}^{p} {\left. \frac{\partial f}{\partial \psi_i}\right|}_{\psi=\widehat\psi} \, {\left. \frac{\partial f}{\partial \psi_j}\right|}_{\psi=\widehat\psi} \, \text{COV}\left[{\widehat{\psi_i},\widehat{\psi_j}}\right]
\end{align}
$$

<br>

## Regresión de componentes principales

<br>

Los componentes principales son

$$
\mathbf{W} = \mathbf{X} \mathbf{V}
$$

Cada componente se define como la proyección de $\mathbf{X}$ en los autovectores.

El modelo de regresión con $k$ componentes es

$$
Y = \gamma_0 + \gamma_1 W_1 + \ldots + \gamma_k W_k + \eta
$$

$\eta = \epsilon$ si $k = p$.

<br>

## Regresión *Ridge*

<br>

Se trata de penalizar la longitud del vector de coeficientes $(\| \boldsymbol{\beta} \|)$. En *OLS* optimizamos la ecuación

$$
\frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
$$

Sin embargo, ahora optimizamos

$$
\frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \| \boldsymbol{\beta} \|^2 = \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \boldsymbol{\beta}^\prime \boldsymbol{\beta}
$$

donde $\lambda > 0$ regula el *trade-off* entre el *MSE* y $\| \boldsymbol{\beta} \|$.

Esta regresión se emplea tras centrar predictores y variable dependiente, para que $\beta_0 = 0$ y así no se penalice el tamaño de la intersección.

Si los predictores tienen diferente escala, entonces conviene estandarizarlos para que la penalización por $\| \boldsymbol{\beta} \|^2$ tenga sentido.

$$
\begin{align}
\nabla_\boldsymbol{\beta} \, \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \boldsymbol{\beta}^\prime \boldsymbol{\beta} &= \frac{2}{n} \left(-\mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} + \lambda \boldsymbol{\beta} \right) \\\\
\frac{2}{n} \left(-\mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\widehat\beta} + \lambda \boldsymbol{\widehat\beta} \right) &= 0 \\\\
\mathbf{X}^\prime \mathbf{y} &= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right) \boldsymbol{\widehat\beta} \\\\
\boldsymbol{\widehat\beta} &= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{y}
\end{align}
$$

La diferencia entre el estimador *OLS* y el anterior es que se le añade $\lambda$ a la diagonal de $\mathbf{X}^\prime \mathbf{X}$ (este es el *ridge*). Como consecuencia, se minimizan posibles problemas por colinealidad y la varianza de los coeficientes en $\boldsymbol{\widehat\beta}$ es menor.

Esta menor varianza implica sesgo (*bias-variance trade-off*)

$$
\begin{align}
\mathbb{E}\left[ \boldsymbol{\widehat\beta} \right] &= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbb{E}\left[ \mathbf{y} \right] \\\\
&= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\text{VAR}\left[ \boldsymbol{\widehat\beta} \right] &= \text{VAR}\left[\left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{y} \right] \\\\
&= \text{VAR}\left[\left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{\epsilon} \right] \\\\
&= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \, \sigma^2 \mathbf{I} \, \mathbf{X} \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \\\\
&= \sigma^2 \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{X} \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1}
\end{align}
$$

Si $\lambda$ no es fijado sino estimado, entonces tendríamos que preocuparnos de su distribución.

Una de las principales ventajas de esta regresión es que permite comprobar con gran precisión la contribución de los predictores sin necesidad de preocuparse de qué variables prescindir.

<br>

## Regresión *Lasso*

<br>

Esta regresión sustituye la penalización de $\| \boldsymbol{\beta} \|^2$ por otra medida de la longitud del vector:

$$
\| \boldsymbol{\beta} \|_q = \left( \sum_{i=1}^p b_i^q \right)^{1/q}
$$

Si $q = 2$, entonces la penalización es la misma que en la regresión *ridge*. La regresión *lasso* (*least angle selection and shrinkage operator*) utiliza la penalización $q = 1$.

<br>

## Corrección de Bonferroni

<br>

Supongamos que estamos interesados en que el error tipo I de $k$ coeficientes sea, conjuntamente, $1 - \alpha$. Una forma común de lograrlo es establecer el error tipo I de cada coeficiente en $\alpha / k$ (o, equivalentemente, multiplicar el valor p por $k$). Sin embargo, esto solo es exacto si los coeficientes son independientes.

En un contraste estándar, cada coeficiente $\beta_i$ posee un conjunto de confianza $C_i(\alpha)$ de manera que

$$
\begin{align}
P(\beta_i \in C_i(\alpha)) &= 1 - \alpha \\\\
P(\beta_i \notin C_i(\alpha)) &= \alpha \\\\
P\left( \bigcup_i^k \beta_i \notin C_i(\alpha) \right) &= \sum_{i}^k P(\beta_i \notin C_i(\alpha)) - P(\text{Conjunto de intersecciones en } \beta) \\\\
&= k \alpha - P(\text{Conjunto de intersecciones en } \beta) \\\\
&\leq k \alpha \\\\
\end{align}
$$

$$
\begin{align}
1 - P\left( \bigcup_i^k \beta_i \notin C_i(\alpha) \right) &\leq 1 - k \alpha \\\\
1 - P\left( \bigcup_i^k \beta_i \notin C_i(\alpha / k) \right) &\leq 1 - \alpha
\end{align}
$$

Por tanto, tras dividir por el número de contrastes $k$, la probabilidad de que, al menos, un solo parámetro no se encuentre en su intervalo de confianza es igual o inferior a $\alpha$. Por esta razón, la correción de Bonferroni es conservadora.

Esta corrección tan solo exige que

$$
C(\alpha) = \prod_{i}^k C(\alpha_i / k)
$$

o equivalentemente,

$$
\alpha = \sum_{i}^k \alpha_i/k
$$

Por tanto, es legítimo asumir distintos niveles de error tipo I $(\alpha/k)$ en cada contraste siempre que la suma de ellos equivalga a $\alpha$.

<br>

## Elipsoides de confianza

<br>

Si nuestros coeficientes siguen una dsitribución normal y son independientes unos de otros,

$$
\begin{align}
\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]} \sim N(0, 1) \\\\
\sum_{i=1}^k \left(\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]}\right)^2 \sim \chi^2_k \\\\
\end{align}
$$

El intervalo $1 - \alpha$ de confianza vendría dado por

<br>

$$
P\left(\sum_{i=1}^k \left(\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]}\right)^2 \leq \;\;  \chi^2_{k, 1-\alpha} \right) = 1 - \alpha
$$

Esta región de confianza es un elipsoide.

Sin embargo, si los coeficientes covarían y la matriz de varianzas-covarianzas de los coeficientes es $\boldsymbol{\Sigma}$, entonces

<br>

$$
\begin{align}
\boldsymbol{\Sigma} &= \mathbf{VUV}^\prime \\\\
\boldsymbol{\Sigma}^{1/2} &= \mathbf{VU}^{1/2} \mathbf{V}^\prime \\\\
\text{VAR}\left[ \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \right] &= 
\boldsymbol{\Sigma}^{-1/2} \, \text{VAR}\left[ \boldsymbol{\widehat\beta - \boldsymbol{\beta}} \right] \, \left(\boldsymbol{\Sigma}^{-1/2}\right)^\prime \\\\
&= \left(\mathbf{VU}^{1/2} \mathbf{V}^\prime\right)^{-1} \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\left(\mathbf{VU}^{1/2} \mathbf{V}^\prime\right)^{-1}\right)^\prime \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\mathbf{VU}^{-1/2} \mathbf{V}^\prime\right)^\prime \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\mathbf{VU}^{-1/2} \mathbf{V}^\prime\right)^\prime \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \mathbf{VU}^{-1/2} \mathbf{V}^\prime \\\\
&= \mathbf{VU}^{-1/2} \mathbf{UU}^{-1/2} \mathbf{V}^\prime \\\\
&= \mathbf{VV}^\prime \\\\
&= \mathbf{I}
\end{align}
$$

$$
\begin{align}
\boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim MVN(\mathbf{0}, \mathbf{I}) \\\\
\left(\boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})\right)^\prime \; \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k \\\\
(\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \; \left(\boldsymbol{\Sigma}^{-1/2} \right)^\prime \; \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k \\\\
(\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k
\end{align}
$$

<br>

$$
P\left( (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \, \leq \,  \chi^2_{k, 1-\alpha} \right) = 1 - \alpha
$$

Si se utiliza $\widehat\sigma^2$ para estimar el error típico, entonces debemos emplear la distribución $F$:

$$
P\left( (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \, \leq \,  F_{q, \, n-p-1, (1-\alpha)} \right) = 1 - \alpha
$$

<br>

## Interacciones

<br>

Si la verdadera función de regresión es $\mathbb{E}\left[Y \, | \, X_1, X_2\right] = \mu(X_1, X_2)$, que incluye una interacción de forma desconocida entre las variables $X_1$ y $X_2$, ¿por qué representar la interacción entre $X_1$ y $X_2$ como $X_1X_2$?


$$
Y = \beta_0 + X_1 \beta_1 + X_2 \beta_2 + X_1X_2\beta_3 + \epsilon
$$


Si hiciéramos una expansión de Taylor,

$$
\mu(X_1, X_2) \approx \mu(X_1^*, X_2^*) + \sum_{i=1}^p (x_i - x_i^*) \, \left. \frac{\partial{\mu}}{\partial x_i} \right|_{x_i = x_i^*} + \sum_{j=1}^p (x_j - x_j^*) \, \left. \frac{\partial{\mu}}{\partial x_j} \right|_{x_j = x_j^*} + \frac{1}{2} \sum_{i=1}^p \sum_{j=1}^p (x_i - x_i^*) (x_j - x_j^*) \left. \frac{\partial^2 \mu}{\partial x_i x_j} \right|_{x_ix_j = x_i^*x_j^*}
$$

entonces podemos comprobar que una buena aproximación incluye el producto entre las variables $X_1$ y $X_2$ así como sus términos cuadráticos.

<br>

## Distribución de los valores p bajo la hipótesis nula

<br>

$$
\begin{align}
P &= F(T) \\\\
\text{Pr}(P < p) &= \text{Pr}\left(F^{-1}(P) < F^{-1}(p)\right) \\\\
&= \text{Pr}(T < t) \\\\
&= F\left(F^{-1}(p)\right) \\\\
&= p
\end{align}
$$

<br>

## Desigualdad de Chebyshev

<br>

$$
\text{P}(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}
$$

La probabilidad de que una variable aleatoria $X$ exceda su valor esperado $\mu$ por $k$ errores típicos es siempre inferior a $\frac{1}{k^2}$.

<br>




## *Capture percentage*

<br>

La esperanza de cobertura del intervalo de confianza para la distribución de $\widehat\beta_0$ o lo que es lo mismo, el promedio de veces que un intervalo de confianza contiene otras estimaciones de $\beta_0$, es:

<br>

$$
\int \phi\left(\widehat\beta_0; \, \beta_0, \text{se}\right) \left(\Phi\left(\widehat\beta_0 + 1.96 \cdot \text{se}; \, \beta_0, \text{se}\right) - \Phi\left(\widehat\beta_0 - 1.96 \cdot \text{se}; \, \beta_0, \text{se}\right)\right) d\widehat\beta_0
$$

<br>


