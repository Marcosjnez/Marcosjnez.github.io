---
title: "Estimación <br> <br>"
output: 
  html_document:
    code_folding: hide
    css: style.css
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

## Distribución normal

<br>

$$
\begin{align}
\mathbf{x} &= x_i, \dots, x_n \\\\
P(\mathbf{x}; \mu, \sigma) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
L(\mu, \sigma; \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &= \sum_{i=1}^n -\frac{(x_i - \mu)^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi}) \\\\
\text{log}(L) &= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi)
\end{align}
$$

<br>

*Location-scale property*

<br>

$$
\begin{align}
Z &\sim \mathcal{N}(\mu, \sigma^2) \\\\
a + bZ &\sim \mathcal{N}(a + \mu, b^2\sigma^2)
\end{align}
$$

<br>

*Stability property*

<br>

Si $Z_{IID} \sim \mathcal{N}(\mu, \sigma^2)$, entonces

$$
\sum_{i=1}^n Z_i \sim \mathcal{N}\left(n \mu, n \sigma^2\right)
$$

<br>

### Estimación de $\mu$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu}&= \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) \\\\
&= \frac{n}{\sigma^2} \, (\bar{x} - \mu)
\end{align}
$$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu} &= 0 \\\\
\widehat\mu &= \bar{x} \\\\
\end{align}
$$

<br>

### Valor esperado de $\bar{x}$

<br>

$$
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n x_i \right] = \mu
$$

<br>

### Varianza de $\widehat\mu$

<br>

$$
\begin{align}
\frac{\partial^2 \text{log}(L)}{\partial \mu^2} &= - \frac{n}{\sigma^2} \\\\
\text{Var}\left[\widehat\mu\right] &= \mathbb{E}\left[(\widehat\mu - \mu)^2\right] \\\\
&= \frac{\sigma^2}{n}
\end{align}
$$

<br>

### Estimación de $\sigma^2$

<br>

$$
\begin{align}
\text{log}(L) &= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi) \\\\
\end{align}
$$

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= 0 \\\\
\sigma^2 &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{n}
\end{align}
$$

<br>

### Valor esperado de $\frac{\sum_{i=1}^n (x_i - \widehat\mu)^2}{n}$

<br>

$$
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &= \frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n ((x_i - \mu) - (\widehat\mu - \mu))^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2 + (\widehat\mu - \mu)^2 - 2(x_i - \mu)(\widehat\mu - \mu))\right] \\\\
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &= n\sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (\widehat\mu - \mu)^2\right] &= \sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)(\widehat\mu - \mu) \right] &= \mathbb{E}\left[\mathcal{N}(\mu^2 + \bar{x}\widehat\mu - \bar{x}\mu - \widehat\mu\mu)\right] \\\\
&= \mathbb{E}\left[\mathcal{N}(\mu^2 + \widehat\mu^2 - 2\widehat\mu\mu)\right] \\\\
&= n \, \mathbb{E}\left[(\widehat\mu - \mu)^2 \right] \\\\
&= \sigma^2
\end{align}
$$

<br>

$$
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &= \sigma^2 + \frac{\sigma^2}{n} - \frac{2\sigma^2}{n} \\\\
&= \sigma^2 \left(\frac{n - 1}{n}\right) \\\\
\end{align}
$$
$$
\begin{align}
\sigma^2 &= \frac{\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right]}{n - 1}
\end{align}
$$

<br>

$$
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n - 1}
$$

<br>

### Varianza de $\sigma^2$ y $\widehat\sigma^2$

<br>

$$
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial^2 \text{log}(L)}{\partial \sigma^4} &= - \frac{\sum_{i=1}^n (x_i - \mu)^2}{\sigma^6} + \frac{n}{2\sigma^4} \\\\
&= \frac{- 2\sum_{i=1}^n (x_i - \mu)^2 + n\sigma^2}{2\sigma^6} \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &= n\sigma^2 \\\\
\mathbb{E}\left[\frac{\partial^2 \text{log}(L)}{\partial \sigma^4}\right] &= \frac{- 2n\sigma^2 + n\sigma^2}{2\sigma^6} \\\\
&= - \frac{n}{2\sigma^4} \\\\
\text{Var}\left[\sigma^2\right] &= \frac{2\sigma^4}{n}
\end{align}
$$

<br>

$$
\begin{align}
\frac{\epsilon_i}{\sigma} &\sim \mathcal{N}(0, 1) \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 &\sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 = \frac{(n-1)\widehat\sigma^2}{\sigma^2} &\sim \chi^2_{n-1} \\\\
\text{Var}\left[\chi^2_{n-1}\right] &= 2(n-1) \\\\
\text{Var}\left[\frac{(n-1)\widehat\sigma^2}{\sigma^2}\right] &= 2(n-1) \\\\
\text{Var}\left[(n-1)\widehat\sigma^2\right] &= 2\sigma^4(n-1) \\\\
\text{Var}\left[\widehat\sigma^2\right] &= \frac{2\sigma^4}{n-1} \\\\
\end{align}
$$

<br>

## Función afín

<br>

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_i + \epsilon_i \\\\
y_i &= \widehat{\beta}_0 + \widehat{\beta}_1x_i + e_i
\end{align}
$$

<br>

$$
\begin{align}
P(\mathbf{y}; \beta_0, \beta_1, \sigma, \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
L(\beta_0, \beta_1, \sigma; \mathbf{y}, \mathbf{x}) &= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &= \sum_{i=1}^n -\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi})
\end{align}
$$

<br>

### Residuos

<br>

$$
\begin{align}
\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})e_i &= 0, \quad \text{Cov}[X, e] = 0 \\\\
\frac{1}{n} \sum_{i=1}^n e_i &= 0
\end{align}
$$

<br>

### Estimación de $\beta_1$

<br>

$$
\begin{align}
\frac{\partial L}{\partial \beta_1} &= - \frac{\sum_{i=1}^n \beta_0x_i + \beta_1x_i^2 - y_ix_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_1} &= 0 \\\\
&= \overline{yx} - \beta_0\bar{x} - \beta_1\overline{x^2} \\\\
\end{align}
$$
$$
\begin{align}
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - \beta_0\mathbb{E}[X] &= 0 \\\\
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - (\mathbb{E}[Y] - \beta_1\mathbb{E}[X]) \, \mathbb{E}[X] &= 0 \\\\
\text{Cov}[YX] - \beta_1\mathbb{E}[X^2] - \beta_1\mathbb{E}[X]^2 &= 0 \\\\
\text{Cov}[YX] - \beta_1\text{Var}[X] &= 0 \\\\
\end{align}
$$
$$
\begin{align}
\beta_1 &= \frac{\text{Cov}[YX]}{\text{Var}[X]} \\\\
\end{align}
$$

<br>

$$
\begin{align}
\frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} &= \frac{\sum_{i=1}^n x_i  y_i - \bar{x} \bar{y}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{\sum_{i=1}^n x_i (\beta_0 + \beta_1x_i + \epsilon_i) - \bar{x} (\beta_0 + \beta_1\bar{x} + \bar{\epsilon})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{n\beta_1(\overline{x^2} - \bar{x}^2)}{\sum_{i=1}^n (x_i - \bar{x})^2} + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_1 + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
\end{align}
$$

<br>

Según la ley de la esperanza total,

$$
\begin{align}
\mathbb{E}\left[\widehat{\beta}_1\right] &= \mathbb{E}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_i, \dots, x_n\right]\right] \\\\
&= \mathbb{E}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \mathbb{E}[\epsilon_i]}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \, , \quad \mathbb{E}[\epsilon \, | \, x_i] = 0 \\\\
&= \beta_1 \\\\
\widehat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{align}
$$

<br>

### Estimación de $\beta_0$

<br>

$$
\begin{align}
\frac{\partial L}{\partial \beta_0} &=- \frac{\sum_{i=1}^n \beta_0 + \beta_1x_i - y_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_0} &= 0 \\\\
\widehat{\beta}_0 &= \bar{y} - \widehat{\beta}_1\bar{x} \\\\
\mathbb{E}\left[\widehat{\beta}_0\right] &= \beta_0 + \beta_1\mathbb{E}[X] - \beta_1\mathbb{E}[X] \\\\
&= \beta_0
\end{align}
$$

<br>

### Varianza de $\widehat{\beta}_1$

<br>

$$
\begin{align}
\text{Var}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right] &= \text{Var}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \\\\
&= \sum_{i=1}^n \frac{(x_i - \bar{x})^2 \, \text{Var}\left[\epsilon_i\right]}{n^2 s_x^4} \\\\
&= \frac{\sigma^2}{n s_x^2}
\end{align}
$$

Aplicando la ley de la varianza total,

$$
\begin{align}
\text{Var}\left[\widehat{\beta}_1\right] &= \text{Var}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{Var}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2}{n s_x^2} \\\\
\widehat{\text{Var}}\left[\widehat{\beta}_1\right] &= \frac{\widehat\sigma^2}{n s_x^2}
\end{align}
$$

Dado que $\epsilon_i$ sigue una distribución normal, la propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{\beta}_1 &\sim \mathcal{N}\left(\beta_1, \frac{\sigma^2}{ns_x^2} \right) \\\\\
\frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} &\sim \mathcal{N}\left(0, 1 \right) \\\\\
\end{align}
$$

Si conociéramos $\sigma^2$, entonces

$$
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
$$

Pero si estimamos $\sigma^2$ a través de $\widehat\sigma^2$, entonces

$$
\begin{align}
\frac{\widehat{\beta}_1 - \beta_1}{\widehat\sigma / s_x\sqrt{n}} &= \frac{\frac{\widehat{\beta}_1 - \beta_1}{\sigma}}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&\sim \frac{\mathcal{N}(0, 1/ns_x^2)}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&\sim \frac{\mathcal{N}(0, 1)}{\frac{\widehat\sigma}{\sigma}} \\\\
&\sim \frac{\mathcal{N}(0, 1)}{\sqrt{\frac{\sum_{i=1}^n e_i^2}{\sigma^2 (n-2)}}} \\\\
&\sim \frac{\mathcal{N}(0, 1)}{\sqrt{\frac{\chi^2_{n-2}}{n-2}}} \\\\
&\sim t_{n-2}
\end{align}
$$

<br>

### Varianza de $\widehat{\beta}_0$

<br>

$$
\begin{align}
\text{Var}\left[ \widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &= \text{Var}\left[\bar{y} - \widehat{\beta}_1\bar{x}\right] \\\\
&= \text{Var}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon} - \widehat{\beta}_1\bar{x}\right] \\\\
&= \text{Var}\left[\bar{\epsilon}\right] + \bar{x}^2 \, \text{Var}\left[\widehat{\beta}_1\right] - 2\bar{x}^2\text{Cov}\left[\bar{\epsilon}, \widehat{\beta}_1\right]
\end{align}
$$

<br>

$$
\text{Cov}\left[\bar{\epsilon}, \widehat{\beta}_1\right] = 0
$$

<br>

$$
\begin{align}
\text{Var}\left[\widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &= \frac{\sigma^2}{n} + \frac{\sigma^2 \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2} - \sigma^2 s_x^2}{n s_x^2} \\\\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2}}{n s_x^2} - \frac{\sigma^2}{n} \\\\
&= \frac{\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
$$

<br>

Según la ley de la esperanza total,

$$
\begin{align}
\text{Var}\left[\widehat{\beta}_0\right] &= \text{Var}\left[\mathbb{E}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{Var}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2 \overline{x^2}}{n s_x^2} \\\\
\widehat{\text{Var}}\left[\widehat{\beta}_0\right] &= \frac{\widehat\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
$$

Dado que $\epsilon_i$ sigue una distribución normal, la propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{\beta}_0 &\sim \mathcal{N}\left(\beta_0, \frac{\sigma^2 \overline{x^2}}{n s_x^2} \right) \\\\\
\frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} &\sim \mathcal{N}\left(0, 1 \right) \\\\\
\end{align}
$$

Si conociéramos $\sigma^2$, entonces

$$
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
$$

Pero si estimamos $\sigma^2$ a través de $\widehat\sigma^2$, entonces

$$
\frac{\widehat{\beta}_0 - \beta_0}{\widehat\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \sim t_{n-2}
$$

<br>

### Predicciones

<br>

$$
\begin{align}
\widehat{m}(x^*) &= \widehat{\beta}_0 + \widehat{\beta}_1x^* \\\\
&= \bar{y} - \widehat{\beta}_1\bar{x} + \widehat{\beta}_1x^* \\\\
&= \bar{y} + (x^* - \bar{x})\widehat{\beta}_1 \\\\
&= \beta_0 + \beta_1\bar{x} + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x^* - \bar{x})\left(\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\\\
&= \beta_0 + \beta_1\bar{x} + (x^* - \bar{x})\beta_1 + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x^* - \bar{x}) \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&= \beta_0 + \beta_1x^* + \frac{1}{n} \sum_{i=1}^n \epsilon_i \left(1 + (x^* - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right)
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[\widehat{m}(x^*)\right] &= \beta_0 + \beta_1x^* + \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\epsilon_i] \left(1 + (x^* - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right) \\\\
&= \beta_0 + \beta_1x^*
\end{align}
$$

<br>

### Varianza del promedio de las predicciones

<br>

$$
\begin{align}
\text{Var}\left[\widehat{m}(x^*) \, | \, x_i, \dots, x_n \right] &= \frac{1}{n^2} \sum_{i=1}^n \text{Var}[\epsilon_i] \left(1 + (x^* - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right)^2 \\\\
&= \frac{\sigma^2}{n^2} \sum_{i=1}^n \left(1 + (x^* - \bar{x})^2 \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&= \frac{\sigma^2}{n^2} \left(n + (x^* - \bar{x})^2 \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&= \frac{\sigma^2}{n^2} \left(n + (x^* - \bar{x})^2 \frac{n}{s_x^2}\right) \\\\
&= \frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)
\end{align}
$$

Según la ley de la esperanza total,

$$
\begin{align}
\text{Var}\left[\widehat{m}(x^*)\right] &= \text{Var}\left[\mathbb{E}\left[\widehat{m}(x^*) \, | \, x_i, \dots, x_n\right]\right] + \mathbb{E}\left[\text{Var}\left[\widehat{m}(x^*) \, | \, x_i, \dots, x_n\right]\right] \\\\
&= \frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)
\end{align}
$$

<br>

La propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{m}(x^*) &\sim \mathcal{N}\left(\beta_0 + \beta_1x^*, \frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)\right) \\\\
\frac{\widehat{m}(x^*) - m(x^*)}{\sqrt{\frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)}} &\sim \mathcal{N}(0, 1) \\\\
\frac{\widehat{m}(x^*) - m(x^*)}{\sqrt{\frac{\widehat\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right)}} &\sim t_{n-2} \\\\
\end{align}
$$

<br>

### Varianza del error de predicción

<br>

$$
\begin{align}
y^* &= \beta_0 + \beta_1x^* + \epsilon^* \\\\
\widehat{m}(x^*) &= \widehat\beta_0 + \widehat\beta_1x^* \\\\
y^* - \widehat{m}(x^*) &= \beta_0 + \beta_1x^* - \widehat\beta_0 - \widehat\beta_1x^* + \epsilon^* \\\\
\text{Cov}[\widehat{m}(x^*), \epsilon^*] &= 0 \\\\
\text{Var}[y^* - \widehat{m}(x^*)] &= \frac{\sigma^2}{n} \left(1 + \frac{(x^* - \bar{x})^2}{s_x^2}\right) + \sigma^2 \\\\
\text{Var}[y^* - \widehat{m}(x^*)] &= \sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{n s_x^2}\right)
\end{align}
$$

<br>

La propiedad de estabilidad asegura que

$$
\begin{align}
\widehat{m}(x^*) &\sim \mathcal{N}\left(y^*, \sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{n s_x^2}\right)\right) \\\\
\frac{y^* - \widehat{m}(x^*)}{\sqrt{\sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{n s_x^2}\right)}} &\sim \mathcal{N}(0, 1) \\\\
\frac{y^* - \widehat{m}(x^*)}{\sqrt{\widehat\sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{n s_x^2}\right)}} &\sim t_{n-2}
\end{align}
$$

<br>

### Varianza de los valores ajustados

<br>

$$
\begin{align}
\widehat{\beta}_0 &= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
$$

<br>

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_i + \epsilon_i \\\\
\text{Var}\left[y_i\right] &= \sigma^2 \\\\
\widehat{m}_i &= \widehat\beta_0 + \widehat\beta_1x_i \\\\
&= \bar{y} - \widehat\beta_1\bar{x} + \widehat\beta_1x_i \\\\
&= \bar{y} + (x_i - \bar{x})\widehat\beta_1 \\\\
&= \bar{y} + (x_i - \bar{x})\left(\beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
&= \beta_0 + \beta_1x_i + (x_i - \bar{x})\left(\frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
\text{Var}\left[\widehat{m}_i\right] &=  \frac{\sigma^2}{n} + (x_i - \bar{x})^2 \frac{\sigma^2}{ns_x^2} \\\\
&= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
\end{align}
$$

<br>

### Varianza del error de estimación (Varianza de un residuo)

<br>

$$
\mathbb{E}\left[(y_i - \widehat{m}_i)^2\right] = \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\,\text{Cov}\left[y_i, \widehat{m}_i\right]
$$

<br>

$$
\begin{align}
\text{Cov}\left[y_i, \widehat{m}_i\right] &= \text{Cov}\left[y_i + \widehat{m}_i - \widehat{m}_i, \widehat{m}_i \right] \\\\
&= \text{Cov}\left[\widehat{m}_i, \widehat{m}_i \right] + \text{Cov}\left[y_i - \widehat{m}_i, \widehat{m}_i \right] \\\\
&= \text{Cov}\left[\widehat{m}_i, \widehat{m}_i \right] + \text{Cov}\left[e_i, \widehat{m}_i \right] \\\\
&= \text{Var}\left[\widehat{m}_i\right] + 0 \\\\
&= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
$$

<br>

$$
\begin{align}
\mathbb{E}\left[(y_i - \widehat{m}_i)^2\right] &= \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&= \sigma^2 - \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&= \sigma^2 \left(1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
$$

<br>

### Varianza de la estimación del error cuadrático medio, $\widehat\sigma^2$

<br>

$$
\mathbb{E}\left[(Y - (\beta_0 + \beta_1X))^2\right] = \sigma^2
$$

<br>

$$
\begin{align}
\text{Var}\left[\bar{y} \, | \, x_i, \dots, x_n\right] &= \text{Var}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon}\right] \\\\
&= \frac{1}{n^2} \sum_{i=1}^n \text{Var}\left[\epsilon_i\right] \\\\
&= \frac{\sigma^2}{n}
\end{align}
$$

<br>

$$
\begin{align}
\widehat{\beta}_0 &= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
$$

<br>

$$
\begin{align}
 \mathbb{E}\left[\sum_{i=1}^n e_i^2 \, | \, x_i, \dots, x_n\right] &= \mathbb{E}\left[\sum_{i=1}^n (y_i - \widehat{m}_i)^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n \left(\beta_0 + \beta_1x_i + \epsilon_i - \widehat{\beta}_0 - \widehat{\beta}_1x_i\right)^2\right] \\\\
&= \mathbb{E}\left[\sum_{i=1}^n \left((\beta_0 - \widehat{\beta}_0)^2 + (\beta_1 - \widehat{\beta}_1)^2x_i^2 + \epsilon_i^2\right)\right] + \\\\
& 2 \, \mathbb{E}\left[\sum_{i=1}^n (\beta_0 - \widehat{\beta}_0)(\beta_1 - \widehat{\beta}_1)x_i + (\beta_0 - \widehat{\beta}_0)\epsilon_i + (\beta_1 - \widehat{\beta}_1)x_i\epsilon_i \right] \\\\
&= n \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2}}{n s_x^2}\right] + 1 \right) - 2\sigma^2 \, \mathbb{E}\left[\frac{\bar{x}^2}{s_x^2} \right] - 2 \sigma^2 - 2 \sigma^2 \\\\
&= \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2} - \bar{x}^2}{s_x^2}\right] + n - 4 \right) \\\\
&= \sigma^2 \left(n - 2 \right) \\\\
\end{align}
$$

<br>

$$
\widehat\sigma^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2 \\\\
$$

$\widehat{\sigma}^2$ es estadísticamente independiente de $\widehat\beta_1$ y $\widehat\beta_0$ a pesar de que todos los estimadores son una función de $\epsilon$.

<br>

Sabiendo que si $Z \sim \mathcal{N}(0, 1)$, entonces $Z^2 \sim \chi^2$ y que $\frac{\epsilon_i}{\sigma} \sim \mathcal{N}(0, 1)$, entonces

<br>

$$
\begin{align}
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 &= \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 \sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 &\sim \chi^2_{n-2} \\\\
P\left(\chi^2_{n-2} \; \leq \; \chi^2_{n-2, \, 1-\alpha}\right) &= 1 - \alpha \\\\
P\left(\sigma^2 \; \leq \; \frac{(n-2) \widehat\sigma^2}{\chi^2_{n-2, \, 1-\alpha}}\right) &= 1 - \alpha \\\\
\text{Var}\left[\chi^2_{n-2}\right] &= 2(n-2) \\\\
\text{Var}\left[\frac{(n-2)\widehat\sigma^2}{\sigma^2}\right] &= 2(n-2) \\\\
\text{Var}\left[(n-2)\widehat\sigma^2\right] &= 2\sigma^4(n-2) \\\\
\text{Var}\left[\widehat\sigma^2\right] &= \frac{2\sigma^4}{n-2} \\\\
\end{align}
$$

<br>

### Modelo sin predictores

<br>

$$
\begin{align}
Y &= \beta_0 + \epsilon \\\\
\widehat\beta_0 &= \bar{y} \sim \mathcal{N}\left(\beta_0, \frac{\sigma^2}{n}\right) \\\\
\widehat\sigma^2 &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \\\\
&= s_y^2 \\\\
\frac{(n-1) \widehat\sigma^2}{\sigma^2} &\sim \chi^2_{n-1}
\end{align}
$$

<br>

## El test F

<br>

La distribución $\chi^2$ procede de la suma del cuadrado de variables con distribución normal y la distribución $F$ procede de la razón entre variables aleatorias con distribución $\chi^2$.

$$
\frac{\chi^2_a}{\chi^2_b} \frac{b}{a} \sim F_{a, b}
$$

Según el modelo sin predictores, 

$$
\frac{(n-1) \widehat\sigma^2_{null}}{\sigma^2} \sim \chi^2_{n-1}
$$

mientras que según el modelo con un predictor,

$$
\frac{(n-2) \widehat\sigma_2^2}{\sigma^2} \sim \chi^2_{n-2}
$$

Por tanto,

$$
\begin{align}
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{\sigma^2} &\sim \chi^2_1 \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{(n-2) \widehat\sigma_2^2} &\sim \frac{\chi^2_1}{\chi^2_{n-2}} \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{(n-2) \widehat\sigma_2^2} \, \frac{n-2}{1} &\sim F_{1, n-2} \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{\widehat\sigma_2^2} &\sim F_{1, n-2}
\end{align}
$$

El test $F$ asume que los errores ($\epsilon$) siguen una distribución normal, son homocedásticos e independientes de la variable $X$ y entre sí. Se trata de un caso particular de la razón de verosimilitudes.

<br>

En el caso general, si estuviéramos contrastando la significación de $p-q$ estimaciones,

$$
\begin{align}
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{\sigma^2} &\sim \chi^2_{p-q} \\\\
\frac{(n-p) \widehat\sigma^2_{p}}{\sigma^2} &\sim \chi^2_{n-p} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(n-p) \widehat\sigma^2_{p}} &\sim \frac{\chi^2_{p-q}}{\chi^2_{n-p}} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(n-p) \widehat\sigma^2_{p}} \, \frac{n-p}{p-q} &\sim F_{p-q, n-p} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(p-q) \widehat\sigma^2_{p}} &\sim F_{p-q, n-p}
\end{align}
$$

<br>

## El test de razón de verosimilitudes

<br>

El espacio paramétrico del modelo nulo de $q$ coeficientes es un subconjunto del espacio paramétrico del modelo general de $p$ coeficientes.

El test de razón de verosimilitudes asume que las estimaciones de los parámetros siguen una distribución normal y esto es solo aproximadamente cierto para $\widehat\sigma^2$. Por tanto, solo es exactamente correcto cuando $n \rightarrow \infty$.

$$
\begin{align}
\Lambda &= L(\widehat\Theta) - L(\widehat\theta) \\\\
\lim_{n \rightarrow \infty} \quad 2\Lambda &\sim \chi^2_{p-q}
\end{align}
$$

<br>

$$
\begin{align}
L(\widehat\theta) &= - \frac{n}{2} \ \log2\pi - \frac{n}{2} \ \log (n-q)\widehat\sigma^2_q - \frac{1}{2(n-q)\widehat\sigma^2_q} \sum_{i=1}^n (y_i - \widehat\beta_0)^2 \\\\
&= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log (n-q)\widehat\sigma^2_q} \\\\
L(\widehat\Theta) &= - \frac{n}{2} \ \log2\pi - \frac{n}{2} \ \log (n-p)\widehat\sigma^2_p - \frac{1}{2(n-p)\widehat\sigma^2_p} \sum_{i=1}^n (y_i - \widehat\beta_0 - \widehat\beta_1x_i)^2 \\\\
&= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\ \log (n-p)\widehat\sigma^2_p}
\end{align}
$$

<br>

$$
\begin{align}
L(\widehat\Theta) - L(\widehat\theta)
&= \frac{n}{2} \log\frac{(n-q)\widehat\sigma^2_q}{(n-p)\widehat\sigma^2_p} \\\\
\lim_{n \rightarrow \infty} \quad n \log\frac{(n-q)\widehat\sigma^2_q}{(n-p)\widehat\sigma^2_p} &\sim \chi^2_{p-q}
\end{align}
$$

Incluso cuando ambos modelos son falsos, el test de razón de verosimilitudes nos puede indicar cuál de ellos se parece más al verdadero.

<br>

## $R^2$

<br>

$$
\begin{align}
R^2 &= \frac{\text{Cov}\left[y, \widehat{m}\right]}{s_y^2} \\\\
\text{Cov}\left[y, \widehat{m}\right] &= \text{Cov}\left[\widehat{m} + e, \widehat{m}\right] \\\\
&= s_\widehat{m}^2 + \text{Cov}\left[e, \widehat{m}\right] \\\\
&= s_\widehat{m}^2 \\\\
R^2 &= \frac{s_\widehat{m}^2}{s_y^2} \\\\
s_\widehat{m}^2 &= s_{\widehat{\beta}_0 + \widehat{\beta}_1X}^2 \\\\
&= \widehat{\beta}_1^2 s_x^2 \\\\
R^2 &= \widehat{\beta}_1^2 \frac{s_x^2}{s_y^2} \\\\
&= \left(\frac{\text{Cov}\left[x, y\right]}{s_x s_y}\right)^2 \\\\
&= \frac{s_y^2 - \overline{e^2}}{s_y^2} \\\\
R_{\text{ajustado}}^2 &= \frac{\widehat{s}_y^2 - \widehat\sigma^2}{\widehat{s}_y^2}
\end{align}
$$

<br>

$$
\begin{align}
R^2 &= \frac{\text{Var}\left[m(X)\right]}{\text{Var}\left[Y\right]} \\\\
&= \frac{\text{Var}\left[\beta_0 + \beta_1X\right]}{\text{Var}\left[\beta_0 + \beta_1X + \epsilon\right]} \\\\
R^2 &= \frac{\beta_1^2 \text{Var}\left[X\right]}{\beta_1^2 \text{Var}\left[X\right] + \sigma}
\end{align}
$$

<br>

## $\rho_{xy}$

<br>

$$
\begin{align}
\rho_{XY} &= \frac{\text{Cov}\left[X, Y\right]}{\sqrt{\text{Var}\left[X\right] \text{Var}\left[Y\right]}} \\\\
&= \beta_1 \frac{\text{Var}\left[X\right]}{\sqrt{\text{Var}\left[Y\right]}}
\end{align}
$$

<br>

## Notación matricial

> \newcommand{\m}[1]{\mathbf{#1}}
> \newcommand{\b}[1]{\boldsymbol{#1}}

<br>

$$
\begin{align}
\mathbf{y} &= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
\text{MSE} &= \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^T - \boldsymbol{\beta}^T \mathbf{X}^T) (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{y} + \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta}) \\\\
\mathbf{y}^T \mathbf{X} \boldsymbol{\beta} &= \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{y} \quad \text{porque} \; \mathbf{y}^T \mathbf{X} \boldsymbol{\beta} \; \text{es una matriz} \; 1 \times 1 \\\\
\text{MSE} &= \frac{1}{n} (\mathbf{y}^T \mathbf{y} - 2 \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{y} + \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

### Estimación de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\nabla \text{MSE} &= \frac{1}{n} (\nabla \mathbf{y}^T \mathbf{y} - 2 \nabla \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{y} + \nabla \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (0 - 2 \mathbf{X}^T \mathbf{y} + 2 \mathbf{X}^T \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{2}{n} (- \mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

$$
\begin{align}
\mathbf{X}^T \mathbf{X} \boldsymbol{\widehat\beta} - \mathbf{X}^T \mathbf{y} &= 0 \\\\
\boldsymbol{\widehat\beta} &= (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \mathbf{y}
\end{align}
$$

<br>

### Varianza de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\mathbf{\widehat\beta} &= (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \mathbf{y} \\\\
&= (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}) \\\\
&= (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} + (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \boldsymbol{\epsilon} \\\\
&= \boldsymbol{\beta} + (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \boldsymbol{\epsilon} \\\\
\text{Var}\left[\boldsymbol{\widehat\beta}\right] &= (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \,  \text{Var}\left[\boldsymbol{\epsilon}\right] \left((\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T\right)^T \\\\
&= (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \, \sigma^2 \mathbf{I} \, \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \\\\
&= \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \, \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \\\\
&= \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
\end{align}
$$

<br>

### Distribución de $\boldsymbol{\beta}$

<br>

$$
\begin{align}
\widehat{\boldsymbol{\beta}} &\sim \mathcal{MVN}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}) \\\\
\widehat{\beta}_i &\sim \mathcal{N}(\beta_i, \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}_{ii}) \\\\
\frac{\widehat{\beta}_i - \beta_i}{\sigma \sqrt{(\mathbf{X}^T \mathbf{X})^{-1}_{ii}}} &\sim \mathcal{N}(0, 1) \\\\
\frac{\widehat{\beta}_i - \beta_i}{\widehat\sigma \sqrt{(\mathbf{X}^T \mathbf{X})^{-1}_{ii}}} &\sim t_{n-p-1}
\end{align}
$$

<br>

### Valores ajustados

<br>

*Hat* o *influence matrix*:

$$
\begin{align}
\mathbf{H} &= \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \\\\
\mathbf{\widehat{m}}(\mathbf{X}) &= \mathbf{X} \boldsymbol{\widehat\beta} \\\\
\mathbf{\widehat{m}}(\mathbf{X}) &= \mathbf{H} \mathbf{y}
\end{align}
$$

Propiedades de $\mathbf{H}$:

$$
\begin{align}
\frac{\partial \widehat{m}_i}{\partial y_j} &= H_{ij} \\\\
\mathbf{H} &= \mathbf{H}^T \\\\
\mathbf{H}^2 &= \mathbf{H} \\\\
\end{align}
$$

<br>

### Varianza de los valores ajustados

<br>

$$
\begin{align}
\text{Var}\left[\mathbf{Hy}\right] &= \text{Var}\left[\mathbf{H} \left( \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} \right) \right] \\\\
&= \text{Var}\left[\mathbf{H} \boldsymbol{\epsilon}\right] \\\\
&= \mathbf{H} \, \text{Var}\left[\boldsymbol{\epsilon}\right] \, \mathbf{H}^T \\\\
&= \sigma^2 \mathbf{H}
\end{align}
$$

<br>

### Predicciones

<br>

$$
\begin{align}
\widehat{\mathbf{m}}(\mathbf{X^*}) &= \mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \mathbf{y} \\\\
&= \mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T (\mathbf{X} \boldsymbol{\widehat\beta} + \mathbf{e}) \\\\
&= \mathbf{X^*} \boldsymbol{\widehat\beta}
\end{align}
$$

<br>

### Varianza de las predicciones

<br>

$$
\begin{align}
\text{Var}\left[\widehat{\mathbf{m}}(\mathbf{X^*})\right] &= \text{Var}\left[\mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \mathbf{y}\right] \\\\
&= \text{Var}\left[\mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
&= \mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \, \text{Var}\left[\boldsymbol{\epsilon}\right] \, (\mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T)^T \\\\
&= \sigma^2 \mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \, \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} \, (\mathbf{X^*})^T \\\\
&= \sigma^2 \mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, (\mathbf{X}^*)^T
\end{align}
$$

<br>

### Varianza del error de predicción

<br>

$$
\begin{align}
\text{Cov}\left[\mathbf{y^*}, \widehat{\mathbf{m}}(\mathbf{X^*})\right] &= 0 \\\\
\text{Var}\left[\mathbf{y^*} - \widehat{\mathbf{m}}(\mathbf{X^*})\right] &= \text{Var}\left[\mathbf{y^*} \right] + \text{Var}\left[\widehat{\mathbf{m}}(\mathbf{X^*}) \right] \\\\
&= \sigma^2 + \sigma^2 \mathbf{X^*} (\mathbf{X}^T \mathbf{X})^{-1} \, (\mathbf{X}^*)^T
\end{align}
$$

<br>

### Residuos

<br>

$$
\begin{align}
\mathbf{e} &= \mathbf{y} - \mathbf{X}\boldsymbol{\widehat\beta} \\\\
&= \mathbf{y} - \mathbf{Hy} \\\\
&= (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
$$

Propiedades de $(\mathbf{I} - \mathbf{H}) \mathbf{y}$:

$$
\begin{align}
\frac{\partial e_i}{\partial y_j} &= (\mathbf{I} - \mathbf{H})_{ij} \\\\
(\mathbf{I} - \mathbf{H}) &= (\mathbf{I} - \mathbf{H})^T \\\\
(\mathbf{I} - \mathbf{H})^2 &= (\mathbf{I} - \mathbf{H}) \\\\
\end{align}
$$

<br>

$$
\begin{align}
\text{MSE} &= \frac{1}{n} \mathbf{e}^T \mathbf{e} \\\\
&= \frac{1}{n} \mathbf{y}^T (\mathbf{I} - \mathbf{H})^T \, (\mathbf{I} - \mathbf{H}) \mathbf{y} \\\\
&= \frac{1}{n} \mathbf{y}^T (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
$$

<br>

### Varianza de los residuos

<br>

$$
\begin{align}
\text{Var}\left[\mathbf{e}\right] &= \text{Var}\left[(\mathbf{I} - \mathbf{H}) (\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
\mathbf{H} \ \mathbf{X} \boldsymbol{\beta} &= \mathbf{X} \boldsymbol{\beta} \\\\
\text{Var}\left[\mathbf{e}\right] &= \text{Var}\left[(\mathbf{I} - \mathbf{H}) \boldsymbol{\epsilon}\right] \\\\
&= (\mathbf{I} - \mathbf{H}) \text{Var}\left[\boldsymbol{\epsilon}\right] (\mathbf{I} - \mathbf{H})^T \\\\
&= \sigma^2 (\mathbf{I} - \mathbf{H})
\end{align}
$$

<br>

### Varianza de la estimación del error cuadrático, $\widehat\sigma^2$

<br>

$$
\begin{align}
\frac{1}{n} \mathbb{E}\left[\mathbf{e}^T \mathbf{e}\right] &= \frac{1}{n} \mathbb{E}\left[((\mathbf{I} - \mathbf{H})\mathbf{e})^T (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \mathbb{E}\left[\mathbf{e}^T(\mathbf{I} - \mathbf{H})^T (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \mathbb{E}\left[\mathbf{e}^T (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&= \frac{1}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \text{Var}[\mathbf{e}]\right]\\\\
&= \frac{1}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \sigma^2 (\mathbf{I} - \mathbf{H}) \right] \\\\
&= \frac{\sigma^2}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \right] \\\\
\text{tr}[\mathbf{I}] &= n \\\\
\text{tr}[\mathbf{H}] &= p + 1 \\\\
\frac{1}{n} \mathbb{E}\left[ \mathbf{e}^T \mathbf{e} \right] &= \frac{\sigma^2}{n} (n - p - 1) \\\\
\sigma^2 &= \frac{\mathbb{E}\left[\mathbf{e}^T \mathbf{e}\right]}{n - p - 1} \\\\
\widehat\sigma^2 &= \frac{\mathbf{e}^T \mathbf{e}}{n - p - 1}
\end{align}
$$

$$
\frac{(n-p-1) \widehat\sigma^2}{\sigma^2} \sim \chi^2_{n-p-1}
$$

<br>

## Derivación alternativa

<br>

$$
\begin{align}
\mathbf{y} &= \mathbf{1}\beta_0 + \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
\text{MSE} &= \frac{1}{n} (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^T - \mathbf{1}^T\beta_0 - \boldsymbol{\beta} \mathbf{X}^T) (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} (\mathbf{y}^T - \mathbf{1}^T\beta_0 - \boldsymbol{\beta}^T \mathbf{X}^T) (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta})
\end{align}
$$

<br>

$$
\begin{align}
\frac{\partial \text{MSE}}{\partial \beta_0} &= \frac{1}{n}\left(- \nabla \mathbf{1}^T \beta_0 \mathbf{y} + \nabla \mathbf{1}^T \beta_0 \beta_0 \mathbf{1} + \\
\nabla \mathbf{1}^T \beta_0 \mathbf{X} \boldsymbol{\beta} - \nabla \mathbf{y}^T \beta_0 \mathbf{1} + \nabla \boldsymbol{\beta}^T \mathbf{X}^T \beta_0 \mathbf{1} \right) \\\\
&= - \frac{2}{n} \mathbf{1}^T \left(\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}\right) \\\\
0 &= - \frac{1}{n} \mathbf{1}^T \mathbf{y} + \frac{1}{n} \mathbf{1}^T \beta_0 \mathbf{1} + \frac{1}{n} \mathbf{1}^T \mathbf{X} \boldsymbol{\beta} \\\\
\widehat\beta_0 &= \frac{1}{n} \mathbf{1}^T \mathbf{y} - \frac{1}{n} \mathbf{1}^T \mathbf{X} \boldsymbol{\beta} \\\\
&= \bar{y} - \mathbf{\bar{x}}^T \boldsymbol{\beta}
\end{align}
$$

<br>

$$
\begin{align}
\nabla_{\boldsymbol{\beta}} \, \text{MSE} &= \frac{1}{n} \left(- \nabla \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{y} + \nabla \boldsymbol{\beta}^T \mathbf{X}^T \beta_0 \mathbf{1} + \\
\nabla \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} - \nabla \mathbf{y}^T \mathbf{X} \boldsymbol{\beta} + \nabla \mathbf{1}^T \beta_0 \mathbf{X} \boldsymbol{\beta} \right) \\\\
&= \frac{2}{n} \left(\beta_0 \mathbf{X}^T \mathbf{1} - \mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \boldsymbol{\beta}\right) \\\\
0 &= \beta_0 \frac{1}{n} \mathbf{X}^T \mathbf{1} - \frac{1}{n} \mathbf{X}^T \mathbf{y} + \frac{1}{n} \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} \\\\
&= \beta_0 \mathbf{\bar{x}} - \frac{1}{n} \mathbf{X}^T \mathbf{y} + \frac{1}{n} \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} \\\\
&= \mathbf{\bar{x}} (\bar{y} - \mathbf{\bar{x}}^T \boldsymbol{\beta}) - \frac{1}{n} \mathbf{X}^T \mathbf{y} + \frac{1}{n} \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} \\\\
&= \mathbf{\bar{x}} \bar{y} - \mathbf{\bar{x}} \mathbf{\bar{x}}^T \boldsymbol{\beta} - \frac{1}{n} \mathbf{X}^T \mathbf{y} + \frac{1}{n} \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} \\\\
\frac{1}{n} \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} - \mathbf{\bar{x}} \mathbf{\bar{x}}^T \boldsymbol{\beta} &= \frac{1}{n} \mathbf{X}^T \mathbf{y} - \mathbf{\bar{x}} \bar{y} \\\\
\left(\frac{1}{n} \mathbf{X}^T \mathbf{X} - \mathbf{\bar{x}} \mathbf{\bar{x}}^T \right) \boldsymbol{\beta} &= \frac{1}{n} \mathbf{X}^T \mathbf{y} - \mathbf{\bar{x}} \bar{y} \\\\
\boldsymbol{\widehat\beta} &= \frac{\frac{1}{n} \mathbf{X}^T \mathbf{y} - \mathbf{\bar{x}}^T \bar{y}}{\frac{1}{n} \mathbf{X}^T \mathbf{X} - \mathbf{\bar{x}} \mathbf{\bar{x}}^T}
\end{align}
$$

<br>

## Sesgo de estimación por omisión de variables relacionadas

<br>

$$
\begin{align}
\widehat\beta_1 &= \frac{\text{Cov}\left[X_1, Y\right]}{\text{Var}\left[X_1\right]} \\\\
&= \frac{\text{Cov}\left[X_1, \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p\right]}{\text{Var}\left[X_1\right]} \\\\
&= \frac{\beta_1 \text{Var}\left[X_1\right] + \sum_{i=2}^p \beta_i \text{Cov}\left[X_1, X_i \right]}{\text{Var}\left[X_1\right]} \\\\
&= \beta_1 + \sum_{i=2}^p \beta_i \frac{\text{Cov}\left[ X_1, X_i \right]}{\text{Var}\left[X_1\right]} \\\\
\end{align}
$$

<br>

## Colinealidad

<br>

Existen constantes $a_0, a_1, \ldots, a_p$ de manera que, para dos filas $i$,

$$
a_0 + \sum_{j=1}^p a_j x_{ij} = 0
$$

$\mathbf{X}^T \mathbf{X}$ no es invertible cuando $n < p + 1$, uno de los predictores es constante o alguno de los predictores es proporcional o está linealmente relacionado con otro.

Dos vectores son linealmente independientes si ninguna combinación lineal de ellos equivale a 0.

La multicolinealidad ocurre cuando

$$
\begin{align}
\sum_{i=1}^p a_i \mathbf{x}_i &= a_0 \\\\
\mathbf{a}^T \mathbf{X} &= a_0 \\\\
\text{Var}\left[\mathbf{a}^T \mathbf{X} \right] &= \mathbf{a}^T \, \text{Var}\left[\mathbf{X}\right] \mathbf{a} \\\\
&= 0
\end{align}
$$

<br>

## Factor de inflación de la varianza (VIF)

<br>

$$
\begin{align}
\text{VIF}_i &= \frac{\widehat\sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}_{i+1, i+1}}{\widehat\sigma^2 / n s_{x_{i}}^2} \\\\
&= n s_{x_{i}}^2  \ (\mathbf{X}^T \mathbf{X})^{-1}_{i+1, i+1}
\end{align}
$$

El factor de inflación de la varianza equivale a regresar $\mathbf{x}_i$ en el resto de predictores y calcular $\frac{1}{1 - R^2}$.

Si expresamos el estadístico de la pendiente en términos del VIF, entonces podemos observar cómo su varianza es un compromiso entre la estimación del error cuadrático medio y el mismo VIF:

<br>

$$
\frac{\widehat{\beta}_i}{\widehat\sigma \sqrt{\text{VIF}_i / n s_{x_i}^2}} \sim t_{n-p-1}
$$

<br>

Esto quiere decir que eliminar los predictores no significativos del modelo no es el método más óptimo. Un predictor no significativo puede reducir el intervalo de confianza del parámetro $\beta_i$ si contriuye más a la reducción de $\widehat\sigma^2$ que al VIF del otro predictor.

<br>

## Autovalores y autovectores

<br>

$$
\begin{align}
\text{Var}\left[\mathbf{X}\right] \mathbf{v}_i &= \lambda_i \mathbf{v}_i \\\\
\mathbf{V}^T \mathbf{V} &= \mathbf{I} \\\\
\mathbf{V}^{-1} &= \mathbf{V}^T \\\\
\text{Var}\left[\mathbf{X}\right] &= \mathbf{V} \, \mathbf{U} \, \mathbf{V}^T
\end{align}
$$

Los autovectores de $\text{Var}\left[\mathbf{X}\right]$ se conocen como los componentes principales de los predictores.

Cualquier vector $\mathbf{a}$ puede ser reescrito como una suma de autovectores:

$$
\mathbf{a} = \sum_{i=1}^p (\mathbf{a}^T \mathbf{v}_i) \mathbf{v}_i
$$

<br>

## Derivadas respecto a vectores

<br>

Si $\mathbf{a}$ y $\mathbf{x}$ son vectores $p \times 1$, entonces

$$
\begin{align}
\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{a}) &= \mathbf{a} \\\\
\nabla_{\mathbf{x}} (\mathbf{b} \mathbf{x}) &= \mathbf{b}^T \\\\
\end{align}
$$

Si $\mathbf{c}$ es una matriz de dimensiones $p \times p$, entonces

$$
\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{c} \mathbf{x}) = (\mathbf{c} + \mathbf{c}^T) \mathbf{x}
$$

Si, además, $\mathbf{c} = \mathbf{c}^T$, entonces

$$
\nabla{\mathbf{x}} (\mathbf{x}^T \mathbf{c} \mathbf{x}) = 2 \mathbf{c} \mathbf{x}
$$

<br>

## Esperanza y varianza de vectores y matrices

<br>

Si $\mathbf{x}$ es un vector aleatorio $n \times 1$, entonces

$$
\begin{align}
\text{Var}\left[\mathbf{x}\right] &= \mathbb{E}\left[\mathbf{x} \mathbf{x}^T\right] - \mathbb{E}\left[\mathbf{x}\right] \mathbb{E}\left[\mathbf{x}\right]^T \\\\
\text{Var}\left[b\mathbf{x}\right] &= b^2 \mathbf{x} \\\\
\text{Var}\left[\mathbf{cx}\right] &= \mathbf{c} \text{Var}\left[\mathbf{x} \right] \mathbf{c}^T \\\\
\mathbb{E}\left[\mathbf{x}^T \mathbf{cx} \right] &= \mathbb{E}\left[\mathbf{x}\right]^T \mathbf{c}\mathbb{E}\left[\mathbf{x}\right] + \text{tr} \, \mathbf{c} \text{Var}\left[\mathbf{x}\right] \\\\
\mathbf{x}^T \mathbf{cx} &= \text{tr} \, \mathbf{x}^T \mathbf{cx} \\\\
&= \text{tr} \, \mathbf{cxx}^T
\end{align}
$$

<br>

## El método Delta

<br>

$\theta$ es un parámetro que pretendemos estimar a través de un estimador insesgado $\hat\theta$. Supongamos que $\theta$ es una función de un vector de parámetros $\boldsymbol{\psi}$,

<br>

$$
\theta = f\left(\psi_1, \dots, \psi_n \right)
$$

Entonces,

$$
\widehat\theta = f\left(\widehat\psi_1, \dots, \widehat\psi_p \right)
$$

<br>

Usando la expansión de Taylor,

<br>

$$
\begin{align}
\theta &\approx \widehat\theta + \sum_{i=1}^p \left(\psi_i - \widehat\psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\hat\theta &\approx \theta + \sum_{i=1}^p \left(\widehat\psi_i - \psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\text{Var}\left[\hat\theta\right] &\approx \sum_{i=1}^{p} {\left. \frac{\partial^2 f}{\partial \psi_i^2} \right|}_{\psi=\widehat\psi} \, \text{Var}\left[{\widehat{\psi_i}}\right] + \\
& 2 \sum_{i=1}^{p-1} \sum_{j=i+1}^{p} {\left. \frac{\partial f}{\partial \psi_i}\right|}_{\psi=\widehat\psi} \, {\left. \frac{\partial f}{\partial \psi_j}\right|}_{\psi=\widehat\psi} \, \text{Cov}\left[{\widehat{\psi_i},\widehat{\psi_j}}\right]
\end{align}
$$

<br>

## Regresión de componentes principales

<br>

Los componentes principales son

$$
\mathbf{W} = \mathbf{X} \mathbf{V}
$$

Cada componente se define como la proyección de $\mathbf{X}$ en los autovectores.

El modelo de regresión con $k$ componentes es

$$
Y = \gamma_0 + \gamma_1 W_1 + \ldots + \gamma_k W_k + \eta
$$

$\eta = \epsilon$ si $k = p$.

<br>

## Regresión *Ridge*

<br>

Se trata de penalizar la longitud del vector de coeficientes $(\| \boldsymbol{\beta} \|)$. En *OLS* optimizamos la ecuación

$$
\frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
$$

Sin embargo, ahora optimizamos

$$
\frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \| \boldsymbol{\beta} \|^2 = \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \boldsymbol{\beta}^T \boldsymbol{\beta}
$$

donde $\lambda > 0$ regula el *trade-off* entre el MSE y $\| \boldsymbol{\beta} \|$.

Esta regresión se emplea tras centrar predictores y variable dependiente, para que $\beta_0 = 0$ y así no se penalice el tamaño de la intersección.

Si los predictores tienen diferente escala, entonces conviene estandarizarlos para que la penalización por $\| \boldsymbol{\beta} \|^2$ tenga sentido.

$$
\begin{align}
\nabla_\boldsymbol{\beta} \, \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \boldsymbol{\beta}^T \boldsymbol{\beta} &= \frac{2}{n} \left(-\mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} + \lambda \boldsymbol{\beta} \right) \\\\
\frac{2}{n} \left(-\mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \boldsymbol{\widehat\beta} + \lambda \boldsymbol{\widehat\beta} \right) &= 0 \\\\
\mathbf{X}^T \mathbf{y} &= \left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right) \boldsymbol{\widehat\beta} \\\\
\boldsymbol{\widehat\beta} &= \left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^T \mathbf{y}
\end{align}
$$

La diferencia entre el estimador *OLS* y el anterior es que se le añade $\lambda$ a la diagonal de $\mathbf{X}^T \mathbf{X}$ (este es el *ridge*). Como consecuencia, se minimizan posibles problemas por colinealidad y la varianza de los coeficientes en $\boldsymbol{\widehat\beta}$ es menor.

Esta menor varianza implica sesgo (*bias-variance trade-off*)

$$
\begin{align}
\mathbb{E}\left[ \boldsymbol{\widehat\beta} \right] &= \left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^T \mathbb{E}\left[ \mathbf{y} \right] \\\\
&= \left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} \\\\
\text{Var}\left[ \boldsymbol{\widehat\beta} \right] &= \text{Var}\left[\left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^T \mathbf{y} \right] \\\\
&= \text{Var}\left[\left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^T \mathbf{\epsilon} \right] \\\\
&= \left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^T \, \sigma^2 \mathbf{I} \, \mathbf{X} \left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \\\\
&= \sigma^2 \left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^T \mathbf{X} \left( \mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \right)^{-1}
\end{align}
$$

Si $\lambda$ no es fijado sino estimado, entonces tendríamos que preocuparnos de su distribución.

Una de las principales ventajas de esta regresión es que permite comprobar con gran precisión la contribución de los predictores sin necesidad de preocuparse de qué variables prescindir.

<br>

## Regresión *Lasso*

<br>

Esta regresión sustituye la penalización de $\| \boldsymbol{\beta} \|^2$ por otra medida de la longitud del vector:

$$
\| \boldsymbol{\beta} \|_q = \left( \sum_{i=1}^p b_i^q \right)^{1/q}
$$

Si $q = 2$, entonces la penalización es la misma que en la regresión *ridge*. La regresión *lasso* (*least angle selection and shrinkage operator*) utiliza la penalización $q = 1$.

<br>

## Corrección de Bonferroni

<br>

Supongamos que estamos interesados en que el error tipo I de $k$ coeficientes sea, conjuntamente, $1 - \alpha$. Una forma común de lograrlo es establecer el error tipo I de cada coeficiente en $\alpha / k$ (o, equivalentemente, multiplicar el valor p por $k$). Sin embargo, esto solo es exacto si los coeficientes son independientes.

En un contraste estándar, cada coeficiente $\beta_i$ posee un conjunto de confianza $C_i(\alpha)$ de manera que

<br>

$$
\begin{align}
P(\beta_i \in C_i(\alpha)) &= 1 - \alpha \\\\
P(\beta_i \notin C_i(\alpha)) &= \alpha \\\\
P\left( \bigcup_i^k \beta_i \notin C_i(\alpha) \right) &= \sum_{i}^k P(\beta_i \notin C_i(\alpha)) - P(\text{Conjunto de intersecciones en } \beta) \\\\
&= k \alpha - P(\text{Conjunto de intersecciones en } \beta) \\\\
&\leq k \alpha \\\\
\end{align}
$$

$$
\begin{align}
1 - P\left( \bigcup_i^k \beta_i \notin C_i(\alpha) \right) &\leq 1 - k \alpha \\\\
1 - P\left( \bigcup_i^k \beta_i \notin C_i(\alpha / k) \right) &\leq 1 - \alpha
\end{align}
$$

<br>

Por tanto, tras dividir por el número de contrastes $k$, la probabilidad de que, al menos, un solo parámetro no se encuentre en su intervalo de confianza es igual o inferior a $\alpha$. Por esta razón, la correción de Bonferroni es conservadora.

Esta corrección tan solo exige que

$$
C(\alpha) = \prod_{i}^k C(\alpha_i / k)
$$

o equivalentemente,

$$
\alpha = \sum_{i}^k \alpha_i/k
$$

Por tanto, es legítimo asumir distintos niveles de error tipo I $(\alpha/k)$ en cada contraste siempre que la suma de ellos equivalga a $\alpha$.

<br>

## Elipsoides de confianza

<br>

Si nuestros coeficientes siguen una diitribución normal y son independientes unos de otros,

$$
\begin{align}
\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]} \sim \mathcal{N}(0, 1) \\\\
\sum_{i=1}^k \left(\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]}\right)^2 \sim \chi^2_k \\\\
\end{align}
$$

El intervalo $1 - \alpha$ de confianza vendría dado por

<br>

$$
P\left(\sum_{i=1}^k \left(\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]}\right)^2 \leq \;\;  \chi^2_{k, 1-\alpha} \right) = 1 - \alpha
$$

Esta región de confianza es un elipsoide.

Sin embargo, si los coeficientes covarían y la matriz de varianzas-covarianzas de los coeficientes es $\boldsymbol{\Sigma}$, entonces

<br>

$$
\begin{align}
\boldsymbol{\Sigma} &= \mathbf{VUV}^T \\\\
\boldsymbol{\Sigma}^{1/2} &= \mathbf{VU}^{1/2} \mathbf{V}^T \\\\
\text{Var}\left[ \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \right] &= 
\boldsymbol{\Sigma}^{-1/2} \, \text{Var}\left[ \boldsymbol{\widehat\beta - \boldsymbol{\beta}} \right] \, \left(\boldsymbol{\Sigma}^{-1/2}\right)^T \\\\
&= \left(\mathbf{VU}^{1/2} \mathbf{V}^T\right)^{-1} \mathbf{V} \mathbf{U} \mathbf{V}^T \left(\left(\mathbf{VU}^{1/2} \mathbf{V}^T\right)^{-1}\right)^T \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^T \mathbf{V} \mathbf{U} \mathbf{V}^T \left(\mathbf{VU}^{-1/2} \mathbf{V}^T\right)^T \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^T \mathbf{V} \mathbf{U} \mathbf{V}^T \left(\mathbf{VU}^{-1/2} \mathbf{V}^T\right)^T \\\\
&= \mathbf{VU}^{-1/2} \mathbf{V}^T \mathbf{V} \mathbf{U} \mathbf{V}^T \mathbf{VU}^{-1/2} \mathbf{V}^T \\\\
&= \mathbf{VU}^{-1/2} \mathbf{UU}^{-1/2} \mathbf{V}^T \\\\
&= \mathbf{VV}^T \\\\
&= \mathbf{I}
\end{align}
$$

$$
\begin{align}
\boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \mathcal{MVN}(\mathbf{0}, \mathbf{I}) \\\\
\left(\boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})\right)^T \; \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k \\\\
(\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^T \; \left(\boldsymbol{\Sigma}^{-1/2} \right)^T \; \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k \\\\
(\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^T \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &\sim \chi^2_k
\end{align}
$$

<br>

$$
P\left( (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^T \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \, \leq \,  \chi^2_{k, 1-\alpha} \right) = 1 - \alpha
$$

Si se utiliza $\widehat\sigma^2$ para estimar el error típico, entonces debemos emplear la distribución $F$:

$$
P\left( (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^T \ \boldsymbol{\widehat\Sigma}^{-1} \ (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \, \leq \,  F_{q, \ n-p-1}^{1-\alpha} \right) = 1 - \alpha
$$

<br>

## Interacciones

<br>

Si la verdadera función de regresión es $\mathbb{E}\left[Y \, | \, X_1, X_2\right] = \mu(X_1, X_2)$, que incluye una interacción de forma desconocida entre las variables $X_1$ y $X_2$, ¿por qué representar la interacción entre $X_1$ y $X_2$ como $X_1X_2$?

<br>

$$
Y = \beta_0 + X_1 \beta_1 + X_2 \beta_2 + X_1X_2\beta_3 + \epsilon
$$

<br>

Si hiciéramos una expansión de Taylor,

<br>

$$
      \begin{align}
\mu(x_1, x_2) \approx \mu(x_1^*, x_2^*) &+ \sum_{n=1}^\infty \frac{1}{n!} (x_1 - x_1^*)^n \, \left. \frac{\partial^n \mu}{\partial x_1^n} \right|_{x_1 = x_1^*} + \\
&+ \sum_{n=1}^\infty \frac{1}{n!} (x_2 - x_2^*)^n \, \left. \frac{\partial^n \mu}{\partial x_{2}^n} \right|_{x_2 = x_2^*} + \\
&+ (x_1 - x_1^*) (x_2 - x_2^*) \left. \frac{\partial^2 \mu}{\partial x_1 x_2} \right|_{x_1x_2 = x_1^*x_2^*} + \\
&+ \frac{1}{2} (x_1 - x_1^*)^2 (x_2 - x_2^*) \left. \frac{\partial^2 \mu}{\partial x_1^2 x_2} \right|_{x_1x_2 = x_1^*x_2^*} + \\
&+ \frac{1}{2} (x_1 - x_1^*) (x_2 - x_2^*)^2 \left. \frac{\partial^2 \mu}{\partial x_1 x_2^2} \right|_{x_1x_2 = x_1^*x_2^*}
\end{align}
$$

<br>

entonces podemos comprobar que una buena aproximación incluye el producto entre las variables $X_1$ y $X_2$ así como sus términos cuadráticos.

<br>

## *Leverage*

<br>

$$
\widehat{m}(x^*) = \bar{y} + \frac{1}{n} \sum_{i=1}^n \frac{(x_i - \bar{x}) (y_i - \bar{y})}{s_x^2} (x^* - \bar{x})
$$

<br>

Cuanto más se distancia $x_i$ respecto a $\bar{x}$, mayor es su contribución a $\widehat{\beta}_1$ y, por tanto, a $\widehat{m}(x^*)$. Esto quiere decir que el modelo tiende a conceder más peso a las realizaciones $y$ cuyo predictor $x$ se aleja más de su media. Si $x_i = \bar{x}$, entonces $y_i$ solo contribuye a $\bar{y}$ (la intersección) y no a la pendiente. 
Visto de otra manera,

<br>

$$
\sum_{i=1}^n e_i(x_i - \bar{x}) = 0
$$

<br>

Cuando $x_i - \bar{x}$ es grande, el residuo y su minimización adquieren más importancia.

La tasa de cambio del valor ajustado $\widehat{m}_i$ respecto a $y_i$ viene dado por

<br>

$$
\frac{\partial \widehat{m}_i}{\partial y_i} = \mathbf{H}_{ii}
$$

<br>

Es decir, $\mathbf{H}_{ii}$ es la influencia de $y_i$ en su propio valor ajustado y nos dice cúanto de $\widehat{m}_i$ es verdaderamente $y_i$ (las filas de $\mathbf{H}$ suman 1). Por tanto, a $\mathbf{H}_{ii}$ se le conoce como el **leverage** de $x_i$.

<br>

$$
\mathbf{H} \mathbf{1} = \mathbf{1}
$$

<br>

El leverage promedio es $\frac{p + 1}{n}$ ya que $\text{tr}\left(\mathbf{H}\right) = p + 1$ y $\mathbf{H}$ es una matrix $n \times n$.

La función de regresión es propensa a ajustar los puntos de mayor *leverage*. Cuanto más cercano sea al valor 1, más se ajusta $\widehat{m}_i$ a $y_i$.

Adicionalmente, $x_i - \bar{x}$ tendrá un mayor *leverage* si la varianza de dicho predictor es menor, como se puede ver en

<br>

$$
\widehat{m}(\mathbf{x}_i) = \bar{y} + \frac{1}{n} (\mathbf{x}_i - \mathbf{\bar{x}}) \text{Var}\left[\mathbf{X_c}\right]^{-1} \mathbf{X_c}^T \mathbf{y_c}
$$

<br>

donde $\mathbf{x}_i$ es el vector de predictores en la fila $i$, $\mathbf{X_c}$ representa la matriz de diseño centrada sin la columna para la intersección y $\mathbf{y_c}$ es el vector $\mathbf{y}$ centrado.

Nota: los valores ajustados del modelo pueden interpretarse como una distancia de Mahalanobis,

<br>

$$
\widehat{m}\left(\mathbf{X}\right) = \bar{y} + \mathbf{X_c} \text{Var}\left[\mathbf{X_c}\right]^{-1} \mathbf{X_c}^T \mathbf{y_c}
$$

<br>

## Residuos estandarizados

<br>

El problema de que la función de regresión tienda a ajustar mejor aquellas realizaciones cuyos predictores tengan un considerable *leverage* es que la varianza de los residuos sea menor,

<br>

$$
\begin{align}
\text{Var}\left[\mathbf{e}\right] &= \sigma^2 \left(\mathbf{I} - \mathbf{H} \right) \\\\
\text{Var}\left[e_i\right] &= \sigma^2 \left(1 - \mathbf{H}_{ii} \right)
\end{align}
$$

<br>

Por esta razón, se suelen considerar los residuos estandarizados

<br>

$$
r_i = \frac{e_i}{\widehat\sigma \sqrt{1 - \mathbf{H}_{ii}}}
$$

<br>

Aunque $r_i$ resulta de la división entre una variable normalmente distribuida y otra variable con distribución $\sqrt{\chi^2}$, $e_i$ y $\widehat\sigma$ no son estadísticamente independientes: la segunda se estima empleando la primera. En realidad,

<br>

$$
\frac{r_i^2}{n-p-1} \sim \beta\left(\frac{1}{2}, \frac{n-p-2}{2} \right)
$$

<br>

Definiendo $v = \frac{(n - p - 2)}{2}$, la probabilidad de densidad de un residuo estandarizado y su función de distribución son

<br>

$$
\begin{align}
f(r) &= \frac{\Gamma \left( v + \frac{1}{2} \right)}{ \Gamma(v) \sqrt{\pi (n-p-1)}} \left( 1 - \frac{r^2}{n-p-1} \right)^{v-1} \ ; \qquad |r| \leq (n-p-1)^{1/2} \\\\
F(r) &= \frac{1}{2} + r \ \frac{\Gamma \left( \frac{1}{2}+ v \right) \ _2F_1 \left( \frac{1}{2}, 1 - v, \frac{3}{2}, \frac{r^2}{2 v + 1} \right)}{\Gamma(v) \ \sqrt{\pi + 2 \pi v}}
\end{align}
$$

<br>

donde $_2F_1$ se corresponde con la función hipergeométrica.

<br>

## Residuos de validación cruzada

<br>

Si eliminamos del modelo de regresión la realización $y_i$, entonces el hipotético valor ajustado sería

<br>

$$
\widehat{m}^{(-i)} = \frac{(\mathbf{Hy})_i - \mathbf{H}_{ii} y_i}{1 - \mathbf{H}_{ii}}
$$

<br>

El residuo de validación cruzada es

<br>

$$
e_i^{(-i)} = y_i - \widehat{m}^{(-i)}
$$

<br>

Y su estadístico es

<br>

$$
\begin{align}
t_i &= \frac{e_i}{\widehat\sigma_{(-i)} \sqrt{1 - \mathbf{H}_{ii}}} \\\\
&= r_i \sqrt{\frac{n - p -2}{n - p -1 - r_i^2}} \\\\
&\sim t_{n-p-2}
\end{align}
$$

<br>

## Distancia de Cook

<br>

$$
\begin{align}
D_i &= \frac{\left\| \widehat{\mathbf{m}} - \widehat{\mathbf{m}}^{(-i)} \right\|^2}{\widehat\sigma^2(p+1)} \\\\
&= \frac{e_i^2 \ \mathbf{H}_{ii}}{\widehat\sigma^2 (p+1) \ (1 - \mathbf{H}_{ii})^2} \\\\
&\sim F_{p+1, \ n-p-1}
\end{align}
$$

<br>

Es decir, la influencia total de un predictor en los valores ajustados depende de su *leverage* y correspondiente residuo.

También podemos obtener una expresión de la estimación de $\beta$ ante la ausencia de la realización $i$:

<br>

$$
\widehat{\beta}^{(-i)} = \widehat{\beta} - \frac{(\mathbf{X}^T \mathbf{X})^{-1} \ \mathbf{x}_i^T \ e_i}{1 - \mathbf{H}_{ii}} \\\\
$$

<br>

Lo que nos permite expresar $D_i$ como

<br>

$$
D_i = \left(\widehat{\beta}^{(-i)} - \widehat{\beta}\right)^T \ \frac{\mathbf{X}^T \mathbf{X}}{(p + 1) \widehat{\sigma}^2}  \left(\widehat{\beta}^{(-i)} - \widehat{\beta}\right)
$$

<br>

## Selección de modelos

<br>

Si repetimos el experimento empleando los mismos valores de los predictores ($\mathbf{X}$), las nuevas realizaciones de la variable $Y$ son:

$$
Y^* = \mathbf{X} \boldsymbol{\beta} + \epsilon^*
$$

<br>

La esperanza del error cuadrático medio fuera de la muestra sería

<br>

$$
\mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \left(Y_i^* - \widehat{m}_i\right)^2 \right]
$$

<br>

$$
\begin{align}
\mathbb{E}\left[ \left(Y_i - \widehat{m}_i \right)^2 \right] &= \text{Var}\left[ Y_i - \widehat{m}_i \right] + \left( \mathbb{E} \left[Y_i - \widehat{m}_i \right] \right)^2 \\\\
&= \text{Var}\left[ Y_i \right] + \text{Var}\left[ \widehat{m}_i \right] - 2 \ \text{Cov}\left[ Y_i, \widehat{m}_i \right] + \left( \mathbb{E} \left[Y_i - \widehat{m}_i \right] \right)^2 \\\\
\mathbb{E}\left[ \left(Y_i^* - \widehat{m}_i \right)^2 \right] &= \text{Var}\left[ Y_i^* - \widehat{m}_i \right] + \left( \mathbb{E} \left[Y_i^* - \widehat{m}_i \right] \right)^2 \\\\
&= \text{Var}\left[ Y_i^* \right] + \text{Var}\left[ \widehat{m}_i \right] + \left( \mathbb{E} \left[Y_i^* - \widehat{m}_i \right] \right)^2
\end{align}
$$

<br>

$\text{Cov}\left[ Y_i^*, \widehat{m}_i \right]$ es necesariamente $0$ porque $\widehat{m}_i$ es independiente de $Y^*$.

<br>

$$
\begin{align}
\mathbb{E}\left[ Y_i \right] &= \mathbb{E}\left[ Y_i^* \right] \\\\
\text{Var}\left[ Y_i \right] &= \text{Var}\left[ Y_i^* \right] \\\\
\mathbb{E}\left[ \left(Y_i^* - \widehat{m}_i \right)^2 \right] &= \mathbb{E}\left[ \left(Y_i - \widehat{m}_i \right)^2 \right] + 2 \ \text{Cov}\left[ Y_i, \widehat{m}_i \right]
\end{align}
$$

<br>

Ahora bien,

<br>

$$
\begin{align}
\text{Cov}\left[ Y_i, \widehat{m}_i \right] &= \sigma^2 \mathbf{H}_{ii} \\\\
\mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \left(Y_i^* - \widehat{m}_i \right)^2 \right] &= \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \left(Y_i - \widehat{m}_i \right)^2 \right] + \frac{2}{n} \sigma^2 (p+1)
\end{align}
$$

<br>

$\frac{2}{n} \sigma^2 (p+1)$ se conoce como el **optimismo** del modelo porque es la cantidad en la que el error cuadrático medio de la muestra infraestima el error cuadrático medio de futuras realizaciones de $Y^*$.

Si tuviéramos una expresión adecuada del optimismo, tendríamos un estimador insesgado del error cuadrático medio de futuras realizaciones.

<br>

### Estadístico $C_p$ de Mallow

<br>

$$
C_p = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \widehat{m}_i \right)^2 + \frac{2}{n} \widehat\sigma^2 (p+1)
$$

<br>

$\widehat\sigma^2$ se corresponde con la estimación del error cuadrático medio del modelo más general entre los que se comparan. $\widehat\sigma^2$ es un estimador insesgado de $\sigma^2$ si dicho modelo subsume el verdadero. La idea es elegir el modelo con menor $C_p$.

<br>

### $R^2$

<br>

Maximizar $R^2$ es equivalente a minimizar el error cuadrático medio ya que 

<br>

$$
R^2 = 1 - \frac{\text{MSE}}{s_y^2}
$$

<br>

Sin embargo,

<br>

$$
\begin{align}
R^2_{\text{adj}} &= 1 - \frac{\widehat\sigma^2}{\hat{s}_y^2} \\\\
&= 1 - \frac{\text{MSE}}{\hat{s}_y^2} \ \frac{n}{n-p-1}
\end{align}
$$

<br>

En este caso, nuestra intención sería minimizar

<br>

$$
\text{MSE} \ \frac{n}{n-p-1} = \text{MSE} \ \frac{1}{1 - (p+1)/n}
$$

<br>

Empleando el teorema binomial para conseguir una aproximación de primer orden,

<br>

$$
\begin{align}
\text{MSE} \ \frac{1}{1 - (p+1)/n} &\approx \text{MSE} \ \left( 1 + \frac{p+1}{n} \right) \\\\
&\approx \text{MSE} + \text{MSE} \left(\frac{p+1}{n} \right)
\end{align}
$$

<br>

Es decir, $R^2_{\text{adj}}$ utiliza la misma penalización que $C_p$ pero $2$ veces menor, como máximo.

<br>

### AIC

<br>

Digamos que $\ell(\widehat\theta)$ es la esperanza del logaritmo de la verosimilitud de una nueva realización bajo el vector estimado $\widehat\theta$. Si $\theta^*$ es el verdadero vector de parámetros, una expansión de Taylor indica que

$$
\ell(\widehat\theta) \approx \ell(\theta^*) + (\widehat\theta - \theta^*)^T \nabla \ell(\theta^*) + \frac{1}{2} (\widehat\theta - \theta^*)^T \ \nabla \nabla \ell(\theta^*) \ (\widehat\theta - \theta^*)
$$

<br>

La esperanza del logaritmo de la verosimilitud es maximizada por el vector de parámetros verdaderos, por lo que

<br>

$$
\begin{align}
\nabla \ell(\theta^*) &= 0 \\\\
\ell(\widehat\theta) &\approx \ell(\theta^*) + \frac{1}{2} (\widehat\theta - \theta^*)^T \ \nabla \nabla \ell(\theta^*) \ (\widehat\theta - \theta^*)
\end{align}
$$

<br>

Si definimos la verdadera matriz hessiana como $\mathbf{h}$, entonces

$$
\begin{align}
\nabla \nabla \ell(\theta^*) &= \mathbf{h} \\\\
\ell(\widehat\theta) &\approx \ell(\theta^*) + \frac{1}{2} (\widehat\theta - \theta^*)^T \ \mathbf{h} \ (\widehat\theta - \theta^*)
\end{align}
$$

<br>

Ahora pensemos en la expansión de Taylor de la derivada del logaritmo de la verosimilitud evaluada en el vector estimado de parámetros, que por definición equivale a 0.

<br>

$$
\begin{align}
\nabla L(\widehat\theta) \approx \nabla L(\theta^*) + (\widehat\theta - \theta^*)^T \ \nabla \nabla L(\theta^*) \\\\
\mathbf{0} \approx \nabla L(\theta^*) + (\widehat\theta - \theta^*)^T \ \nabla \nabla L(\theta^*)
\end{align}
$$

<br>

$\nabla \nabla L(\theta^*)$ es la matriz hessiana evaluada en el vector de parámetros, así que, si la definimos con $\mathbf{H}$,

<br>

$$
\begin{align}
\mathbf{0} &\approx \nabla L(\theta^*) + (\widehat\theta - \theta^*)^T \ \mathbf{H} \\\\
\widehat\theta &\approx \theta^* - \nabla L(\theta^*) \ \mathbf{H}^{-1}
\end{align}
$$

<br>

Si sustituimos esta última expresión de $\widehat\theta$ por $\widehat\theta$ en

<br>

$$
\ell(\widehat\theta) \approx \ell(\theta^*) + \frac{1}{2} (\widehat\theta - \theta^*)^T \ \mathbf{h} \ (\widehat\theta - \theta^*)
$$

<br>

entonces,

<br>

$$
\begin{align}
\ell(\widehat\theta) &\approx \ell(\theta^*) + \frac{1}{2} \left( \theta^* - \nabla L(\theta^*) \ \mathbf{H}^{-1} - \theta^* \right)^T \ \mathbf{h} \ \left( \theta^* - \nabla L(\theta^*) \ \mathbf{H}^{-1} - \theta^* \right) \\\\
&\approx \ell(\theta^*) + \frac{1}{2} \left( \nabla L(\theta^*) \ \mathbf{H}^{-1} \right)^T \ \mathbf{h} \ \left( \nabla L(\theta^*) \ \mathbf{H}^{-1} \right) \\\\
&\approx \ell(\theta^*) + \frac{1}{2} \nabla L(\theta^*) \ \mathbf{H}^{-1} \ \mathbf{h} \ \mathbf{H}^{-1} \ \nabla L(\theta^*)
\end{align}
$$

<br>

Si a continuación tomamos esperanzas, sabiendo que $\text{Var} \left[ \nabla L(\theta^*) \right] = - \mathbf{h}$ según la **identidad de Fisher**, entonces

<br>

$$
\begin{align}
\mathbb{E} \left[ \ell(\widehat\theta) \right] &\approx \ell(\theta^*) - \frac{1}{2} \ \mathbf{h}^{-1} \ \mathbf{h} \\\\
&\approx \ell(\theta^*) - \frac{1}{2} \ \text{tr} \ \mathbf{I}
\end{align}
$$

<br>

Volvamos a hacer una expansión de Taylor, esta vez del logaritmo de la verosimilitud del verdadero vector de parámetros.

<br>

$$
\begin{align}
L(\theta^*) &\approx L(\widehat\theta) + (\theta^* - \widehat\theta)^T \ \nabla L(\widehat\theta) + \frac{1}{2} (\theta^* - \widehat\theta)^T \ \nabla \nabla L(\widehat\theta) \ (\theta^* - \widehat\theta) \\\\
&\approx L(\widehat\theta) + \frac{1}{2} (\theta^* - \widehat\theta)^T \ \nabla \nabla L(\widehat\theta) \ (\theta^* - \widehat\theta) \\\\
&\approx L(\widehat\theta) + \frac{1}{2} \left( \theta^* + \nabla L(\theta^*) \ \mathbf{H}^{-1} - \theta^* \right)^T \ \nabla \nabla L(\widehat\theta) \ \left( \theta^* + \nabla L(\theta^*) \ \mathbf{H}^{-1} - \theta^* \right) \\\\
&\approx L(\widehat\theta) + \frac{1}{2} \left( \nabla L(\theta^*) \ \mathbf{H}^{-1} \right)^T \ \nabla \nabla L(\widehat\theta) \ \left( \nabla L(\theta^*) \ \mathbf{H}^{-1} \right) \\\\
&\approx L(\widehat\theta) + \frac{1}{2} \nabla L(\theta^*) \ \mathbf{H}^{-1} \ \nabla \nabla L(\widehat\theta) \ \mathbf{H}^{-1} \ \nabla L(\theta^*)
\end{align}
$$

<br>

Según la identidad de Fisher, $\text{Var} \left[ \nabla L(\theta^*) \right] = - \mathbf{h}$, y volviendo a tomar esperanzas,

<br>

$$
\begin{align}
\mathbb{E} \left[ \nabla \nabla L(\widehat\theta) \right] &= \mathbb{E} \left[\nabla \nabla L(\theta^*) \right] = \nabla \nabla \ell(\theta^*) = \mathbf{h} \\\\
\ell(\theta^*) &\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \frac{1}{2} \ \mathbf{h}^{-1} \ \mathbf{h} \\\\
&\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \frac{1}{2} \ \text{tr} \ \mathbf{I}
\end{align}
$$

<br>

Por último, combinando las expresiones

<br>

$$
\begin{align}
\ell(\theta^*) &\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \frac{1}{2} \ \text{tr} \ \mathbf{I} \\\\
\mathbb{E} \left[ \ell(\widehat\theta) \right] &\approx \ell(\theta^*) - \frac{1}{2} \ \text{tr} \ \mathbf{I}
\end{align}
$$

<br>

obtenemos

<br>

$$\begin{align}
\mathbb{E} \left[ \ell(\widehat\theta) \right] &\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \frac{1}{2} \ \text{tr} \ \mathbf{I} - \frac{1}{2} \ \text{tr} \ \mathbf{I} \\\\
\mathbb{E} \left[ \ell(\widehat\theta) \right] &\approx \mathbb{E} \left[ L(\widehat\theta) \right] - \text{tr} \ \mathbf{I}
\end{align}
$$

<br>

Por tanto, si nuestro modelo es correcto, una estimación insesgada de $\ell(\widehat\theta)$ es 

<br>

$$
\text{AIC} = L(\widehat\theta) - \text{número de parámetros estimados}
$$

<br>

En este caso, $AIC/n$ se corresponde con la esperanza del logaritmo de la verosimilitud de una nueva realización bajo el modelo estimado si dicho modelo es correcto.

Sin embargo, si no hemos estimado el modelo correcto, entonces $\text{Var}\left[ \nabla L(\theta^*) \right] \neq - \nabla\nabla \ \ell(\theta^*) \neq - \mathbf{h}$.

<br>

### Equivalencias

<br>

Supongamos que $L(\widehat\Theta)$ es el logaritmo de la verosimilitud del modelo más general y el verdadero mientras que $L(\widehat\theta)$ es el de un modelo más simple e incorrecto. Bajo el modelo lineal normal,

<br>

$$
\begin{align}
L(\widehat\Theta) &= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log \text{MSE}_{\ \widehat\Theta}} \\\\
L(\widehat\theta) &= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log \text{MSE}_{\ \widehat\theta}} \\\\
\Delta \ \text{AIC} &= L(\widehat\Theta) - L(\widehat\theta) - (p_{\Theta} - p_{\theta}) \\\\
&= \frac{n}{2}{\log \text{MSE}_{\ \widehat\theta}} - \frac{n}{2}{\log \text{MSE}_{\ \widehat\Theta}} - (p_{\Theta} - p_{\theta}) \\\\
&= \frac{n}{2} \log \left( \frac{\text{MSE}_{\ \widehat\theta}}{\text{MSE}_{\ \widehat\Theta}} \right) - (p_{\Theta} - p_{\theta}) \\\\
&= \frac{n}{2} \log \left( 1 + \frac{\text{MSE}_{\ \widehat\theta} - \text{MSE}_{\ \widehat\Theta}}{\text{MSE}_{\ \widehat\Theta}} \right) - (p_{\Theta} - p_{\theta}) \\\\
&\approx \frac{n}{2} \ \frac{\text{MSE}_{\ \widehat\theta} - \text{MSE}_{\ \widehat\Theta}}{\text{MSE}_{\ \widehat\Theta}} - (p_{\Theta} - p_{\theta})
\end{align}
$$

<br>

Dado que $\Theta$ es el modelo verdadero,

<br>

$$
\begin{align}
\Delta \ \text{AIC} &\approx \frac{n}{2} \ \frac{\text{MSE}_{\ \widehat\theta} - \text{MSE}_{\ \widehat\Theta}}{\widehat\sigma^2} - (p_{\Theta} - p_{\theta}) \\\\
\frac{2 \widehat\sigma^2}{n} \Delta \ \text{AIC} &\approx \text{MSE}_{\ \widehat\theta} - \text{MSE}_{\ \widehat\Theta} + \frac{2 \widehat\sigma^2}{n} (p_{\Theta} - p_{\theta}) \\\\
&\approx \Delta \ C_p
\end{align}
$$

<br>

Por tanto, si uno de los modelos es el verdadero, entonces escoger el modelo que maximiza el $AIC$ o el que minimiza el $C_p$ convergerá, aproximadamente, en el mismo resultado.

La diferencia es que $AIC$ puede aplicarse en cualquier modelo mientras que el $C_p$ solo tiene sentido cuando se emplea el error cuadrático medio.

<br>

### Validación cruzada *Leave-one-out* (LOOCV)

<br>

$$
\begin{align}
\text{LOOCV} &= \frac{1}{n} \sum_{i=1}^n \left( Y_i - \widehat{m}_i^{(-i)} \right)^2 \\\\
&= \frac{1}{n} \sum_{i=1}^n \left( \frac{Y_i - \widehat{m}_i}{1 - \mathbf{H}_{ii}} \right)^2 \\\\
&\approx \frac{1}{n} \sum_{i=1}^n \left( Y_i - \widehat{m}_i \right)^2 \ \left(1 + \mathbf{H}_{ii} \right)^2 \\\\
&\approx \frac{1}{n} \sum_{i=1}^n \left( Y_i - \widehat{m}_i \right)^2 \ \left(1 + 2 \ \mathbf{H}_{ii} \right) \\\\
&\approx \text{MSE} + 2 \ \text{MSE} \ \text{tr} \ \mathbf{H}
\end{align}
$$

<br>

A medida que $n \rightarrow \infty$, LOOCV, $C_p$ y AIC convergen y, si el modelo verdadero se encuentra entre los que se comparan, tenderán a elegir un modelo con más parámetros que el verdadero.

LOOCV, $C_p$ y AIC son estimadores insesgados del error de generalización pero no penalizan la varianza de las estimaciones, que incrementa con la adición de más parámetros al modelo.

Por otro lado, si el modelo verdadero se encuentra entre los que se comparan, la validación cruzada *k-fold* tiende a favorecer el modelo verdadero a medida que $n \rightarrow \infty$. Sin embargo, LOOCV tiende a dar mejores predicciones del error de generalización, sobre todo cuando el modelo verdadero no se encuentra entre los candidatos.

Es importante recordar que los estadísticos inferenciales son válidos si el modelo elegido es independiente a los datos obtenidos. Eliminar variables en función de la significación de un estadístico incorpora una fuente de variabilidad aleatoria no considerada.

<br>

### BIC

<br>

$$
\text{BIC} = L(\widehat\theta) - \frac{\log n}{2} \cdot \text{número de parámetros estimados}
$$

<br>

A diferencia de los otros métodos, a medida que $n \rightarrow \infty$ y asi el modelo verdadero se encuentra entre los comparados, BIC tenderá a seleccionar el modelo verdadero.

<br>

## Regresión lineal robusta

<br>

$$
\widehat{\beta} = \underset{\mathbf{b}}{\text{argmin}} \ \frac{1}{n} \sum_{i=1}^n p(y_i - \mathbf{x}_i \mathbf{b})
$$

<br>

$p$ es una *loss function*. Si $p(u) = |u|$, entonces incurrimos en la estimación por desviación absoluta y si $p(u) = u^2$, entonces la estimación es por mínimos cuadrados ordinarios.

El *loss function* de Huber viene definido por

$$
p(u) = \begin{cases} 
      u^2 & |u| \leq c \\
      2c|u| - c^2 & |u| > c
\end{cases}
$$

<br>

## Mínimos cuadrados ponderados

<br>

**Teorema de Gauss-Markov**: Si los términos error no correlacionan y poseen misma varianza, entonces el mejor estimador lineal insesgado es el de mínimos cuadrados.

Sin embargo, si nuestro modelo de regresión es

<br>

$$
\begin{align}
\mathbf{y} &=  \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
\mathbb{E} \left[ \boldsymbol{\epsilon} | \mathbf{X} \right] &= \mathbf{0} \\\\
\text{Var} \left[ \boldsymbol{\epsilon} | \mathbf{X} \right] &= \boldsymbol{\Sigma}
\end{align}
$$

<br>

donde $\boldsymbol{\Sigma}$ representa la matriz de varianzas-covarianzas entre los términos error, entonces es conveniente minimizar

<br>

$$
\begin{align}
\text{WMSE} &= \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T \ \mathbf{W} \ (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&= \frac{1}{n} \left( \mathbf{y}^T \mathbf{W} \mathbf{y} - \mathbf{y}^T \mathbf{W} \mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{W}  \mathbf{y} + \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{W} \mathbf{X} \boldsymbol{\beta} \right) \\\\
\end{align}
$$

<br>

donde $\mathbf{W}$ es una matriz de pesos.

<br>

$$
\begin{align}
\nabla_{\boldsymbol{\beta}} \text{WMSE} &= \frac{2}{n} \left( - \mathbf{X}^T \mathbf{W} \mathbf{y} + \mathbf{X}^T \mathbf{W} \mathbf{X} \boldsymbol{\beta} \right) \\\\
\widehat{\boldsymbol{\beta}} &= \left( \mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{W} \mathbf{y} \\\\
&= \left( \mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{W} \mathbf{X} \boldsymbol{\beta} + \left( \mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{W} \boldsymbol{\epsilon} \\\\
&= \boldsymbol{\beta} + \left( \mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{W} \boldsymbol{\epsilon} \\\\
\mathbb{E} \left[ \widehat{\boldsymbol{\beta}} \right] &= \boldsymbol{\beta} \\\\
\text{Var} \left[ \widehat{\boldsymbol{\beta}} \right] &= \left( \mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{W} \ \boldsymbol{\Sigma} \ \mathbf{W}^T \mathbf{X} \left( \mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1}
\end{align}
$$

<br>

La explicación es esta: supongamos que conocemos la matriz $\boldsymbol{\Sigma}$ y estandarizamos el modelo de regresión, de manera que

<br>

$$
\begin{align}
\mathbf{SS}^T &= \boldsymbol{\Sigma} \\\\
\mathbf{S}^{-1} \mathbf{y} &= \mathbf{S}^{-1} \mathbf{X} \boldsymbol{\beta} + \mathbf{S}^{-1} \boldsymbol{\Sigma} \\\\
\text{Var} \left[ \mathbf{S}^{-1} \boldsymbol{\epsilon} | \mathbf{X} \right] &= \mathbf{S}^{-1} \boldsymbol{\Sigma} \ \mathbf{S}^{-T} \\\\
&= \mathbf{I} \\\\
\widehat{\boldsymbol{\beta}} &= \left( \left( \mathbf{S}^{-1} \mathbf{X} \right)^T \mathbf{S}^{-1} \mathbf{X} \right)^{-1} \left( \mathbf{S}^{-1} \mathbf{X} \right)^T \mathbf{S}^{-1} \mathbf{y} \\\\
&= \left( \mathbf{X}^T \mathbf{S}^{-T} \mathbf{S}^{-1} \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{S}^{-T} \mathbf{S}^{-1} \mathbf{y} \\\\
&= \left( \mathbf{X}^T \boldsymbol{\Sigma}^{-1} \mathbf{X} \right)^{-1} \mathbf{X}^T \boldsymbol{\Sigma}^{-1} \mathbf{y}
\end{align}
$$

<br>

Ahora que el paralelismo es claro, vemos que la estimación de $\boldsymbol{\beta}$ es insesgada y tiene mínima varianza cuando $\mathbf{W} = \boldsymbol{\Sigma}^{-1}$. Por esta razón conviene que los pesos sean inversamente proporcionales a la varianza-covarianza de las realizaciones. En este caso, haríamos algo equivalente a estandarizar el modelo de regresión para cancelar las covarianzas entre los errores y forzar su homocedastidad.

Sin embargo, en situaciones reales no conocemos $\boldsymbol{\Sigma}$. Ante esta situación, existen dos formas genéricas de estimar la varianza condicional: 

**Método del cuadrado de los residuos**: aplicando OLS, se computa el cuadrado de los residuos. Si estos residuos cuadráticos dependen de los mismos predictores, entonces se promedian.

**Método del logaritmo del cuadrado de los residuos**: aplicando OLS, se computa el logaritmo del cuadrado de los residuos y luego se exponencian. Si estos residuos cuadráticos dependen de los mismos predictores, entonces se promedian.

Una vez estimadas las varianzas, se vuelve a estimar el modelo de regresión empleando la inversa de esta estimación como matriz de pesos $\mathbf{W}$ para luego volver a estimar la varianza condicional usando los residuos de este último modelo. La iteración se detiene cuando el cambio de coeficientes según el nuevo modelo y el anterior es pequeño.

También se puede estimar la varianza condicional usando algún modelo paramétrico con sentido teórico o mediante *splines* y validación cruzada empleando el logaritmo del cuadrado de los residuos (para que la predicción sea en la recta real).

Para estimar las covarianzas entre los términos error, necesitamos conocer la distancia entre los términos error (**lag**) en cuestión de tiempo o distancia, por ejemplo. A continuación, se toman todos los pares de observaciones con un mismo *lag* y se realiza la media de sus productos.

<br>

## Mínimos cuadrados generalizados

<br>

En este caso, la matriz de pesos $\mathbf{W}$ no es diagonal.

<br>

## Bootstrapping

<br>

Si definimos $\mathbf{k} = \left( \mathbf{X}^T \mathbf{X} \right)^{-1} \mathbf{X}^T$, entonces

<br>

$$
\begin{align}
\widehat{\beta}_i &= \beta_i + \sum_{j=1}^n k_{ij} \epsilon_j \\\\
&\sim \mathcal{N} \left( \beta_i, \ \sigma^2 \sum_{j=1}^n k_{ij} \right)
\end{align}
$$

<br>

Es decir, la distribución de $\widehat{\beta}_i$ depende de la suma de variables independientes pero no idénticamente distribuidas. Aún así, según el teorema central del Lindeberg, si las constantes $k_{ij}$ no son muy distintas de otras, la suma continúa siendo gausiana asintóticamente.

Sin embargo, no sabemos cómo de grande debe ser $n$ para que $\widehat{\beta}_i$ siga una distribución normal.

El *bootstrap* nos permite aproximar la distribución de cualquier estadístico usando los datos o el modelo estimado.

**Bootstrap paramétrico**: a los valores ajustados del modelo le añadimos ruido aleatorio procedente de su hipotética distribución y según los parámetros estimados. Es adecuado de utilizar si la relación es verdaderamente lineal y la distribución de los residuos es correcta.

**Remuestreo de residuos**: a cada valor ajustado del modelo le añadimos un residuo de forma aleatoria y con reposición. También asume que la relación verdadera es lineal pero no asume una distribución específica para los residuos. Sin embargo, sí deben seguir la misma distribución y ser homocedásticos e independientes tanto de otros términos error como de los predictores.

**Remuestreo de casos**: se seleccionan las realizaciones y sus predictores de manera aleatoria y con reposición para luego ajustar el modelo a dichos datos. La base de datos completa sería análoga a la población. Se asume que todos los pares $(x, y)$ son independientes y están idénticamente distribuidos pero no asume homocedasticidad, independencia entre los errores y los predictores ni que el modelo lineal es correcto.

Hay dos tipos de error en *bootstrap*:

**Error de simulación**: proporcional a $1\sqrt{b}$, donde $b$ es el número de muestras *bootstrap*.

**Error de aproximación**: se divide en error de estimación, por emplear con una muestra finita, y en error sistemático, por usar con un modelo falso.

<br>

## Distribución de los valores p bajo la hipótesis nula

<br>

$$
\begin{align}
P &= F(T) \\\\
\text{Pr}(P < p) &= \text{Pr}\left(F^{-1}(P) < F^{-1}(p)\right) \\\\
&= \text{Pr}(T < t) \\\\
&= F\left(F^{-1}(p)\right) \\\\
&= p
\end{align}
$$

<br>

## Desigualdad de Chebyshev

<br>

$$
\text{P}(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}
$$

La probabilidad de que una variable aleatoria $X$ exceda su valor esperado $\mu$ por $k$ errores típicos es siempre inferior a $\frac{1}{k^2}$.

<br>




## *Capture percentage*

<br>

La esperanza de cobertura del intervalo de confianza para la distribución de $\widehat\beta_0$ o lo que es lo mismo, el promedio de veces que un intervalo de confianza contiene otras estimaciones de $\beta_0$, es:

<br>

$$
\int \phi\left(\widehat\beta_0; \, \beta_0, \text{se}\right) \left(\Phi\left(\widehat\beta_0 + 1.96 \cdot \text{se}; \, \beta_0, \text{se}\right) - \Phi\left(\widehat\beta_0 - 1.96 \cdot \text{se}; \, \beta_0, \text{se}\right)\right) d\widehat\beta_0
$$

<br>


















