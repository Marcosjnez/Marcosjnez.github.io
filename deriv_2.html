<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Estimación</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Marcos Jiménez</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About me</a>
</li>
<li>
  <a href="deriv_2.html">Estadística</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Estimación</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#distribucion-normal">Distribución normal</a></li>
<li><a href="#estimacion-de-mu">Estimación de <span class="math inline">\(\mu\)</span></a></li>
<li><a href="#valor-esperado-de-barx">Valor esperado de <span class="math inline">\(\bar{x}\)</span></a></li>
<li><a href="#varianza-de-widehatmu">Varianza de <span class="math inline">\(\widehat\mu\)</span></a></li>
<li><a href="#estimacion-de-sigma2">Estimación de <span class="math inline">\(\sigma^2\)</span></a></li>
<li><a href="#valor-esperado-de-fracsum_i1n-x_i---widehatmu2n">Valor esperado de <span class="math inline">\(\frac{\sum_{i=1}^n (x_i - \widehat\mu)^2}{n}\)</span></a></li>
<li><a href="#varianza-de-sigma2-y-widehatsigma2">Varianza de <span class="math inline">\(\sigma^2\)</span> y <span class="math inline">\(\widehat\sigma^2\)</span></a></li>
<li><a href="#funcion-afin">Función afín</a></li>
<li><a href="#residuos">Residuos</a></li>
<li><a href="#estimacion-de-beta_1">Estimación de <span class="math inline">\(\beta_1\)</span></a></li>
<li><a href="#estimacion-de-beta_0">Estimación de <span class="math inline">\(\beta_0\)</span></a></li>
<li><a href="#varianza-de-widehatbeta_1">Varianza de <span class="math inline">\(\widehat{\beta}_1\)</span></a></li>
<li><a href="#varianza-de-widehatbeta_0">Varianza de <span class="math inline">\(\widehat{\beta}_0\)</span></a></li>
<li><a href="#predicciones">Predicciones</a></li>
<li><a href="#varianza-del-promedio-de-las-predicciones">Varianza del promedio de las predicciones</a></li>
<li><a href="#varianza-del-error-de-prediccion">Varianza del error de predicción</a></li>
<li><a href="#varianza-del-error-de-estimacion-varianza-de-un-residuo">Varianza del error de estimación (Varianza de un residuo)</a></li>
<li><a href="#varianza-de-la-estimacion-del-error-cuadratico-widehatsigma2">Varianza de la estimación del error cuadrático, <span class="math inline">\(\widehat\sigma^2\)</span></a></li>
<li><a href="#modelo-sin-predictores">Modelo sin predictores</a></li>
<li><a href="#el-test-f">El test F</a></li>
<li><a href="#el-test-de-razon-de-verosimilitudes">El test de razón de verosimilitudes</a></li>
<li><a href="#r2"><span class="math inline">\(R^2\)</span></a></li>
<li><a href="#rho_xy"><span class="math inline">\(\rho_{xy}\)</span></a></li>
<li><a href="#notacion-matricial">Notación matricial</a><ul>
<li><a href="#estimacion-de-boldsymbolbeta">Estimación de <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
<li><a href="#varianza-de-boldsymbolbeta">Varianza de <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
<li><a href="#distribucion-de-boldsymbolbeta">Distribución de <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
<li><a href="#valores-ajustados">Valores ajustados</a></li>
<li><a href="#varianza-de-los-valores-ajustados">Varianza de los valores ajustados</a></li>
<li><a href="#predicciones-1">Predicciones</a></li>
<li><a href="#varianza-de-las-predicciones">Varianza de las predicciones</a></li>
<li><a href="#residuos-1">Residuos</a></li>
<li><a href="#varianza-de-los-residuos">Varianza de los residuos</a></li>
<li><a href="#varianza-de-la-estimacion-del-error-cuadratico-widehatsigma2-1">Varianza de la estimación del error cuadrático, <span class="math inline">\(\widehat\sigma^2\)</span></a></li>
</ul></li>
<li><a href="#derivacion-alternativa">Derivación alternativa</a></li>
<li><a href="#sesgo-de-estimacion-por-omision-de-variables-relacionadas">Sesgo de estimación por omisión de variables relacionadas</a></li>
<li><a href="#colinealidad">Colinealidad</a></li>
<li><a href="#factor-de-inflacion-de-la-varianza">Factor de inflación de la varianza</a></li>
<li><a href="#autovalores-y-autovectores">Autovalores y autovectores</a></li>
<li><a href="#derivadas-respecto-a-vectores">Derivadas respecto a vectores</a></li>
<li><a href="#esperanza-y-varianza-de-vectores-y-matrices">Esperanza y varianza de vectores y matrices</a></li>
<li><a href="#el-metodo-delta">El método Delta</a></li>
<li><a href="#regresion-de-componentes-principales">Regresión de componentes principales</a></li>
<li><a href="#regresion-ridge">Regresión <em>Ridge</em></a></li>
<li><a href="#regresion-lasso">Regresión <em>Lasso</em></a></li>
<li><a href="#correccion-de-bonferroni">Corrección de Bonferroni</a></li>
<li><a href="#elipsoides-de-confianza">Elipsoides de confianza</a></li>
<li><a href="#interacciones">Interacciones</a></li>
<li><a href="#distribucion-de-los-valores-p-bajo-la-hipotesis-nula">Distribución de los valores p bajo la hipótesis nula</a></li>
<li><a href="#desigualdad-de-chebyshev">Desigualdad de Chebyshev</a></li>
<li><a href="#capture-percentage"><em>Capture percentage</em></a></li>
</ul>
</div>

<p><br></p>
<div id="distribucion-normal" class="section level2">
<h2>Distribución normal</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbf{x} &amp;= x_i, \dots, x_n \\\\
P(\mathbf{x}; \mu, \sigma) &amp;= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
L(\mu, \sigma; \mathbf{x}) &amp;= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &amp;= \sum_{i=1}^n -\frac{(x_i - \mu)^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi}) \\\\
\text{log}(L) &amp;= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi)
\end{align}
\]</span></p>
<p><br></p>
<p><em>Location-scale property</em></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
Z &amp;\sim N(\mu, \sigma^2) \\\\
a + bZ &amp;\sim N(a + \mu, b^2\sigma^2)
\end{align}
\]</span></p>
<p><br></p>
<p><em>Stability property</em></p>
<p><br></p>
<p>Si <span class="math inline">\(Z_{IID} \sim N(\mu, \sigma^2)\)</span>, entonces</p>
<p><span class="math display">\[
\sum_{i=1}^n Z_i \sim N\left(n \mu, n \sigma^2\right)
\]</span></p>
<p><br></p>
</div>
<div id="estimacion-de-mu" class="section level2">
<h2>Estimación de <span class="math inline">\(\mu\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu}&amp;= \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) \\\\
&amp;= \frac{n}{\sigma^2} \, (\bar{x} - \mu)
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\partial \text{log}(L)}{\partial \mu} &amp;= 0 \\\\
\widehat\mu &amp;= \bar{x} \\\\
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="valor-esperado-de-barx" class="section level2">
<h2>Valor esperado de <span class="math inline">\(\bar{x}\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n x_i \right] = \mu
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-widehatmu" class="section level2">
<h2>Varianza de <span class="math inline">\(\widehat\mu\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\partial^2 \text{log}(L)}{\partial \mu^2} &amp;= - \frac{n}{\sigma^2} \\\\
\text{VAR}\left[\widehat\mu\right] &amp;= \mathbb{E}\left[(\widehat\mu - \mu)^2\right] \\\\
&amp;= \frac{\sigma^2}{n}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="estimacion-de-sigma2" class="section level2">
<h2>Estimación de <span class="math inline">\(\sigma^2\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{log}(L) &amp;= -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2} - n\text{log}(\sigma) - \frac{n}{2} \text{log}(2\pi) \\\\
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &amp;= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial \text{log}(L)}{\partial \sigma^2} &amp;= 0 \\\\
\sigma^2 &amp;= \frac{\sum_{i=1}^n (x_i - \mu)^2}{n}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="valor-esperado-de-fracsum_i1n-x_i---widehatmu2n" class="section level2">
<h2>Valor esperado de <span class="math inline">\(\frac{\sum_{i=1}^n (x_i - \widehat\mu)^2}{n}\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &amp;= \frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n ((x_i - \mu) - (\widehat\mu - \mu))^2\right] \\\\
&amp;= \mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2 + (\widehat\mu - \mu)^2 - 2(x_i - \mu)(\widehat\mu - \mu))\right] \\\\
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &amp;= n\sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (\widehat\mu - \mu)^2\right] &amp;= \sigma^2 \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)(\widehat\mu - \mu) \right] &amp;= \mathbb{E}\left[n(\mu^2 + \bar{x}\widehat\mu - \bar{x}\mu - \widehat\mu\mu)\right] \\\\
&amp;= \mathbb{E}\left[n(\mu^2 + \widehat\mu^2 - 2\widehat\mu\mu)\right] \\\\
&amp;= n \, \mathbb{E}\left[(\widehat\mu - \mu)^2 \right] \\\\
&amp;= \sigma^2
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right] &amp;= \sigma^2 + \frac{\sigma^2}{n} - \frac{2\sigma^2}{n} \\\\
&amp;= \sigma^2 \left(\frac{n - 1}{n}\right) \\\\
\end{align}
\]</span> <span class="math display">\[
\begin{align}
\sigma^2 &amp;= \frac{\mathbb{E}\left[\sum_{i=1}^n (x_i - \widehat\mu)^2\right]}{n - 1}
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\widehat\sigma^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n - 1}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-sigma2-y-widehatsigma2" class="section level2">
<h2>Varianza de <span class="math inline">\(\sigma^2\)</span> y <span class="math inline">\(\widehat\sigma^2\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\partial \text{log}(L)}{\partial \sigma^2} &amp;= \frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^4} - \frac{n}{2\sigma^2} \\\\
\frac{\partial^2 \text{log}(L)}{\partial \sigma^4} &amp;= - \frac{\sum_{i=1}^n (x_i - \mu)^2}{\sigma^6} + \frac{n}{2\sigma^4} \\\\
&amp;= \frac{- 2\sum_{i=1}^n (x_i - \mu)^2 + n\sigma^2}{2\sigma^6} \\\\
\mathbb{E}\left[\sum_{i=1}^n (x_i - \mu)^2\right] &amp;= n\sigma^2 \\\\
\mathbb{E}\left[\frac{\partial^2 \text{log}(L)}{\partial \sigma^4}\right] &amp;= \frac{- 2n\sigma^2 + n\sigma^2}{2\sigma^6} \\\\
&amp;= - \frac{n}{2\sigma^4} \\\\
\text{VAR}\left[\sigma^2\right] &amp;= \frac{2\sigma^4}{n}
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\epsilon_i}{\sigma} &amp;\sim N(0, 1) \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 &amp;\sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 = \frac{(n-1)\widehat\sigma^2}{\sigma^2} &amp;\sim \chi^2_{n-1} \\\\
\text{VAR}\left[\chi^2_{n-1}\right] &amp;= 2(n-1) \\\\
\text{VAR}\left[\frac{(n-1)\widehat\sigma^2}{\sigma^2}\right] &amp;= 2(n-1) \\\\
\text{VAR}\left[(n-1)\widehat\sigma^2\right] &amp;= 2\sigma^4(n-1) \\\\
\text{VAR}\left[\widehat\sigma^2\right] &amp;= \frac{2\sigma^4}{n-1} \\\\
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="funcion-afin" class="section level2">
<h2>Función afín</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
y_i &amp;= \beta_0 + \beta_1x_i + \epsilon_i \\\\
y_i &amp;= \widehat{\beta}_0 + \widehat{\beta}_1x_i + e_i
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
P(\mathbf{y}; \beta_0, \beta_1, \sigma, \mathbf{x}) &amp;= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
L(\beta_0, \beta_1, \sigma; \mathbf{y}, \mathbf{x}) &amp;= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \, \text{exp}\left(-\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2}\right) \\\\
\text{log}(L) &amp;= \sum_{i=1}^n -\frac{(y_i - (\beta_0 + \beta_1 x_i))^2}{2 \sigma^2} - \text{log}(\sigma \sqrt{2 \pi})
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="residuos" class="section level2">
<h2>Residuos</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})e_i &amp;= 0, \quad \text{COV}[X, e] = 0 \\\\
\frac{1}{n} \sum_{i=1}^n e_i &amp;= 0
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="estimacion-de-beta_1" class="section level2">
<h2>Estimación de <span class="math inline">\(\beta_1\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\partial L}{\partial \beta_1} &amp;= - \frac{\sum_{i=1}^n \beta_0x_i + \beta_1x_i^2 - y_ix_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_1} &amp;= 0 \\\\
&amp;= \overline{yx} - \beta_0\bar{x} - \beta_1\overline{x^2} \\\\
\end{align}
\]</span> <span class="math display">\[
\begin{align}
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - \beta_0\mathbb{E}[X] &amp;= 0 \\\\
\mathbb{E}[YX] - \beta_1\mathbb{E}[X^2] - (\mathbb{E}[Y] - \beta_1\mathbb{E}[X]) \, \mathbb{E}[X] &amp;= 0 \\\\
\text{COV}[YX] - \beta_1\mathbb{E}[X^2] - \beta_1\mathbb{E}[X]^2 &amp;= 0 \\\\
\text{COV}[YX] - \beta_1\text{VAR}[X] &amp;= 0 \\\\
\end{align}
\]</span> <span class="math display">\[
\begin{align}
\beta_1 &amp;= \frac{\text{COV}[YX]}{\text{VAR}[X]} \\\\
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} &amp;= \frac{\sum_{i=1}^n x_i  y_i - \bar{x} \bar{y}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&amp;= \frac{\sum_{i=1}^n x_i (\beta_0 + \beta_1x_i + \epsilon_i) - \bar{x} (\beta_0 + \beta_1\bar{x} + \bar{\epsilon})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&amp;= \frac{n\beta_1(\overline{x^2} - \bar{x}^2)}{\sum_{i=1}^n (x_i - \bar{x})^2} + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&amp;= \beta_1 + \frac{\sum_{i=1}^n x_i\epsilon_i - \bar{x}\bar{\epsilon}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&amp;= \beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
\end{align}
\]</span></p>
<p><br></p>
<p>Según la ley de la esperanza total,</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}\left[\widehat{\beta}_1\right] &amp;= \mathbb{E}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_i, \dots, x_n\right]\right] \\\\
&amp;= \mathbb{E}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \mathbb{E}[\epsilon_i]}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \, , \quad \mathbb{E}[\epsilon \, | \, x_i] = 0 \\\\
&amp;= \beta_1 \\\\
\widehat{\beta}_1 &amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="estimacion-de-beta_0" class="section level2">
<h2>Estimación de <span class="math inline">\(\beta_0\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\partial L}{\partial \beta_0} &amp;=- \frac{\sum_{i=1}^n \beta_0 + \beta_1x_i - y_i}{\sigma^2} \\\\
\frac{\partial L}{\partial \beta_0} &amp;= 0 \\\\
\widehat{\beta}_0 &amp;= \bar{y} - \widehat{\beta}_1\bar{x} \\\\
\mathbb{E}\left[\widehat{\beta}_0\right] &amp;= \beta_0 + \beta_1\mathbb{E}[X] - \beta_1\mathbb{E}[X] \\\\
&amp;= \beta_0
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-widehatbeta_1" class="section level2">
<h2>Varianza de <span class="math inline">\(\widehat{\beta}_1\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right] &amp;= \text{VAR}\left[\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2}\right] \\\\
&amp;= \sum_{i=1}^n \frac{(x_i - \bar{x})^2 \, \text{VAR}\left[\epsilon_i\right]}{n^2 s_x^4} \\\\
&amp;= \frac{\sigma^2}{n s_x^2}
\end{align}
\]</span></p>
<p>Aplicando la ley de la varianza total,</p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\widehat{\beta}_1\right] &amp;= \text{VAR}\left[\mathbb{E}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{\beta}_1 \, | \, x_1, \dots, x_n\right]\right] \\\\
&amp;= \frac{\sigma^2}{n s_x^2} \\\\
\widehat{\text{VAR}}\left[\widehat{\beta}_1\right] &amp;= \frac{\widehat\sigma^2}{n s_x^2}
\end{align}
\]</span></p>
<p>Dado que <span class="math inline">\(\epsilon_i\)</span> sigue una distribución normal, la propiedad de estabilidad asegura que</p>
<p><span class="math display">\[
\begin{align}
\widehat{\beta}_1 &amp;\sim N\left(\beta_1, \frac{\sigma^2}{ns_x^2} \right) \\\\\
\frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} &amp;\sim N\left(0, 1 \right) \\\\\
\end{align}
\]</span></p>
<p>Si conociéramos <span class="math inline">\(\sigma^2\)</span>, entonces</p>
<p><span class="math display">\[
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_1 - \beta_1}{\sigma / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
\]</span></p>
<p>Pero si estimamos <span class="math inline">\(\sigma^2\)</span> a través de <span class="math inline">\(\widehat\sigma^2\)</span>, entonces</p>
<p><span class="math display">\[
\begin{align}
\frac{\widehat{\beta}_1 - \beta_1}{\widehat\sigma / s_x\sqrt{n}} &amp;= \frac{\frac{\widehat{\beta}_1 - \beta_1}{\sigma}}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&amp;\sim \frac{N(0, 1/ns_x^2)}{\frac{\widehat\sigma}{\sigma s_x\sqrt{n}}} \\\\
&amp;\sim \frac{N(0, 1)}{\frac{\widehat\sigma}{\sigma}} \\\\
&amp;\sim \frac{N(0, 1)}{\sqrt{\frac{\sum_{i=1}^n e_i^2}{\sigma^2 (n-2)}}} \\\\
&amp;\sim \frac{N(0, 1)}{\sqrt{\frac{\chi^2_{n-2}}{n-2}}} \\\\
&amp;\sim t_{n-2}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-widehatbeta_0" class="section level2">
<h2>Varianza de <span class="math inline">\(\widehat{\beta}_0\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[ \widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &amp;= \text{VAR}\left[\bar{y} - \widehat{\beta}_1\bar{x}\right] \\\\
&amp;= \text{VAR}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon} - \widehat{\beta}_1\bar{x}\right] \\\\
&amp;= \text{VAR}\left[\bar{\epsilon}\right] + \bar{x}^2 \, \text{VAR}\left[\widehat{\beta}_1\right] - 2\bar{x}^2\text{COV}\left[\bar{\epsilon}, \widehat{\beta}_1\right]
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\text{COV}\left[\bar{\epsilon}, \widehat{\beta}_1\right] = 0
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\widehat{\beta}_0 \, | \, x_i, \dots, x_n \right] &amp;= \frac{\sigma^2}{n} + \frac{\sigma^2 \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&amp;= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2} - \sigma^2 s_x^2}{n s_x^2} \\\\
&amp;= \frac{\sigma^2}{n} + \frac{\sigma^2 \, \overline{x^2}}{n s_x^2} - \frac{\sigma^2}{n} \\\\
&amp;= \frac{\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
\]</span></p>
<p><br></p>
<p>Según la ley de la esperanza total,</p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\widehat{\beta}_0\right] &amp;= \text{VAR}\left[\mathbb{E}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{\beta}_0 \, | \, x_1, \dots, x_n\right]\right] \\\\
&amp;= \frac{\sigma^2 \overline{x^2}}{n s_x^2} \\\\
\widehat{\text{VAR}}\left[\widehat{\beta}_0\right] &amp;= \frac{\widehat\sigma^2 \overline{x^2}}{n s_x^2}
\end{align}
\]</span></p>
<p>Dado que <span class="math inline">\(\epsilon_i\)</span> sigue una distribución normal, la propiedad de estabilidad asegura que</p>
<p><span class="math display">\[
\begin{align}
\widehat{\beta}_0 &amp;\sim N\left(\beta_0, \frac{\sigma^2 \overline{x^2}}{n s_x^2} \right) \\\\\
\frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} &amp;\sim N\left(0, 1 \right) \\\\\
\end{align}
\]</span></p>
<p>Si conociéramos <span class="math inline">\(\sigma^2\)</span>, entonces</p>
<p><span class="math display">\[
P\left(\Phi^{-1}(.025) \leq \frac{\widehat{\beta}_0 - \beta_0}{\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \leq \Phi^{-1}(.975) \right) = .95
\]</span></p>
<p>Pero si estimamos <span class="math inline">\(\sigma^2\)</span> a través de <span class="math inline">\(\widehat\sigma^2\)</span>, entonces</p>
<p><span class="math display">\[
\frac{\widehat{\beta}_0 - \beta_0}{\widehat\sigma \sqrt{s_x^2 + \bar{x}^2} / s_x\sqrt{n}} \sim t_{n-2}
\]</span></p>
<p><br></p>
</div>
<div id="predicciones" class="section level2">
<h2>Predicciones</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\widehat{m}(x_0) &amp;= \widehat{\beta}_0 + \widehat{\beta}_1x_0 \\\\
&amp;= \bar{y} - \widehat{\beta}_1\bar{x} + \widehat{\beta}_1x_0 \\\\
&amp;= \bar{y} + (x_0 - \bar{x})\widehat{\beta}_1 \\\\
&amp;= \beta_0 + \beta_1\bar{x} + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x_0 - \bar{x})\left(\beta_1 + \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\\\
&amp;= \beta_0 + \beta_1\bar{x} + (x_0 - \bar{x})\beta_1 + \frac{1}{n} \sum_{i=1}^n \epsilon_i + (x_0 - \bar{x}) \frac{\sum_{i=1}^n (x_i - \bar{x}) \epsilon_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\\\
&amp;= \beta_0 + \beta_1x_0 + \frac{1}{n} \sum_{i=1}^n \epsilon_i \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right)
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}\left[\widehat{m}(x_0)\right] &amp;= \beta_0 + \beta_1x_0 + \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\epsilon_i] \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right) \\\\
&amp;= \beta_0 + \beta_1x_0
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-del-promedio-de-las-predicciones" class="section level2">
<h2>Varianza del promedio de las predicciones</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n \right] &amp;= \frac{1}{n^2} \sum_{i=1}^n \text{VAR}[\epsilon_i] \left(1 + (x_0 - \bar{x}) \frac{x_i - \bar{x}}{s_x^2}\right)^2 \\\\
&amp;= \frac{\sigma^2}{n^2} \sum_{i=1}^n \left(1 + (x_0 - \bar{x})^2 \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&amp;= \frac{\sigma^2}{n^2} \left(n + (x_0 - \bar{x})^2 \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{s_x^4}\right) \\\\
&amp;= \frac{\sigma^2}{n^2} \left(n + (x_0 - \bar{x})^2 \frac{n}{s_x^2}\right) \\\\
&amp;= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)
\end{align}
\]</span></p>
<p>Según la ley de la esperanza total,</p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\widehat{m}(x_0)\right] &amp;= \text{VAR}\left[\mathbb{E}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n\right]\right] + \mathbb{E}\left[\text{VAR}\left[\widehat{m}(x_0) \, | \, x_i, \dots, x_n\right]\right] \\\\
&amp;= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)
\end{align}
\]</span></p>
<p><br></p>
<p>La propiedad de estabilidad asegura que</p>
<p><span class="math display">\[
\begin{align}
\widehat{m}(x_0) &amp;\sim N\left(\beta_0 + \beta_1x_0, \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)\right) \\\\
\frac{\widehat{m}(x_0) - m(x_0)}{\sqrt{\frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)}} &amp;\sim N(0, 1) \\\\
\frac{\widehat{m}(x_0) - m(x_0)}{\sqrt{\frac{\widehat\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right)}} &amp;\sim t_{n-2} \\\\
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-del-error-de-prediccion" class="section level2">
<h2>Varianza del error de predicción</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
y_0 &amp;= \beta_0 + \beta_1x_0 + \epsilon_0 \\\\
\widehat{m}(x_0) &amp;= \widehat\beta_0 + \widehat\beta_1x_0 \\\\
y_0 - \widehat{m}(x_0) &amp;= \beta_0 + \beta_1x_0 - \widehat\beta_0 - \widehat\beta_1x_0 + \epsilon_0 \\\\
\text{COV}[\widehat{m}(x_0), \epsilon_0] &amp;= 0 \\\\
\text{VAR}[y_0 - \widehat{m}(x_0)] &amp;= \frac{\sigma^2}{n} \left(1 + \frac{(x_0 - \bar{x})^2}{s_x^2}\right) + \sigma^2 \\\\
\text{VAR}[y_0 - \widehat{m}(x_0)] &amp;= \sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)
\end{align}
\]</span></p>
<p><br></p>
<p>La propiedad de estabilidad asegura que</p>
<p><span class="math display">\[
\begin{align}
\widehat{m}(x_0) &amp;\sim N\left(y_0, \sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)\right) \\\\
\frac{y_0 - \widehat{m}(x_0)}{\sqrt{\sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)}} &amp;\sim N(0, 1) \\\\
\frac{y_0 - \widehat{m}(x_0)}{\sqrt{\widehat\sigma^2 \left(1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{n s_x^2}\right)}} &amp;\sim t_{n-2}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-del-error-de-estimacion-varianza-de-un-residuo" class="section level2">
<h2>Varianza del error de estimación (Varianza de un residuo)</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\widehat{\beta}_0 &amp;= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &amp;= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
y_i &amp;= \beta_0 + \beta_1x_i + \epsilon_i \\\\
\text{VAR}\left[y_i\right] &amp;= \sigma^2 \\\\
\widehat{m}(x_i) &amp;= \widehat\beta_0 + \widehat\beta_1x_i \\\\
&amp;= \bar{y} - \widehat\beta_1\bar{x} + \widehat\beta_1x_i \\\\
&amp;= \bar{y} + (x_i - \bar{x})\widehat\beta_1 \\\\
&amp;= \bar{y} + (x_i - \bar{x})\left(\beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
&amp;= \beta_0 + \beta_1x_i + (x_i - \bar{x})\left(\frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i\right) \\\\
\text{VAR}\left[\widehat{m}(x_i)\right] &amp;=  \frac{\sigma^2}{n} + (x_i - \bar{x})^2 \frac{\sigma^2}{ns_x^2} \\\\
&amp;= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
\mathbb{E}\left[(y_i - \widehat{m}(x_i))^2\right] &amp;= \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\,\text{COV}\left[y_i, \widehat{m}(x_i)\right]
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{COV}\left[y_i, \widehat{m}(x_i)\right] &amp;= \text{COV}\left[y_i + \widehat{m}(x_i) - \widehat{m}(x_i), \widehat{m}(x_i) \right] \\\\
&amp;= \text{COV}\left[\widehat{m}(x_i), \widehat{m}(x_i) \right] + \text{COV}\left[y_i - \widehat{m}(x_i), \widehat{m}(x_i) \right] \\\\
&amp;= \text{COV}\left[\widehat{m}(x_i), \widehat{m}(x_i) \right] + \text{COV}\left[e_i, \widehat{m}(x_i) \right] \\\\
&amp;= \text{VAR}\left[\widehat{m}(x_i)\right] + 0 \\\\
&amp;= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}\left[(y_i - \widehat{m}(x_i))^2\right] &amp;= \sigma^2 + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) - 2\sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&amp;= \sigma^2 - \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{ns_x^2}\right) \\\\
&amp;= \sigma^2 \left(1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{ns_x^2}\right)
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-la-estimacion-del-error-cuadratico-widehatsigma2" class="section level2">
<h2>Varianza de la estimación del error cuadrático, <span class="math inline">\(\widehat\sigma^2\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\mathbb{E}\left[(Y - (\beta_0 + \beta_1X))^2\right] = \sigma^2
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\bar{y} \, | \, x_i, \dots, x_n\right] &amp;= \text{VAR}\left[\beta_0 + \beta_1\bar{x} + \bar{\epsilon}\right] \\\\
&amp;= \frac{1}{n^2} \sum_{i=1}^n \text{VAR}\left[\epsilon_i\right] \\\\
&amp;= \frac{\sigma^2}{n}
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\widehat{\beta}_0 &amp;= \beta_0 + \frac{1}{n} \sum_{i=1}^n \left(1 - \bar{x} \, \frac{x_i - \bar{x}}{s_x^2}\right) \epsilon_i \\\\
\widehat{\beta}_1 &amp;= \beta_1 + \frac{1}{n} \sum_{i=1}^n \frac{x_i - \bar{x}}{s_x^2} \epsilon_i
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
 \mathbb{E}\left[\sum_{i=1}^n e_i^2 \, | \, x_i, \dots, x_n\right] &amp;= \mathbb{E}\left[\sum_{i=1}^n (y_i - \widehat{m}(x_i))^2\right] \\\\
&amp;= \mathbb{E}\left[\sum_{i=1}^n \left(\beta_0 + \beta_1x_i + \epsilon_i - \widehat{\beta}_0 - \widehat{\beta}_1x_i\right)^2\right] \\\\
&amp;= \mathbb{E}\left[\sum_{i=1}^n \left((\beta_0 - \widehat{\beta}_0)^2 + (\beta_1 - \widehat{\beta}_1)^2x_i^2 + \epsilon_i^2\right)\right] + \\\\
&amp; 2 \, \mathbb{E}\left[\sum_{i=1}^n (\beta_0 - \widehat{\beta}_0)(\beta_1 - \widehat{\beta}_1)x_i + (\beta_0 - \widehat{\beta}_0)\epsilon_i + (\beta_1 - \widehat{\beta}_1)x_i\epsilon_i \right] \\\\
&amp;= n \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2}}{n s_x^2}\right] + 1 \right) - 2\sigma^2 \, \mathbb{E}\left[\frac{\bar{x}^2}{s_x^2} \right] - 2 \sigma^2 - 2 \sigma^2 \\\\
&amp;= \sigma^2 \left(2 \, \mathbb{E}\left[\frac{\overline{x^2} - \bar{x}^2}{s_x^2}\right] + n - 4 \right) \\\\
&amp;= \sigma^2 \left(n - 2 \right) \\\\
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\widehat\sigma^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2 \\\\
\]</span></p>
<p><span class="math inline">\(\widehat{\sigma}^2\)</span> es estadísticamente independiente de <span class="math inline">\(\widehat\beta_1\)</span> y <span class="math inline">\(\widehat\beta_0\)</span> a pesar de que todos los estimadores son una función de <span class="math inline">\(\epsilon\)</span>.</p>
<p><br></p>
<p>Sabiendo que si <span class="math inline">\(Z \sim N(0, 1)\)</span>, entonces <span class="math inline">\(Z^2 \sim \chi^2\)</span> y que <span class="math inline">\(\frac{\epsilon_i}{\sigma} \sim N(0, 1)\)</span>, entonces</p>
<p><span class="math display">\[
\begin{align}
\frac{1}{\sigma^2} \sum_{i=1}^n \epsilon_i^2 &amp;= \sum_{i=1}^n \left(\frac{\epsilon_i}{\sigma}\right)^2 \sim \chi^2_n \\\\
\frac{1}{\sigma^2} \sum_{i=1}^n e_i^2 &amp;\sim \chi^2_{n-2} \\\\
P\left(\chi^2_{n-2} \; \leq \; \chi^2_{n-2, \, 1-\alpha}\right) &amp;= 1 - \alpha \\\\
P\left(\sigma^2 \; \leq \; \frac{(n-2) \widehat\sigma^2}{\chi^2_{n-2, \, 1-\alpha}}\right) &amp;= 1 - \alpha \\\\
\text{VAR}\left[\chi^2_{n-2}\right] &amp;= 2(n-2) \\\\
\text{VAR}\left[\frac{(n-2)\widehat\sigma^2}{\sigma^2}\right] &amp;= 2(n-2) \\\\
\text{VAR}\left[(n-2)\widehat\sigma^2\right] &amp;= 2\sigma^4(n-2) \\\\
\text{VAR}\left[\widehat\sigma^2\right] &amp;= \frac{2\sigma^4}{n-2} \\\\
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="modelo-sin-predictores" class="section level2">
<h2>Modelo sin predictores</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
Y &amp;= \beta_0 + \epsilon \\\\
\widehat\beta_0 &amp;= \bar{y} \sim N\left(\beta_0, \frac{\sigma^2}{n}\right) \\\\
\widehat\sigma^2 &amp;= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \\\\
&amp;= s_y^2 \\\\
\frac{(n-1) \widehat\sigma^2}{\sigma^2} &amp;\sim \chi^2_{n-1}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="el-test-f" class="section level2">
<h2>El test F</h2>
<p><br></p>
<p>La distribución <span class="math inline">\(\chi^2\)</span> procede de la suma del cuadrado de variables con distribución normal y la distribución <span class="math inline">\(F\)</span> procede de la razón entre variables aleatorias con distribución <span class="math inline">\(\chi^2\)</span>.</p>
<p><span class="math display">\[
\frac{\chi^2_a}{\chi^2_b} \frac{b}{a} \sim F_{a, b}
\]</span></p>
<p>Según el modelo sin predictores,</p>
<p><span class="math display">\[
\frac{(n-1) \widehat\sigma^2_{null}}{\sigma^2} \sim \chi^2_{n-1}
\]</span></p>
<p>mientras que según el modelo con un predictor,</p>
<p><span class="math display">\[
\frac{(n-2) \widehat\sigma_2^2}{\sigma^2} \sim \chi^2_{n-2}
\]</span></p>
<p>Por tanto,</p>
<p><span class="math display">\[
\begin{align}
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{\sigma^2} &amp;\sim \chi^2_1 \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{(n-2) \widehat\sigma_2^2} &amp;\sim \frac{\chi^2_1}{\chi^2_{n-2}} \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{(n-2) \widehat\sigma_2^2} \, \frac{n-2}{1} &amp;\sim F_{1, n-2} \\\\
\frac{(n-1) \widehat\sigma^2_{null} - (n-2) \widehat\sigma_2^2}{\widehat\sigma_2^2} &amp;\sim F_{1, n-2}
\end{align}
\]</span></p>
<p>El test <span class="math inline">\(F\)</span> asume que los errores (<span class="math inline">\(\epsilon\)</span>) siguen una distribución normal, son homocedásticos e independientes de la variable <span class="math inline">\(X\)</span> y entre sí. Se trata de un caso particular de la razón de verosimilitudes.</p>
<p><br></p>
<p>En el caso general, si estuviéramos contrastando la significación de <span class="math inline">\(p-q\)</span> estimaciones,</p>
<p><span class="math display">\[
\begin{align}
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{\sigma^2} &amp;\sim \chi^2_{p-q} \\\\
\frac{(n-p) \widehat\sigma^2_{p}}{\sigma^2} &amp;\sim \chi^2_{n-p} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(n-p) \widehat\sigma^2_{p}} &amp;\sim \frac{\chi^2_{p-q}}{\chi^2_{n-p}} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(n-p) \widehat\sigma^2_{p}} \, \frac{n-p}{p-q} &amp;\sim F_{p-q, n-p} \\\\
\frac{(n-q) \widehat\sigma^2_{q} - (n-p) \widehat\sigma^2_{p}}{(p-q) \widehat\sigma^2_{p}} &amp;\sim F_{p-q, n-p}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="el-test-de-razon-de-verosimilitudes" class="section level2">
<h2>El test de razón de verosimilitudes</h2>
<p><br></p>
<p>El espacio paramétrico del modelo nulo de <span class="math inline">\(q\)</span> coeficientes es un subconjunto del espacio paramétrico del modelo general de <span class="math inline">\(p\)</span> coeficientes.</p>
<p>El test de razón de verosimilitudes asume que las estimaciones de los parámetros siguen una distribución normal y esto es solo aproximadamente cierto para <span class="math inline">\(\widehat\sigma^2\)</span>. Por tanto, solo es exactamente correcto cuando <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\Lambda &amp;= L(\widehat\Theta) - L(\widehat\theta) \\\\
\lim_{n \rightarrow \infty} \quad 2\Lambda &amp;\sim \chi^2_{p-q}
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
L(\widehat\theta) &amp;= - \frac{n}{2} \log2\pi - \frac{n}{2} \log2(n-q)\widehat\sigma^2_q - \frac{1}{2(n-q)\widehat\sigma^2_q} \sum_{i=1}^n (y_i - \widehat\beta_0)^2 \\\\
&amp;= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log (n-q)\widehat\sigma^2_q} \\\\
L(\widehat\Theta) &amp;= - \frac{n}{2} \log2\pi - \frac{n}{2} \log2(n-p)\widehat\sigma^2_p - \frac{1}{2(n-p)\widehat\sigma^2_p} \sum_{i=1}^n (y_i - \widehat\beta_0 - \widehat\beta_1x_i)^2 \\\\
&amp;= - \frac{n}{2}(1 + \log 2\pi) - \frac{n}{2}{\log (n-p)\widehat\sigma^2_p}
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
L(\widehat\Theta) - L(\widehat\theta)
&amp;= \frac{n}{2} \log\frac{(n-q)\widehat\sigma^2_q}{(n-p)\widehat\sigma^2_p} \\\\
\lim_{n \rightarrow \infty} \quad n \log\frac{(n-q)\widehat\sigma^2_q}{(n-p)\widehat\sigma^2_p} &amp;\sim \chi^2_{p-q}
\end{align}
\]</span></p>
<p>Incluso cuando ambos modelos son falsos, el test de razón de verosimilitudes nos puede indicar cuál de ellos se parece más al verdadero.</p>
<p><br></p>
</div>
<div id="r2" class="section level2">
<h2><span class="math inline">\(R^2\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
R^2 &amp;= \frac{\text{COV}\left[y, \widehat{m}\right]}{s_y^2} \\\\
\text{COV}\left[y, \widehat{m}\right] &amp;= \text{COV}\left[\widehat{m} + e, \widehat{m}\right] \\\\
&amp;= s_\widehat{m}^2 + \text{COV}\left[e, \widehat{m}\right] \\\\
&amp;= s_\widehat{m}^2 \\\\
R^2 &amp;= \frac{s_\widehat{m}^2}{s_y^2} \\\\
s_\widehat{m}^2 &amp;= s_{\widehat{\beta}_0 + \widehat{\beta}_1X}^2 \\\\
&amp;= \widehat{\beta}_1^2 s_x^2 \\\\
R^2 &amp;= \widehat{\beta}_1^2 \frac{s_x^2}{s_y^2} \\\\
&amp;= \left(\frac{\text{COV}\left[x, y\right]}{s_x s_y}\right)^2 \\\\
&amp;= \frac{s_y^2 - \overline{e^2}}{s_y^2} \\\\
R_{\text{ajustado}}^2 &amp;= \frac{\widehat{s}_y^2 - \widehat\sigma^2}{\widehat{s}_y^2}
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
R^2 &amp;= \frac{\text{VAR}\left[m(X)\right]}{\text{VAR}\left[Y\right]} \\\\
&amp;= \frac{\text{VAR}\left[\beta_0 + \beta_1X\right]}{\text{VAR}\left[\beta_0 + \beta_1X + \epsilon\right]} \\\\
R^2 &amp;= \frac{\beta_1^2 \text{VAR}\left[X\right]}{\beta_1^2 \text{VAR}\left[X\right] + \sigma}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="rho_xy" class="section level2">
<h2><span class="math inline">\(\rho_{xy}\)</span></h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\rho_{XY} &amp;= \frac{\text{COV}\left[X, Y\right]}{\sqrt{\text{VAR}\left[X\right] \text{VAR}\left[Y\right]}} \\\\
&amp;= \beta_1 \frac{\text{VAR}\left[X\right]}{\sqrt{\text{VAR}\left[Y\right]}}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="notacion-matricial" class="section level2">
<h2>Notación matricial</h2>
<blockquote>

</blockquote>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbf{y} &amp;= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
MSE &amp;= \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&amp;= \frac{1}{n} (\mathbf{y}^\prime - \boldsymbol{\beta}^\prime \mathbf{X}^\prime) (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\\\
&amp;= \frac{1}{n} (\mathbf{y}^\prime \mathbf{y} - \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
\mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} &amp;= \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} \quad \text{porque} \; \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} \; \text{es una matriz} \; 1 \times 1 \\\\
MSE &amp;= \frac{1}{n} (\mathbf{y}^\prime \mathbf{y} - 2 \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta})
\end{align}
\]</span></p>
<p><br></p>
<div id="estimacion-de-boldsymbolbeta" class="section level3">
<h3>Estimación de <span class="math inline">\(\boldsymbol{\beta}\)</span></h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\nabla MSE &amp;= \frac{1}{n} (\nabla \mathbf{y}^\prime \mathbf{y} - 2 \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
&amp;= \frac{1}{n} (0 - 2 \mathbf{X}^\prime \mathbf{y} + 2 \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}) \\\\
&amp;= \frac{2}{n} (- \mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta})
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbf{X}^\prime \mathbf{X} \boldsymbol{\widehat\beta} - \mathbf{X}^\prime \mathbf{y} &amp;= 0 \\\\
\boldsymbol{\widehat\beta} &amp;= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-boldsymbolbeta" class="section level3">
<h3>Varianza de <span class="math inline">\(\boldsymbol{\beta}\)</span></h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbf{\widehat\beta} &amp;= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y} \\\\
&amp;= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}) \\\\
&amp;= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} + (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \boldsymbol{\epsilon} \\\\
&amp;= \boldsymbol{\beta} + (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \boldsymbol{\epsilon} \\\\
\text{VAR}\left[\boldsymbol{\widehat\beta}\right] &amp;= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \,  \text{VAR}\left[\boldsymbol{\epsilon}\right] \left((\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime\right)^\prime \\\\
&amp;= (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \sigma^2 \mathbf{I} \, \mathbf{X}(\mathbf{X}^\prime \mathbf{X})^{-1} \\\\
&amp;= \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \mathbf{X}(\mathbf{X}^\prime \mathbf{X})^{-1} \\\\
&amp;= \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="distribucion-de-boldsymbolbeta" class="section level3">
<h3>Distribución de <span class="math inline">\(\boldsymbol{\beta}\)</span></h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\widehat{\boldsymbol{\beta}} &amp;\sim MVN(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}) \\\\
\widehat{\beta}_i &amp;\sim N(\beta_i, \sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}) \\\\
\frac{\widehat{\beta}_i - \beta_i}{\sigma \sqrt{(\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}}} &amp;\sim N(0, 1) \\\\
\frac{\widehat{\beta}_i - \beta_i}{\widehat\sigma \sqrt{(\mathbf{X}^\prime \mathbf{X})^{-1}_{ii}}} &amp;\sim t_{n-p-1}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="valores-ajustados" class="section level3">
<h3>Valores ajustados</h3>
<p><br></p>
<p><em>Hat</em> o <em>influence matrix</em>:</p>
<p><span class="math display">\[
\begin{align}
\mathbf{H} &amp;= \mathbf{X} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \\\\
\mathbf{\widehat{m}}(\mathbf{X}) &amp;= \mathbf{X} \boldsymbol{\widehat\beta} \\\\
\mathbf{\widehat{m}}(\mathbf{X}) &amp;= \mathbf{H} \mathbf{y}
\end{align}
\]</span></p>
<p>Propiedades de <span class="math inline">\(\mathbf{H}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial \widehat{m}_i}{\partial y_j} &amp;= H_{ij} \\\\
\mathbf{H} &amp;= \mathbf{H}^\prime \\\\
\mathbf{H}^2 &amp;= \mathbf{H} \\\\
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-los-valores-ajustados" class="section level3">
<h3>Varianza de los valores ajustados</h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\mathbf{Hy}\right] &amp;= \text{VAR}\left[\mathbf{H (\mathbf{X} \boldsymbol{\beta + \boldsymbol{\epsilon}})}\right] \\\\
&amp;= \text{VAR}\left[\mathbf{H} \boldsymbol{\epsilon}\right] \\\\
&amp;= \mathbf{H} \, \text{VAR}\left[\boldsymbol{\epsilon}\right] \, \mathbf{H}^\prime \\\\
&amp;= \sigma^2 \mathbf{H}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="predicciones-1" class="section level3">
<h3>Predicciones</h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\widehat{m}(X^*) &amp;= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y} \\\\
&amp;= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\widehat\beta} + \mathbf{e}) \\\\
&amp;= \mathbf{X^*} \boldsymbol{\widehat\beta}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-las-predicciones" class="section level3">
<h3>Varianza de las predicciones</h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\widehat{m}(X^*)\right] &amp;= \text{VAR}\left[\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \mathbf{y}\right] \\\\
&amp;= \text{VAR}\left[\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
&amp;= \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \text{VAR}\left[\boldsymbol{\epsilon}\right] \, (\mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime)^\prime \\\\
&amp;= \sigma^2 \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, \mathbf{X}^\prime \, \mathbf{X} (\mathbf{X}^\prime \mathbf{X})^{-1} \, (\mathbf{X^*})^\prime \\\\
&amp;= \sigma^2 \mathbf{X^*} (\mathbf{X}^\prime \mathbf{X})^{-1} \, (\mathbf{X}^*)^\prime
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="residuos-1" class="section level3">
<h3>Residuos</h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbf{e} &amp;= \mathbf{y} - \mathbf{X}\boldsymbol{\widehat\beta} \\\\
&amp;= \mathbf{y} - \mathbf{Hy} \\\\
&amp;= (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
\]</span></p>
<p>Propiedades de <span class="math inline">\((\mathbf{I} - \mathbf{H}) \mathbf{y}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial e_i}{\partial y_j} &amp;= (\mathbf{I} - H)_{ij} \\\\
(\mathbf{I} - H) &amp;= (\mathbf{I} - H)^\prime \\\\
(\mathbf{I} - H)^2 &amp;= (\mathbf{I} - H) \\\\
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
MSE &amp;= \frac{1}{n} \mathbf{e}^\prime \mathbf{e} \\\\
&amp;= \frac{1}{n} \mathbf{y}^\prime (\mathbf{I} - \mathbf{H})^\prime \, (\mathbf{I} - \mathbf{H}) \mathbf{y} \\\\
&amp;= \frac{1}{n} \mathbf{y}^\prime (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-los-residuos" class="section level3">
<h3>Varianza de los residuos</h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\mathbf{e}\right] &amp;= \text{VAR}\left[(\mathbf{I} - \mathbf{H}) (\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon})\right] \\\\
\mathbf{HX} \boldsymbol{\beta} &amp;= \mathbf{X} \boldsymbol{\beta} \\\\
\text{VAR}\left[\mathbf{e}\right] &amp;= \text{VAR}\left[(\mathbf{I} - \mathbf{H}) \boldsymbol{\epsilon}\right] \\\\
&amp;= (\mathbf{I} - \mathbf{H}) \text{VAR}\left[\boldsymbol{\epsilon}\right] (\mathbf{I} - \mathbf{H})^\prime \\\\
&amp;= \sigma^2 (\mathbf{I} - \mathbf{H})
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="varianza-de-la-estimacion-del-error-cuadratico-widehatsigma2-1" class="section level3">
<h3>Varianza de la estimación del error cuadrático, <span class="math inline">\(\widehat\sigma^2\)</span></h3>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{1}{n} \mathbb{E}\left[\mathbf{e}&#39;\mathbf{e}\right] &amp;= \frac{1}{n} \mathbb{E}\left[((\mathbf{I} - \mathbf{H})\mathbf{e})^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&amp;= \frac{1}{n} \mathbb{E}\left[\mathbf{e}^\prime(\mathbf{I} - \mathbf{H})^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&amp;= \frac{1}{n} \mathbb{E}\left[\mathbf{e}^\prime (\mathbf{I} - \mathbf{H})\mathbf{e} \right] \\\\
&amp;= \frac{1}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \text{VAR}[\mathbf{e}]\right]\\\\
&amp;= \frac{1}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \sigma^2 (\mathbf{I} - \mathbf{H}) \right] \\\\
&amp;= \frac{\sigma^2}{n} \text{tr}\left[(\mathbf{I} - \mathbf{H}) \right] \\\\
\text{tr}[\mathbf{I}] &amp;= n \\\\
\text{tr}[\mathbf{H}] &amp;= p + 1 \\\\
\frac{1}{n} \mathbb{E}\left[\mathbf{e}&#39;\mathbf{e}\right] &amp;= \frac{\sigma^2}{n} (n - p - 1) \\\\
\sigma^2 &amp;= \frac{\mathbb{E}\left[\mathbf{e}&#39;\mathbf{e}\right]}{n - p - 1} \\\\
\widehat\sigma^2 &amp;= \frac{\mathbf{e}&#39;\mathbf{e}}{n - p - 1}
\end{align}
\]</span></p>
<p><span class="math display">\[
\frac{(n-p-1) \widehat\sigma^2}{\sigma^2} \sim \chi^2_{n-p-1}
\]</span></p>
<p><br></p>
</div>
</div>
<div id="derivacion-alternativa" class="section level2">
<h2>Derivación alternativa</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\mathbf{y} &amp;= \mathbf{1}\beta_0 + \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\\\
MSE &amp;= \frac{1}{n} (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}) \\\\
&amp;= \frac{1}{n} (\mathbf{y}^\prime - \mathbf{1}^\prime\beta_0 - \boldsymbol{\beta} \mathbf{X}^\prime) (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}) \\\\
&amp;= \frac{1}{n} (\mathbf{y}^\prime - \mathbf{1}^\prime\beta_0 - \boldsymbol{\beta}^\prime \mathbf{X}^\prime) (\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta})
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\frac{\partial MSE}{\partial \beta_0} &amp;= \frac{1}{n}\left(- \nabla \mathbf{1}^\prime \beta_0 \mathbf{y} + \nabla \mathbf{1}^\prime \beta_0 \beta_0 \mathbf{1} + \nabla \mathbf{1}^\prime \beta_0 \mathbf{X} \boldsymbol{\beta} - \nabla \mathbf{y}^\prime \beta_0 \mathbf{1} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \beta_0 \mathbf{1} \right) \\\\
&amp;= - \frac{2}{n} \mathbf{1}^\prime \left(\mathbf{y} - \beta_0\mathbf{1} - \mathbf{X} \boldsymbol{\beta}\right) \\\\
0 &amp;= - \frac{1}{n} \mathbf{1}^\prime \mathbf{y} + \frac{1}{n} \mathbf{1}^\prime \beta_0 \mathbf{1} + \frac{1}{n} \mathbf{1}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\widehat\beta_0 &amp;= \frac{1}{n} \mathbf{1}^\prime \mathbf{y} - \frac{1}{n} \mathbf{1}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&amp;= \bar{y} - \mathbf{\bar{x}}^\prime \boldsymbol{\beta}
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\nabla_{\boldsymbol{\beta}} \, MSE &amp;= \frac{1}{n} \left(- \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{y} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \beta_0 \mathbf{1} + \nabla \boldsymbol{\beta}^\prime \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} - \nabla \mathbf{y}^\prime \mathbf{X} \boldsymbol{\beta} + \nabla \mathbf{1}^\prime \beta_0 \mathbf{X} \boldsymbol{\beta} \right) \\\\
&amp;= \frac{2}{n} \left(\beta_0 \mathbf{X}^\prime \mathbf{1} - \mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta}\right) \\\\
0 &amp;= \beta_0 \frac{1}{n} \mathbf{X}^\prime \mathbf{1} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&amp;= \beta_0 \mathbf{\bar{x}} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&amp;= \mathbf{\bar{x}} (\bar{y} - \mathbf{\bar{x}}^\prime \boldsymbol{\beta}) - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
&amp;= \mathbf{\bar{x}} \bar{y} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \boldsymbol{\beta} - \frac{1}{n} \mathbf{X}^\prime \mathbf{y} + \frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\frac{1}{n} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \boldsymbol{\beta} &amp;= \frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}} \bar{y} \\\\
\left(\frac{1}{n} \mathbf{X}^\prime \mathbf{X} - \mathbf{\bar{x}} \mathbf{\bar{x}}^\prime \right) \boldsymbol{\beta} &amp;= \frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}} \bar{y} \\\\
\boldsymbol{\widehat\beta} &amp;= \frac{\frac{1}{n} \mathbf{X}^\prime \mathbf{y} - \mathbf{\bar{x}}^\prime \bar{y}}{\frac{1}{n} \mathbf{X}^\prime \mathbf{X} - \mathbf{\bar{x}}^\prime \mathbf{\bar{x}}}
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="sesgo-de-estimacion-por-omision-de-variables-relacionadas" class="section level2">
<h2>Sesgo de estimación por omisión de variables relacionadas</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\widehat\beta_1 &amp;= \frac{\text{COV}\left[X_1, Y\right]}{\text{VAR}\left[X_1\right]} \\\\
&amp;= \frac{\text{COV}\left[X_1, \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p\right]}{\text{VAR}\left[X_1\right]} \\\\
&amp;= \frac{\beta_1 \text{VAR}\left[X_1\right] + \sum_{i=2}^p \beta_p \text{COV}\left[X_1, X_p\right]}{\text{VAR}\left[X_1\right]} \\\\
&amp;= \beta_1 + \sum_{i=2}^p \beta_p \frac{\text{COV}\left[X_1, X_p\right]}{\text{VAR}\left[X_1\right]} \\\\
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="colinealidad" class="section level2">
<h2>Colinealidad</h2>
<p><br></p>
<p>Existen constantes <span class="math inline">\(a_0, a_1, \ldots, a_p\)</span> de manera que, para dos filas <span class="math inline">\(i\)</span>,</p>
<p><span class="math display">\[
a_0 + \sum_{j=1}^p a_j x_{ij} = 0
\]</span></p>
<p><span class="math inline">\(\mathbf{X}^\prime \mathbf{X}\)</span> no es invertible cuando <span class="math inline">\(n &lt; p + 1\)</span>, uno de los predictores es constante o alguno de los predictores es proporcional o está linealmente relacionado con otro.</p>
<p>Dos vectores son linealmente independientes si ninguna combinación lineal de ellos equivale a 0.</p>
<p>La multicolinealidad ocurre cuando</p>
<p><span class="math display">\[
\begin{align}
\sum_{i=1}^p a_i \mathbf{x}_i &amp;= a_0 \\\\
\mathbf{a}^\prime \mathbf{X} &amp;= a_0 \\\\
\text{VAR}\left[\mathbf{a}^\prime \mathbf{X} \right] &amp;= \mathbf{a}^\prime \, \text{VAR}\left[\mathbf{X}\right] \mathbf{a} \\\\
&amp;= 0
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="factor-de-inflacion-de-la-varianza" class="section level2">
<h2>Factor de inflación de la varianza</h2>
<p><br></p>
<p><span class="math display">\[
\text{VIF}_i = \frac{\frac{\widehat\sigma^2}{n s_{x_{i}}^2}}{\widehat\sigma^2 (\mathbf{X}^\prime \mathbf{X})^{-1}_{i+1, i+1}}
\]</span></p>
<p>El factor de inflación de la varianza equivale a regresar <span class="math inline">\(\mathbf{x}_i\)</span> en el resto de predictores y calcular <span class="math inline">\(\frac{1}{1 - R^2}\)</span>.</p>
<p><br></p>
</div>
<div id="autovalores-y-autovectores" class="section level2">
<h2>Autovalores y autovectores</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\mathbf{X}\right] \mathbf{v}_i &amp;= \lambda_i \mathbf{v}_i \\\\
\mathbf{V}^\prime \mathbf{V} &amp;= \mathbf{I} \\\\
\mathbf{V}^{-1} &amp;= \mathbf{V}^\prime \\\\
\text{VAR}\left[\mathbf{X}\right] &amp;= \mathbf{V} \, \mathbf{U} \, \mathbf{V}^\prime
\end{align}
\]</span></p>
<p>Los autovectores de <span class="math inline">\(\text{VAR}\left[\mathbf{X}\right]\)</span> se conocen como los componentes principales de los predictores.</p>
<p>Cualquier vector <span class="math inline">\(\mathbf{a}\)</span> puede ser reescrito como una suma de autovectores:</p>
<p><span class="math display">\[
a = \sum_{i=1}^p (\mathbf{a}^\prime \mathbf{v}_i) \mathbf{v}_i
\]</span></p>
<p><br></p>
</div>
<div id="derivadas-respecto-a-vectores" class="section level2">
<h2>Derivadas respecto a vectores</h2>
<p><br></p>
<p>Si <span class="math inline">\(\mathbf{a}\)</span> y <span class="math inline">\(\mathbf{x}\)</span> son vectores <span class="math inline">\(p \times 1\)</span>, entonces</p>
<p><span class="math display">\[
\begin{align}
\nabla_{\mathbf{x}} (\mathbf{x}^\prime \mathbf{a}) &amp;= \mathbf{a} \\\\
\nabla_{\mathbf{x}} (\mathbf{b} \mathbf{x}) &amp;= \mathbf{b}^\prime \\\\
\end{align}
\]</span></p>
<p>Si <span class="math inline">\(\mathbf{c}\)</span> es una matriz de dimensiones <span class="math inline">\(p \times p\)</span>, entonces</p>
<p><span class="math display">\[
\nabla_{\mathbf{x}} (\mathbf{x}^\prime \mathbf{c} \mathbf{x}) = (\mathbf{c} + \mathbf{c}^\prime) \mathbf{x}
\]</span></p>
<p>Si, además, <span class="math inline">\(\mathbf{c} = \mathbf{c}^\prime\)</span>, entonces</p>
<p><span class="math display">\[
\nabla{\mathbf{x}} (\mathbf{x}^\prime \mathbf{c} \mathbf{x}) = 2 \mathbf{c} \mathbf{x}
\]</span></p>
<p><br></p>
</div>
<div id="esperanza-y-varianza-de-vectores-y-matrices" class="section level2">
<h2>Esperanza y varianza de vectores y matrices</h2>
<p><br></p>
<p>Si <span class="math inline">\(\mathbf{x}\)</span> es un vector aleatorio <span class="math inline">\(n \times 1\)</span>, entonces</p>
<p><span class="math display">\[
\begin{align}
\text{VAR}\left[\mathbf{x}\right] &amp;= \mathbb{E}\left[\mathbf{x} \mathbf{x}^\prime\right] - \mathbb{E}\left[\mathbf{x}\right] \mathbb{E}\left[\mathbf{x}\right]^\prime \\\\
\text{VAR}\left[b\mathbf{x}\right] &amp;= b^2 \mathbf{x} \\\\
\text{VAR}\left[\mathbf{cx}\right] &amp;= \mathbf{c} \text{VAR}\left[\mathbf{x} \right] \mathbf{c}^\prime \\\\
\mathbb{E}\left[\mathbf{x}^\prime \mathbf{cx} \right] &amp;= \mathbb{E}\left[\mathbf{x}\right]^\prime \mathbf{c}\mathbb{E}\left[\mathbf{x}\right] + \text{tr} \, \mathbf{c} \text{VAR}\left[\mathbf{x}\right] \\\\
\mathbf{x}^\prime \mathbf{cx} &amp;= \text{tr} \, \mathbf{x}^\prime \mathbf{cx} \\\\
&amp;= \text{tr} \, \mathbf{cxx}^\prime
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="el-metodo-delta" class="section level2">
<h2>El método Delta</h2>
<p><br></p>
<p><span class="math inline">\(\theta\)</span> es un parámetro que pretendemos estimar a través de un estimador insesgado <span class="math inline">\(\hat\theta\)</span>. Supongamos que <span class="math inline">\(\theta\)</span> es una función de un vector de parámetros <span class="math inline">\(\boldsymbol{\psi}\)</span>,</p>
<p><br></p>
<p><span class="math display">\[
\theta = f\left(\psi_1, \dots, \psi_n \right)
\]</span></p>
<p>Entonces,</p>
<p><span class="math display">\[
\widehat\theta = f\left(\widehat\psi_1, \dots, \widehat\psi_p \right)
\]</span></p>
<p><br></p>
<p>Usando la expansión de Taylor,</p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\theta &amp;\approx \widehat\theta + \sum_{i=1}^p \left(\psi_i - \widehat\psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\hat\theta &amp;\approx \theta + \sum_{i=1}^p \left(\widehat\psi_i - \psi_i \right) {\left. \frac{\partial f}{\partial \psi_i} \right|}_{\psi=\widehat\psi} \\\\
\text{VAR}\left[\hat\theta\right] &amp;\approx \sum_{i=1}^{p} {\left. \frac{\partial^2 f}{\partial \psi_i^2} \right|}_{\psi=\widehat\psi} \, \text{VAR}\left[{\widehat{\psi_i}}\right] + 2 \sum_{i=1}^{p-1} \sum_{j=i+1}^{p} {\left. \frac{\partial f}{\partial \psi_i}\right|}_{\psi=\widehat\psi} \, {\left. \frac{\partial f}{\partial \psi_j}\right|}_{\psi=\widehat\psi} \, \text{COV}\left[{\widehat{\psi_i},\widehat{\psi_j}}\right]
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="regresion-de-componentes-principales" class="section level2">
<h2>Regresión de componentes principales</h2>
<p><br></p>
<p>Los componentes principales son</p>
<p><span class="math display">\[
\mathbf{W} = \mathbf{X} \mathbf{V}
\]</span></p>
<p>Cada componente se define como la proyección de <span class="math inline">\(\mathbf{X}\)</span> en los autovectores.</p>
<p>El modelo de regresión con <span class="math inline">\(k\)</span> componentes es</p>
<p><span class="math display">\[
Y = \gamma_0 + \gamma_1 W_1 + \ldots + \gamma_k W_k + \eta
\]</span></p>
<p><span class="math inline">\(\eta = \epsilon\)</span> si <span class="math inline">\(k = p\)</span>.</p>
<p><br></p>
</div>
<div id="regresion-ridge" class="section level2">
<h2>Regresión <em>Ridge</em></h2>
<p><br></p>
<p>Se trata de penalizar la longitud del vector de coeficientes <span class="math inline">\((\| \boldsymbol{\beta} \|)\)</span>. En <em>OLS</em> optimizamos la ecuación</p>
<p><span class="math display">\[
\frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
\]</span></p>
<p>Sin embargo, ahora optimizamos</p>
<p><span class="math display">\[
\frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \| \boldsymbol{\beta} \|^2 = \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \boldsymbol{\beta}^\prime \boldsymbol{\beta}
\]</span></p>
<p>donde <span class="math inline">\(\lambda &gt; 0\)</span> regula el <em>trade-off</em> entre el <em>MSE</em> y <span class="math inline">\(\| \boldsymbol{\beta} \|\)</span>.</p>
<p>Esta regresión se emplea tras centrar predictores y variable dependiente, para que <span class="math inline">\(\beta_0 = 0\)</span> y así no se penalice el tamaño de la intersección.</p>
<p>Si los predictores tienen diferente escala, entonces conviene estandarizarlos para que la penalización por <span class="math inline">\(\| \boldsymbol{\beta} \|^2\)</span> tenga sentido.</p>
<p><span class="math display">\[
\begin{align}
\nabla_\boldsymbol{\beta} \, \frac{1}{n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\prime (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) - \frac{\lambda}{n} \boldsymbol{\beta}^\prime \boldsymbol{\beta} &amp;= \frac{2}{n} \left(-\mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} + \lambda \boldsymbol{\beta} \right) \\\\
\frac{2}{n} \left(-\mathbf{X}^\prime \mathbf{y} + \mathbf{X}^\prime \mathbf{X} \boldsymbol{\widehat\beta} + \lambda \boldsymbol{\widehat\beta} \right) &amp;= 0 \\\\
\mathbf{X}^\prime \mathbf{y} &amp;= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right) \boldsymbol{\widehat\beta} \\\\
\boldsymbol{\widehat\beta} &amp;= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{y}
\end{align}
\]</span></p>
<p>La diferencia entre el estimador <em>OLS</em> y el anterior es que se le añade <span class="math inline">\(\lambda\)</span> a la diagonal de <span class="math inline">\(\mathbf{X}^\prime \mathbf{X}\)</span> (este es el <em>ridge</em>). Como consecuencia, se minimizan posibles problemas por colinealidad y la varianza de los coeficientes en <span class="math inline">\(\boldsymbol{\widehat\beta}\)</span> es menor.</p>
<p>Esta menor varianza implica sesgo (<em>bias-variance trade-off</em>)</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}\left[ \boldsymbol{\widehat\beta} \right] &amp;= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbb{E}\left[ \mathbf{y} \right] \\\\
&amp;= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{X} \boldsymbol{\beta} \\\\
\text{VAR}\left[ \boldsymbol{\widehat\beta} \right] &amp;= \text{VAR}\left[\left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{y} \right] \\\\
&amp;= \text{VAR}\left[\left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{\epsilon} \right] \\\\
&amp;= \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \, \sigma^2 \mathbf{I} \, \mathbf{X} \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \\\\
&amp;= \sigma^2 \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}^\prime \mathbf{X} \left( \mathbf{X}^\prime \mathbf{X} + \lambda \mathbf{I} \right)^{-1}
\end{align}
\]</span></p>
<p>Si <span class="math inline">\(\lambda\)</span> no es fijado sino estimado, entonces tendríamos que preocuparnos de su distribución.</p>
<p>Una de las principales ventajas de esta regresión es que permite comprobar con gran precisión la contribución de los predictores sin necesidad de preocuparse de qué variables prescindir.</p>
<p><br></p>
</div>
<div id="regresion-lasso" class="section level2">
<h2>Regresión <em>Lasso</em></h2>
<p><br></p>
<p>Esta regresión sustituye la penalización de <span class="math inline">\(\| \boldsymbol{\beta} \|^2\)</span> por otra medida de la longitud del vector:</p>
<p><span class="math display">\[
\| \boldsymbol{\beta} \|_q = \left( \sum_{i=1}^p b_i^q \right)^{1/q}
\]</span></p>
<p>Si <span class="math inline">\(q = 2\)</span>, entonces la penalización es la misma que en la regresión <em>ridge</em>. La regresión <em>lasso</em> (<em>least angle selection and shrinkage operator</em>) utiliza la penalización <span class="math inline">\(q = 1\)</span>.</p>
<p><br></p>
</div>
<div id="correccion-de-bonferroni" class="section level2">
<h2>Corrección de Bonferroni</h2>
<p><br></p>
<p>Supongamos que estamos interesados en que el error tipo I de <span class="math inline">\(k\)</span> coeficientes sea, conjuntamente, <span class="math inline">\(1 - \alpha\)</span>. Una forma común de lograrlo es establecer el error tipo I de cada coeficiente en <span class="math inline">\(\alpha / k\)</span> (o, equivalentemente, multiplicar el valor p por <span class="math inline">\(k\)</span>). Sin embargo, esto solo es exacto si los coeficientes son independientes.</p>
<p>En un contraste estándar, cada coeficiente <span class="math inline">\(\beta_i\)</span> posee un conjunto de confianza <span class="math inline">\(C_i(\alpha)\)</span> de manera que</p>
<p><span class="math display">\[
\begin{align}
P(\beta_i \in C_i(\alpha)) &amp;= 1 - \alpha \\\\
P(\beta_i \notin C_i(\alpha)) &amp;= \alpha \\\\
P\left( \bigcup_i^k \beta_i \notin C_i(\alpha) \right) &amp;= \sum_{i}^k P(\beta_i \notin C_i(\alpha)) - P(\text{Conjunto de intersecciones en } \beta) \\\\
&amp;= k \alpha - P(\text{Conjunto de intersecciones en } \beta) \\\\
&amp;\leq k \alpha \\\\
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
1 - P\left( \bigcup_i^k \beta_i \notin C_i(\alpha) \right) &amp;\leq 1 - k \alpha \\\\
1 - P\left( \bigcup_i^k \beta_i \notin C_i(\alpha / k) \right) &amp;\leq 1 - \alpha
\end{align}
\]</span></p>
<p>Por tanto, tras dividir por el número de contrastes <span class="math inline">\(k\)</span>, la probabilidad de que, al menos, un solo parámetro no se encuentre en su intervalo de confianza es igual o inferior a <span class="math inline">\(\alpha\)</span>. Por esta razón, la correción de Bonferroni es conservadora.</p>
<p>Esta corrección tan solo exige que</p>
<p><span class="math display">\[
C(\alpha) = \prod_{i}^k C(\alpha_i / k)
\]</span></p>
<p>o equivalentemente,</p>
<p><span class="math display">\[
\alpha = \sum_{i}^k \alpha_i/k
\]</span></p>
<p>Por tanto, es legítimo asumir distintos niveles de error tipo I <span class="math inline">\((\alpha/k)\)</span> en cada contraste siempre que la suma de ellos equivalga a <span class="math inline">\(\alpha\)</span>.</p>
<p><br></p>
</div>
<div id="elipsoides-de-confianza" class="section level2">
<h2>Elipsoides de confianza</h2>
<p><br></p>
<p>Si nuestros coeficientes siguen una dsitribución normal y son independientes unos de otros,</p>
<p><span class="math display">\[
\begin{align}
\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]} \sim N(0, 1) \\\\
\sum_{i=1}^k \left(\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]}\right)^2 \sim \chi^2_k \\\\
\end{align}
\]</span></p>
<p>El intervalo <span class="math inline">\(1 - \alpha\)</span> de confianza vendría dado por</p>
<p><br></p>
<p><span class="math display">\[
P\left(\sum_{i=1}^k \left(\frac{\widehat\beta_i - \beta_i}{\text{se}\left[\beta_i\right]}\right)^2 \leq \;\;  \chi^2_{k, 1-\alpha} \right) = 1 - \alpha
\]</span></p>
<p>Esta región de confianza es un elipsoide.</p>
<p>Sin embargo, si los coeficientes covarían y la matriz de varianzas-covarianzas de los coeficientes es <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, entonces</p>
<p><br></p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\Sigma} &amp;= \mathbf{VUV}^\prime \\\\
\boldsymbol{\Sigma}^{1/2} &amp;= \mathbf{VU}^{1/2} \mathbf{V}^\prime \\\\
\text{VAR}\left[ \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \right] &amp;= 
\boldsymbol{\Sigma}^{-1/2} \, \text{VAR}\left[ \boldsymbol{\widehat\beta - \boldsymbol{\beta}} \right] \, \left(\boldsymbol{\Sigma}^{-1/2}\right)^\prime \\\\
&amp;= \left(\mathbf{VU}^{1/2} \mathbf{V}^\prime\right)^{-1} \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\left(\mathbf{VU}^{1/2} \mathbf{V}^\prime\right)^{-1}\right)^\prime \\\\
&amp;= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\mathbf{VU}^{-1/2} \mathbf{V}^\prime\right)^\prime \\\\
&amp;= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \left(\mathbf{VU}^{-1/2} \mathbf{V}^\prime\right)^\prime \\\\
&amp;= \mathbf{VU}^{-1/2} \mathbf{V}^\prime \mathbf{V} \mathbf{U} \mathbf{V}^\prime \mathbf{VU}^{-1/2} \mathbf{V}^\prime \\\\
&amp;= \mathbf{VU}^{-1/2} \mathbf{UU}^{-1/2} \mathbf{V}^\prime \\\\
&amp;= \mathbf{VV}^\prime \\\\
&amp;= \mathbf{I}
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &amp;\sim MVN(\mathbf{0}, \mathbf{I}) \\\\
\left(\boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})\right)^\prime \; \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &amp;\sim \chi^2_k \\\\
(\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \; \left(\boldsymbol{\Sigma}^{-1/2} \right)^\prime \; \boldsymbol{\Sigma}^{-1/2} (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &amp;\sim \chi^2_k \\\\
(\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) &amp;\sim \chi^2_k
\end{align}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
P\left( (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \, \leq \,  \chi^2_{k, 1-\alpha} \right) = 1 - \alpha
\]</span></p>
<p>Si se utiliza <span class="math inline">\(\widehat\sigma^2\)</span> para estimar el error típico, entonces debemos emplear la distribución <span class="math inline">\(F\)</span>:</p>
<p><span class="math display">\[
P\left( (\boldsymbol{\widehat\beta - \boldsymbol{\beta}})^\prime \, \boldsymbol{\Sigma}^{-1} \, (\boldsymbol{\widehat\beta - \boldsymbol{\beta}}) \, \leq \,  F_{q, \, n-p-1, (1-\alpha)} \right) = 1 - \alpha
\]</span></p>
<p><br></p>
</div>
<div id="interacciones" class="section level2">
<h2>Interacciones</h2>
<p><br></p>
<p>Si la verdadera función de regresión es <span class="math inline">\(\mathbb{E}\left[Y \, | \, X_1, X_2\right] = \mu(X_1, X_2)\)</span>, que incluye una interacción de forma desconocida entre las variables <span class="math inline">\(X_1\)</span> y <span class="math inline">\(X_2\)</span>, ¿por qué representar la interacción entre <span class="math inline">\(X_1\)</span> y <span class="math inline">\(X_2\)</span> como <span class="math inline">\(X_1X_2\)</span>?</p>
<p><span class="math display">\[
Y = \beta_0 + X_1 \beta_1 + X_2 \beta_2 + X_1X_2\beta_3 + \epsilon
\]</span></p>
<p>Si hiciéramos una expansión de Taylor,</p>
<p><span class="math display">\[
\mu(X_1, X_2) \approx \mu(X_1^*, X_2^*) + \sum_{i=1}^p (x_i - x_i^*) \, \left. \frac{\partial{\mu}}{\partial x_i} \right|_{x_i = x_i^*} + \sum_{j=1}^p (x_j - x_j^*) \, \left. \frac{\partial{\mu}}{\partial x_j} \right|_{x_j = x_j^*} + \frac{1}{2} \sum_{i=1}^p \sum_{j=1}^p (x_i - x_i^*) (x_j - x_j^*) \left. \frac{\partial^2 \mu}{\partial x_i x_j} \right|_{x_ix_j = x_i^*x_j^*}
\]</span></p>
<p>entonces podemos comprobar que una buena aproximación incluye el producto entre las variables <span class="math inline">\(X_1\)</span> y <span class="math inline">\(X_2\)</span> así como sus términos cuadráticos.</p>
<p><br></p>
</div>
<div id="distribucion-de-los-valores-p-bajo-la-hipotesis-nula" class="section level2">
<h2>Distribución de los valores p bajo la hipótesis nula</h2>
<p><br></p>
<p><span class="math display">\[
\begin{align}
P &amp;= F(T) \\\\
\text{Pr}(P &lt; p) &amp;= \text{Pr}\left(F^{-1}(P) &lt; F^{-1}(p)\right) \\\\
&amp;= \text{Pr}(T &lt; t) \\\\
&amp;= F\left(F^{-1}(p)\right) \\\\
&amp;= p
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="desigualdad-de-chebyshev" class="section level2">
<h2>Desigualdad de Chebyshev</h2>
<p><br></p>
<p><span class="math display">\[
\text{P}(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}
\]</span></p>
<p>La probabilidad de que una variable aleatoria <span class="math inline">\(X\)</span> exceda su valor esperado <span class="math inline">\(\mu\)</span> por <span class="math inline">\(k\)</span> errores típicos es siempre inferior a <span class="math inline">\(\frac{1}{k^2}\)</span>.</p>
<p><br></p>
</div>
<div id="capture-percentage" class="section level2">
<h2><em>Capture percentage</em></h2>
<p><br></p>
<p>La esperanza de cobertura del intervalo de confianza para la distribución de <span class="math inline">\(\widehat\beta_0\)</span> o lo que es lo mismo, el promedio de veces que un intervalo de confianza contiene otras estimaciones de <span class="math inline">\(\beta_0\)</span>, es:</p>
<p><br></p>
<p><span class="math display">\[
\int \phi\left(\widehat\beta_0; \, \beta_0, \text{se}\right) \left(\Phi\left(\widehat\beta_0 + 1.96 \cdot \text{se}; \, \beta_0, \text{se}\right) - \Phi\left(\widehat\beta_0 - 1.96 \cdot \text{se}; \, \beta_0, \text{se}\right)\right) d\widehat\beta_0
\]</span></p>
<p><br></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
