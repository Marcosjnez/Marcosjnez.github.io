% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{Andrew,
  Title                    = {Philosophy and the practice of Bayesian statistics},
  Author                   = {Gelman Andrew and Shalizi Cosma Rohilla},
  Journal                  = {British Journal of Mathematical and Statistical Psychology},
  Number                   = {1},
  Pages                    = {8-38},
  Volume                   = {66},
  Abstract                 = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico‐deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
  Doi                      = {10.1111/j.2044-8317.2011.02037.x},
  Eprint                   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.2011.02037.x},
  Owner                    = {marcos},
  Timestamp                = {2018.05.18},
  Url                      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.2011.02037.x}
}

@Manual{Auguie2017,
  Title                    = {gridExtra: Miscellaneous Functions for "Grid" Graphics},
  Author                   = {Baptiste Auguie},
  Note                     = {R package version 2.3},
  Year                     = {2017},

  Url                      = {https://CRAN.R-project.org/package=gridExtra}
}

@Manual{R-citr,
  Title                    = {citr: 'RStudio' Add-in to Insert Markdown Citations},
  Author                   = {Frederik Aust},
  Note                     = {R package version 0.2.0.9055},
  Year                     = {2017},

  Url                      = {https://github.com/crsh/citr}
}

@Manual{R-papaja,
  Title                    = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  Author                   = {Frederik Aust and Marius Barth},
  Note                     = {R package version 0.1.0.9655},
  Year                     = {2017},

  Url                      = {https://github.com/crsh/papaja}
}

@Article{Banks2016,
  Title                    = {Editorial: Evidence on Questionable Research Practices: The Good, the Bad, and the Ugly},
  Author                   = {Banks, George C.
and Rogelberg, Steven G.
and Woznyj, Haley M.
and Landis, Ronald S.
and Rupp, Deborah E.},
  Journal                  = {Journal of Business and Psychology},
  Year                     = {2016},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {323--338},
  Volume                   = {31},

  Abstract                 = {Questionable research or reporting practices (QRPs) contribute to a growing concern regarding the credibility of research in the organizational sciences and related fields. Such practices include design, analytic, or reporting practices that may introduce biased evidence, which can have harmful implications for evidence-based practice, theory development, and perceptions of the rigor of science.},
  Day                      = {01},
  Doi                      = {10.1007/s10869-016-9456-7},
  ISSN                     = {1573-353X},
  Owner                    = {marcos},
  Timestamp                = {2018.04.26},
  Url                      = {https://doi.org/10.1007/s10869-016-9456-7}
}

@Article{Barberia2018,
  Title                    = {A short educational intervention diminishes causal illusions and specific paranormal beliefs in undergraduates},
  Author                   = {Barberia, Itxaso AND Tubau, Elisabet AND Matute, Helena AND Rodríguez-Ferreiro, Javier},
  Journal                  = {PLOS ONE},
  Year                     = {2018},

  Month                    = {01},
  Number                   = {1},
  Pages                    = {1-14},
  Volume                   = {13},

  Abstract                 = {Cognitive biases such as causal illusions have been related to paranormal and pseudoscientific beliefs and, thus, pose a real threat to the development of adequate critical thinking abilities. We aimed to reduce causal illusions in undergraduates by means of an educational intervention combining training-in-bias and training-in-rules techniques. First, participants directly experienced situations that tend to induce the Barnum effect and the confirmation bias. Thereafter, these effects were explained and examples of their influence over everyday life were provided. Compared to a control group, participants who received the intervention showed diminished causal illusions in a contingency learning task and a decrease in the precognition dimension of a paranormal belief scale. Overall, results suggest that evidence-based educational interventions like the one presented here could be used to significantly improve critical thinking skills in our students.},
  Doi                      = {10.1371/journal.pone.0191907},
  Owner                    = {marcos},
  Publisher                = {Public Library of Science},
  Timestamp                = {2018.03.13},
  Url                      = {https://doi.org/10.1371/journal.pone.0191907}
}

@Manual{Barrett2018,
  Title                    = {ggdag: Analyze and Create Elegant Directed Acyclic Graphs},
  Author                   = {Malcolm Barrett},
  Note                     = {R package version 0.1.1.9000},
  Year                     = {2018},

  Url                      = {https://github.com/malcolmbarrett/ggdag}
}

@Manual{Chang2017,
  Title                    = {shiny: Web Application Framework for R},
  Author                   = {Winston Chang and Joe Cheng and JJ Allaire and Yihui Xie and Jonathan McPherson},
  Note                     = {R package version 1.0.5},
  Year                     = {2017},

  Url                      = {https://CRAN.R-project.org/package=shiny}
}

@Manual{R-shiny,
  Title                    = {shiny: Web Application Framework for R},
  Author                   = {Winston Chang and Joe Cheng and JJ Allaire and Yihui Xie and Jonathan McPherson},
  Note                     = {R package version 1.0.5},
  Year                     = {2017},

  Url                      = {https://CRAN.R-project.org/package=shiny}
}

@Manual{Gabry2017,
  Title                    = {bayesplot: Plotting for Bayesian Models},
  Author                   = {Jonah Gabry and Tristan Mahr},
  Note                     = {R package version 1.4.0},
  Year                     = {2017},

  Url                      = {https://CRAN.R-project.org/package=bayesplot}
}

@Article{Greenland2016,
  Title                    = {Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations},
  Author                   = {Greenland, Sander
and Senn, Stephen J.
and Rothman, Kenneth J.
and Carlin, John B.
and Poole, Charles
and Goodman, Steven N.
and Altman, Douglas G.},
  Journal                  = {European Journal of Epidemiology},
  Year                     = {2016},

  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {337--350},
  Volume                   = {31},

  Abstract                 = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so---and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  Day                      = {01},
  Doi                      = {10.1007/s10654-016-0149-3},
  ISSN                     = {1573-7284},
  Owner                    = {marcos},
  Timestamp                = {2018.04.23},
  Url                      = {https://doi.org/10.1007/s10654-016-0149-3}
}

@Manual{Gronau2017,
  Title                    = {bridgesampling: Bridge Sampling for Marginal Likelihoods and Bayes Factors},
  Author                   = {Quentin F. Gronau and Henrik Singmann},
  Note                     = {R package version 0.4-0},
  Year                     = {2017},

  Url                      = {https://CRAN.R-project.org/package=bridgesampling}
}

@Other{Imai2010,
  Title                    = {A general approach to causal mediation analysis.},
  Abstract                 = {Traditionally in the social sciences, causal mediation analysis has been formulated, understood, and implemented within the framework of linear structural equation models. We argue and demonstrate that this is problematic for 3 reasons: the lack of a general definition of causal mediation effects independent of a particular statistical model, the inability to specify the key identification assumption, and the difficulty of extending the framework to nonlinear models. In this article, we propose an alternative approach that overcomes these limitations. Our approach is general because it offers the definition, identification, estimation, and sensitivity analysis of causal mediation effects without reference to any specific statistical model. Further, our approach explicitly links these 4 elements closely together within a single framework. As a result, the proposed framework can accommodate linear and nonlinear relationships, parametric and nonparametric models, continuous and discrete mediators, and various types of outcome variables. The general definition and identification result also allow us to develop sensitivity analysis in the context of commonly used models, which enables applied researchers to formally assess the robustness of their empirical conclusions to violations of the key assumption. We illustrate our approach by applying it to the Job Search Intervention Study. We also offer easy-to-use software that implements all our proposed methods. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  Address                  = {Imai, Kosuke: Department of Politics, Princeton University, Princeton, NJ, US, 08544, kimai@princeton.edu},
  Author                   = {Imai, Kosuke and Keele, Luke and Tingley, Dustin},
  Doi                      = {10.1037/a0020761},
  ISSN                     = {1939-1463(Electronic),1082-989X(Print)},
  Journal                  = {Psychological Methods},
  Keywords                 = {*Causal Analysis, *Statistical Measurement, Structural Equation Modeling},
  Number                   = {4},
  Owner                    = {marcos},
  Pages                    = {309--334},
  Publisher                = {American Psychological Association},
  Refid                    = {2010-21388-001},
  Timestamp                = {2018.04.19},
  Volume                   = {15},
  Year                     = {2010}
}

@Article{Lakens2017,
  Title                    = {Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses},
  Author                   = {Daniël Lakens},
  Journal                  = {Social Psychological and Personality Science},
  Year                     = {2017},
  Note                     = {PMID: 28736600},
  Number                   = {4},
  Pages                    = {355-362},
  Volume                   = {8},

  Abstract                 = { Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences. },
  Doi                      = {10.1177/1948550617697177},
  Eprint                   = { 
 https://doi.org/10.1177/1948550617697177
 
},
  Owner                    = {marcos},
  Timestamp                = {2018.04.04},
  Url                      = { 
 https://doi.org/10.1177/1948550617697177
 
}
}

@Article{Lindsay2015,
  Title                    = {Replication in Psychological Science},
  Author                   = {Lindsay, D. Stephen},
  Journal                  = {Psychol Sci},
  Year                     = {2015},

  Month                    = nov,
  Number                   = {12},
  Pages                    = {1827--1832},
  Volume                   = {26},

  Booktitle                = {Psychological Science},
  Comment                  = {doi: 10.1177/0956797615616374},
  Doi                      = {10.1177/0956797615616374},
  ISSN                     = {0956-7976},
  Owner                    = {marcos},
  Publisher                = {SAGE Publications Inc},
  Timestamp                = {2018.03.14},
  Url                      = {https://doi.org/10.1177/0956797615616374}
}

@InBook{Mayo2006a,
  Title                    = {Frequentist statistics as a theory of inductive inference},
  Author                   = {Mayo, Deborah G. and Cox, D. R.},
  Editor                   = {Rojo, Javier},
  Pages                    = {77--97},
  Publisher                = {Institute of Mathematical Statistics},
  Year                     = {2006},

  Address                  = {Beachwood, Ohio, USA},
  Series                   = {Lecture Notes--Monograph Series},
  Volume                   = {Number 49},

  Booktitle                = {Optimality},
  Doi                      = {10.1214/074921706000000400},
  Url                      = {https://doi.org/10.1214/074921706000000400}
}

@InCollection{Mayo2011,
  Title                    = {Error Statistics },
  Author                   = {Deborah G. Mayo and Aris Spanos},
  Booktitle                = {Philosophy of Statistics },
  Publisher                = {North-Holland},
  Year                     = {2011},

  Address                  = {Amsterdam},
  Editor                   = {Bandyopadhyay, Prasanta S. and Forster, Malcolm R. },
  Pages                    = {153 - 198},
  Series                   = {Handbook of the Philosophy of Science},
  Volume                   = {7},

  Doi                      = {https://doi.org/10.1016/B978-0-444-51862-0.50005-8},
  ISSN                     = {18789846},
  Owner                    = {marcos},
  Timestamp                = {2018.04.23},
  Url                      = {https://www.sciencedirect.com/science/article/pii/B9780444518620500058}
}

@Article{Mayo2006,
  Title                    = {Severe Testing as a Basic Concept in a Neyman–Pearson Philosophy of Induction},
  Author                   = {Mayo, Deborah G. and Spanos, Aris},
  Journal                  = {The British Journal for the Philosophy of Science},
  Year                     = {2006},
  Number                   = {2},
  Pages                    = {323-357},
  Volume                   = {57},

  Doi                      = {10.1093/bjps/axl003},
  Eprint                   = {/oup/backfile/content_public/journal/bjps/57/2/10.1093/bjps/axl003/2/axl003.pdf},
  Owner                    = {marcos},
  Timestamp                = {2018.04.26},
  Url                      = {http://dx.doi.org/10.1093/bjps/axl003}
}

@Manual{Meschiari2015,
  Title                    = {latex2exp: Use LaTeX Expressions in Plots},
  Author                   = {Stefano Meschiari},
  Note                     = {R package version 0.4.0},
  Year                     = {2015},

  Url                      = {https://CRAN.R-project.org/package=latex2exp}
}

@Article{Morey2016,
  Title                    = {The philosophy of Bayes factors and the quantification of statistical evidence},
  Author                   = {Richard D. Morey and Jan-Willem Romeijn and Jeffrey N. Rouder},
  Journal                  = {Journal of Mathematical Psychology},
  Year                     = {2016},
  Note                     = {Bayes Factors for Testing Hypotheses in Psychological Research: Practical Relevance and New Developments},
  Pages                    = {6 - 18},
  Volume                   = {72},

  Doi                      = {https://doi.org/10.1016/j.jmp.2015.11.001},
  ISSN                     = {0022-2496},
  Keywords                 = {Bayes factor, Hypothesis testing},
  Owner                    = {marcos},
  Timestamp                = {2018.05.06},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0022249615000723}
}

@Article{Morris2017,
  Title                    = {Scale Imposition as Quantitative Alchemy: Studies on the Transitivity of Neuroticism Ratings},
  Author                   = {Stefanie Dorough Morris and James W. Grice and Ryan A. Cox},
  Journal                  = {Basic and Applied Social Psychology},
  Year                     = {2017},
  Number                   = {1},
  Pages                    = {1-18},
  Volume                   = {39},

  Doi                      = {10.1080/01973533.2016.1256288},
  Eprint                   = { 
 https://doi.org/10.1080/01973533.2016.1256288
 
},
  Owner                    = {marcos},
  Publisher                = {Routledge},
  Timestamp                = {2018.04.03},
  Url                      = { 
 https://doi.org/10.1080/01973533.2016.1256288
 
}
}

@Article{Munafo2017,
  Title                    = {A manifesto for reproducible science},
  Author                   = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  Journal                  = {Nature Human Behaviour},
  Year                     = {2017},

  Month                    = jan,
  Pages                    = {0021--},
  Volume                   = {1},

  Owner                    = {marcos},
  Publisher                = {Macmillan Publishers Limited},
  Timestamp                = {2018.03.13},
  Url                      = {http://dx.doi.org/10.1038/s41562-016-0021}
}

@Article{NP1933,
  Title                    = {IX. On the problem of the most efficient tests of statistical hypotheses},
  Author                   = {J. Neyman and E. S. Pearson},
  Journal                  = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  Year                     = {1933},
  Number                   = {694-706},
  Pages                    = {289--337},
  Volume                   = {231},

  Doi                      = {10.1098/rsta.1933.0009},
  Eprint                   = {http://rsta.royalsocietypublishing.org/content/231/694-706/289.full.pdf},
  ISSN                     = {0264-3952},
  Owner                    = {marcos},
  Publisher                = {The Royal Society},
  Timestamp                = {2018.04.03},
  Url                      = {http://rsta.royalsocietypublishing.org/content/231/694-706/289}
}

@Article{Nuijten2015,
  Title                    = {A default Bayesian hypothesis test for mediation},
  Author                   = {Nuijten, Mich{\`e}le B.
and Wetzels, Ruud
and Matzke, Dora
and Dolan, Conor V.
and Wagenmakers, Eric-Jan},
  Journal                  = {Behavior Research Methods},
  Year                     = {2015},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {85--97},
  Volume                   = {47},

  Abstract                 = {In order to quantify the relationship between multiple variables, researchers often carry out a mediation analysis. In such an analysis, a mediator (e.g., knowledge of a healthy diet) transmits the effect from an independent variable (e.g., classroom instruction on a healthy diet) to a dependent variable (e.g., consumption of fruits and vegetables). Almost all mediation analyses in psychology use frequentist estimation and hypothesis-testing techniques. A recent exception is Yuan and MacKinnon (Psychological Methods, 14, 301--322, 2009), who outlined a Bayesian parameter estimation procedure for mediation analysis. Here we complete the Bayesian alternative to frequentist mediation analysis by specifying a default Bayesian hypothesis test based on the Jeffreys--Zellner--Siow approach. We further extend this default Bayesian test by allowing a comparison to directional or one-sided alternatives, using Markov chain Monte Carlo techniques implemented in JAGS. All Bayesian tests are implemented in the R package BayesMed (Nuijten, Wetzels, Matzke, Dolan, {\&} Wagenmakers, 2014).},
  Day                      = {01},
  Doi                      = {10.3758/s13428-014-0470-2},
  ISSN                     = {1554-3528},
  Owner                    = {marcos},
  Timestamp                = {2018.05.06},
  Url                      = {https://doi.org/10.3758/s13428-014-0470-2}
}

@Article{OSC2015,
  Title                    = {Estimating the reproducibility of psychological science},
  Author                   = {{Open Science Collaboration}},
  Journal                  = {Science},
  Year                     = {2015},
  Number                   = {6251},
  Volume                   = {349},

  Abstract                 = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that {\textquotedblleft}we already know this{\textquotedblright} belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  Doi                      = {10.1126/science.aac4716},
  Eprint                   = {http://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
  ISSN                     = {0036-8075},
  Owner                    = {marcos},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2018.03.13},
  Url                      = {http://science.sciencemag.org/content/349/6251/aac4716}
}

@Other{Pearl2014,
  Title                    = {Interpretation and identification of causal mediation.},
  Abstract                 = {This article reviews the foundations of causal mediation analysis and offers a general and transparent account of the conditions necessary for the identification of natural direct and indirect effects, thus facilitating a more informed judgment of the plausibility of these conditions in specific applications. I show that the conditions usually cited in the literature are overly restrictive and can be relaxed substantially without compromising identification. In particular, I show that natural effects can be identified by methods that go beyond standard adjustment for confounders, applicable to observational studies in which treatment assignment remains confounded with the mediator or with the outcome. These identification conditions can be validated algorithmically from the diagrammatic description of one’s model and are guaranteed to produce unbiased results whenever the description is correct. The identification conditions can be further relaxed in parametric models, possibly including interactions, and permit one to compare the relative importance of several pathways, mediated by interdependent variables. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  Address                  = {Pearl, Judea: Computer Science Department, University of California Los Angeles, Los Angeles, CA, US, 90095-1596, judea@cs.ucla.edu},
  Author                   = {Pearl, Judea},
  Doi                      = {10.1037/a0036434},
  ISSN                     = {1939-1463(Electronic),1082-989X(Print)},
  Journal                  = {Psychological Methods},
  Keywords                 = {*Causality, *Mediation, Models},
  Number                   = {4},
  Owner                    = {marcos},
  Pages                    = {459--481},
  Publisher                = {American Psychological Association},
  Refid                    = {2014-21922-001},
  Timestamp                = {2018.04.19},
  Volume                   = {19},
  Year                     = {2014}
}

@Manual{Pedersen2017,
  Title                    = {patchwork: The Composer of ggplots},
  Author                   = {Thomas Lin Pedersen},
  Note                     = {R package version 0.0.1},
  Year                     = {2017},

  Url                      = {https://github.com/thomasp85/patchwork}
}

@Article{Peterson1990,
  Title                    = {Partial Proportional Odds Models for Ordinal Response Variables},
  Author                   = {Peterson, Bercedis and Harrell, Frank E.},
  Year                     = {1990},
  Number                   = {2},
  Pages                    = {205--217},
  Volume                   = {39},

  Abstract                 = {The ordinal logistic regression model that McCullagh calls the proportional odds model is extended to models that allow non-proportional odds for a subset of the explanatory variables. The maximum likelihood method is used for estimation of parameters of general and restricted partial proportional odds models as well as for the derivation of Wald, Rao score and likelihood ratio tests. These tests assess association without assuming proportional odds and test proportional odds against various alternatives. Simulation results compare the score test for proportional odds with tests suggested by Koch, Amara and Singer that are based on a series of binary logistic models.},
  Booktitle                = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  ISSN                     = {00359254, 14679876},
  Owner                    = {marcos},
  Publisher                = {[Wiley, Royal Statistical Society]},
  Timestamp                = {2018.04.26},
  Url                      = {http://www.jstor.org/stable/2347760}
}

@Manual{RCT2018,
  Title                    = {R: A Language and Environment for Statistical Computing},

  Address                  = {Vienna, Austria},
  Author                   = {{R Core Team}},
  Organization             = {R Foundation for Statistical Computing},
  Year                     = {2018},

  Url                      = {https://www.R-project.org/}
}

@Manual{R-base,
  Title                    = {R: A Language and Environment for Statistical Computing},

  Address                  = {Vienna, Austria},
  Author                   = {{R Core Team}},
  Organization             = {R Foundation for Statistical Computing},
  Year                     = {2017},

  Url                      = {https://www.R-project.org/}
}

@Article{Rouder2014,
  Title                    = {Optional stopping: No problem for Bayesians},
  Author                   = {Rouder, Jeffrey N.},
  Journal                  = {Psychonomic Bulletin {\&} Review},
  Year                     = {2014},

  Month                    = {Apr},
  Number                   = {2},
  Pages                    = {301--308},
  Volume                   = {21},

  __markedentry            = {[marcos:]},
  Abstract                 = {Optional stopping refers to the practice of peeking at data and then, based on the results, deciding whether or not to continue an experiment. In the context of ordinary significance-testing analysis, optional stopping is discouraged, because it necessarily leads to increased type I error rates over nominal values. This article addresses whether optional stopping is problematic for Bayesian inference with Bayes factors. Statisticians who developed Bayesian methods thought not, but this wisdom has been challenged by recent simulation results of Yu, Sprenger, Thomas, and Dougherty (2013) and Sanborn and Hills (2013). In this article, I show through simulation that the interpretation of Bayesian quantities does not depend on the stopping rule. Researchers using Bayesian methods may employ optional stopping in their own research and may provide Bayesian analysis of secondary data regardless of the employed stopping rule. I emphasize here the proper interpretation of Bayesian quantities as measures of subjective belief on theoretical positions, the difference between frequentist and Bayesian interpretations, and the difficulty of using frequentist intuition to conceptualize the Bayesian approach.},
  Day                      = {01},
  Doi                      = {10.3758/s13423-014-0595-4},
  ISSN                     = {1531-5320},
  Owner                    = {marcos},
  Timestamp                = {2018.05.18},
  Url                      = {https://doi.org/10.3758/s13423-014-0595-4}
}

@Article{Senn2002,
  Title                    = {A comment on replication, p-values and evidence S.N.Goodman, Statistics in Medicine 1992; 11:875-879},
  Author                   = {Senn, Stephen},
  Journal                  = {Statistics in Medicine},
  Year                     = {2002},
  Number                   = {16},
  Pages                    = {2437--2444},
  Volume                   = {21},

  Doi                      = {10.1002/sim.1072},
  ISSN                     = {1097-0258},
  Owner                    = {marcos},
  Publisher                = {John Wiley \& Sons, Ltd.},
  Timestamp                = {2018.03.13},
  Url                      = {http://dx.doi.org/10.1002/sim.1072}
}

@Misc{SDT2018,
  Title                    = {{RStan}: the {R} interface to {Stan}},

  Author                   = {{Stan Development Team}},
  Note                     = {R package version 2.17.3},
  Year                     = {2018},

  Url                      = {http://mc-stan.org/}
}

@Misc{SDT2017,
  Title                    = {shinystan: Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models.},

  Author                   = {{Stan Development Team}},
  Note                     = {R package version 2.4.0},
  Year                     = {2017},

  Url                      = {http://mc-stan.org/}
}

@Article{Sterling1959,
  Title                    = {Publication Decisions and Their Possible Effects on Inferences Drawn from Tests of Significance--Or Vice Versa},
  Author                   = {Theodore D. Sterling},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1959},
  Number                   = {285},
  Pages                    = {30--34},
  Volume                   = {54},

  Abstract                 = {There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs-an "error of the first kind"-and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance.},
  ISSN                     = {01621459},
  Owner                    = {marcos},
  Publisher                = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  Timestamp                = {2018.03.14},
  Url                      = {http://www.jstor.org/stable/2282137}
}

@Manual{Textor2016,
  Title                    = {dagitty: Graphical Analysis of Structural Causal Models},
  Author                   = {Johannes Textor and Benito {van der Zander}},
  Note                     = {R package version 0.2-2},
  Year                     = {2016},

  Url                      = {https://CRAN.R-project.org/package=dagitty}
}

@Article{Trafimow2015,
  Title                    = {Editorial},
  Author                   = { David Trafimow and Michael Marks },
  Journal                  = {Basic and Applied Social Psychology},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {1-2},
  Volume                   = {37},

  Doi                      = {10.1080/01973533.2015.1012991},
  Eprint                   = { 
 https://doi.org/10.1080/01973533.2015.1012991
 
},
  Owner                    = {marcos},
  Publisher                = {Routledge},
  Timestamp                = {2018.03.13},
  Url                      = { 
 https://doi.org/10.1080/01973533.2015.1012991
 
}
}

@Article{VanderWeele2016,
  Title                    = {Brief Report: Mediation Analysis with an Ordinal Outcome},
  Author                   = {VanderWeele, Tyler J. and Zhang, Yun and Lim, Pilar},
  Journal                  = {Epidemiology},
  Year                     = {2016},
  Number                   = {5},
  Pages                    = {--},
  Volume                   = {27},

  Abstract                 = {The article presents concepts and methods for mediation analysis for an ordinal outcome. We give definitions of natural direct and indirect effects using counterfactuals for ordinal outcomes; in this context, there are potentially different effects for any two levels of the outcome, and we consider difference and ratio scales. The confounding assumptions required for identification are similar to that in the existing mediation analysis literature. We discuss different modeling strategies for estimation. Under a proportional odds model with a reference category that is common, the direct and indirect effects on a ratio scale can each be summarized by a single estimate and are available in closed form; otherwise the effects may differ across categories compared and can be obtained by numeric simulation methods.},
  ISSN                     = {1044-3983},
  Owner                    = {marcos},
  Refid                    = {00001648-201609000-00008},
  Timestamp                = {2018.04.19},
  Url                      = {https://journals.lww.com/epidem/Fulltext/2016/09000/Brief_Report___Mediation_Analysis_with_an_Ordinal.8.aspx}
}

@Misc{Vehtari2018,
  Title                    = {loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models},

  Author                   = {Aki Vehtari and Jonah Gabry and Yuling Yao and Andrew Gelman},
  Note                     = {R package version 2.0.0},
  Year                     = {2018},

  Url                      = {https://CRAN.R-project.org/package=loo}
}

@Article{Wason1960,
  Title                    = {On the Failure to Eliminate Hypotheses in a Conceptual Task},
  Author                   = {Wason, P. C.},
  Journal                  = {Quarterly Journal of Experimental Psychology},
  Year                     = {1960},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {129--140},
  Volume                   = {12},

  Abstract                 = {This investigation examines the extent to which intelligent young adults seek (i) confirming evidence alone (enumerative induction) or (ii) confirming and discontinuing evidence (eliminative induction), in order to draw conclusions in a simple conceptual task. The experiment is designed so that use of confirming evidence alone will almost certainly lead to erroneous conclusions because (i) the correct concept is entailed by many more obvious ones, and (ii) the universe of possible instances (numbers) is infinite.Six out of 29 subjects reached the correct conclusion without previous incorrect ones, 13 reached one incorrect conclusion, nine reached two or more incorrect conclusions, and one reached no conclusion. The results showed that those subjects, who reached two or more incorrect conclusions, were unable, or unwilling to test their hypotheses. The implications are discussed in relation to scientific thinking.
This investigation examines the extent to which intelligent young adults seek (i) confirming evidence alone (enumerative induction) or (ii) confirming and discontinuing evidence (eliminative induction), in order to draw conclusions in a simple conceptual task. The experiment is designed so that use of confirming evidence alone will almost certainly lead to erroneous conclusions because (i) the correct concept is entailed by many more obvious ones, and (ii) the universe of possible instances (numbers) is infinite.Six out of 29 subjects reached the correct conclusion without previous incorrect ones, 13 reached one incorrect conclusion, nine reached two or more incorrect conclusions, and one reached no conclusion. The results showed that those subjects, who reached two or more incorrect conclusions, were unable, or unwilling to test their hypotheses. The implications are discussed in relation to scientific thinking.},
  Booktitle                = {Quarterly Journal of Experimental Psychology},
  Comment                  = {doi: 10.1080/17470216008416717},
  Doi                      = {10.1080/17470216008416717},
  ISSN                     = {0033-555X},
  Owner                    = {marcos},
  Publisher                = {SAGE Publications},
  Timestamp                = {2018.03.13},
  Url                      = {http://journals.sagepub.com/doi/abs/10.1080/17470216008416717}
}

@Article{Wetzels2009,
  Title                    = {How to quantify support for and against the null hypothesis: A flexible WinBUGS implementation of a default Bayesian t test},
  Author                   = {Wetzels, Ruud
and Raaijmakers, Jeroen G. W.
and Jakab, Em{\"o}ke
and Wagenmakers, Eric-Jan},
  Journal                  = {Psychonomic Bulletin {\&} Review},
  Year                     = {2009},

  Month                    = {Aug},
  Number                   = {4},
  Pages                    = {752--760},
  Volume                   = {16},

  __markedentry            = {[marcos:6]},
  Abstract                 = {We propose a sampling-based Bayesian t test that allows researchers to quantify the statistical evidence in favor of the null hypothesis. This Savage---Dickey (SD) t test is inspired by the Jeffreys---Zellner---Siow (JZS) t test recently proposed by Rouder, Speckman, Sun, Morey, and Iverson (2009). The SD test retains the key concepts of the JZS test but is applicable to a wider range of statistical problems. The SD test allows researchers to test order restrictions and applies to two-sample situations in which the different groups do not share the same variance.},
  Day                      = {01},
  Doi                      = {10.3758/PBR.16.4.752},
  ISSN                     = {1531-5320},
  Owner                    = {marcos},
  Timestamp                = {2018.05.23},
  Url                      = {https://doi.org/10.3758/PBR.16.4.752}
}

@Manual{Wickham2017,
  Title                    = {tidyverse: Easily Install and Load the 'Tidyverse'},
  Author                   = {Hadley Wickham},
  Note                     = {R package version 1.2.1},
  Year                     = {2017},

  Url                      = {https://CRAN.R-project.org/package=tidyverse}
}

@Book{Wickham2016,
  Title                    = {ggplot2: Elegant Graphics for Data Analysis},
  Author                   = {Hadley Wickham},
  Publisher                = {Springer-Verlag New York},
  Year                     = {2016},

  ISBN                     = {978-3-319-24277-4},
  Url                      = {http://ggplot2.org}
}

@Article{Yates1951,
  Title                    = {The Influence of Statistical Methods for Research Workers on the Development of the Science of Statistics},
  Author                   = { F. Yates },
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1951},
  Number                   = {253},
  Pages                    = {19-34},
  Volume                   = {46},

  Doi                      = {10.1080/01621459.1951.10500764},
  Eprint                   = { 
 https://doi.org/10.1080/01621459.1951.10500764
 
},
  Owner                    = {marcos},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2018.03.14},
  Url                      = { 
 https://doi.org/10.1080/01621459.1951.10500764
 
}
}

@Manual{Yee2018,
  Title                    = {{VGAM}: Vector Generalized Linear and Additive Models},
  Author                   = {Thomas W. Yee},
  Note                     = {R package version 1.0-5},
  Year                     = {2018},

  Url                      = {https://CRAN.R-project.org/package=VGAM}
}

